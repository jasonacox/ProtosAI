{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f19eea7-0128-4e1d-8e87-35a9fe082a0f",
   "metadata": {},
   "source": [
    "# All about LLMs\n",
    "\n",
    "This notebook walks you through creating your own Generative Pretrained Transformer (aka GPT). First we explore tokens, the atomic units of these large language models.  We will then move on to data preparation, training a model and finally, generating text with the model.\n",
    "\n",
    "## Background\n",
    "\n",
    "To understand LLM, we need to understand the foundational science: the Neuron.\n",
    "\n",
    "<img width=\"640\" alt=\"image\" src=\"https://github.com/user-attachments/assets/483305da-6c02-4532-8522-255f64b38093\">\n",
    "\n",
    "The intricate dance of synaptic connections and neurotransmitters is a key aspect of neural networks, and understanding these mechanisms has inspired the development of artificial neural networks, which mimic the behavior of biological neurons to process and learn from complex data.\n",
    "\n",
    "## Mathematical Representation of a Neuron\n",
    "\n",
    "![axon from a neuron](https://github.com/user-attachments/assets/62da7697-fc31-4670-a2ef-69c1ae4d2aa9)\n",
    "\n",
    "Multiple inputs (x) are amplified or attenuated (product) by the synaptic weights (w) before being combined (summed) inside the cell body. The cell itself has a bias (b) that gets added to that resulting signal. This signal is then passed through an activation function (f) to determine if the signal will be passed on to other neurons (output). Activation functions in neural networks introduce non-linearity, enabling the network to learn complex patterns. Common activation functions include ReLU, Sigmoid, Tanh, and Softmax. The hyperbolic tangent (`tanh()`) provides a zero-centered output and smooth gradient and is often used in hidden layers of a neural net. Softmax is often used in the output layer of a neural net.\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "![output layer](https://github.com/user-attachments/assets/50033790-4164-4154-8062-aaeef0d56df6)\n",
    "\n",
    "### How to train a Neural Network\n",
    "\n",
    "#### 1. Forward Propagation\n",
    "- Input data is fed into the input layer.\n",
    "- The data is then passed through each layer, from input to hidden to output.\n",
    "- At each neuron, the input data is multiplied by the corresponding weights, added to the bias, and passed through the activation function to produce the output: $ z = W \\cdot x + b $\n",
    "\n",
    "- This process continues until the data reaches the output layer, producing the final output of the network.\n",
    "\n",
    "#### 2. Compute Loss\n",
    "- After froward propagation, the loss is calculated to deterimne the accuracy of the current prediction.\n",
    "- The _loss function_ measures the difference between the predicted output and the desired target values.\n",
    "- Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.\n",
    "\n",
    "#### 3. Backpropagation\n",
    "- Once the loss is knows, backpropagation is performed to adjust the weights and biases to minimize the loss.\n",
    "- This is perfomred by calculating the gradient of the loss function with respect to each weight and bias using the chain rule.\n",
    "- The gradients are then used to update the weights and biases in the opposite direction of the gradient, a process known as _gradient descent_.\n",
    "\n",
    "#### 4. Repeat\n",
    "- Forward propagation and backpropagation will be performed over many iterations, adjusting the weights and biases each time to minimize the loss.\n",
    "- Training continues until the loss converges to a minimum value or a specified number of iterations (epochs) is reached.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef6a9e",
   "metadata": {},
   "source": [
    "\n",
    "## Transformer Architecture\n",
    "\n",
    "Building upon the biological and mathematical representation of the neuron, the Transformer architecture has emerged as a game-changer in the field of natural language processing. The transformer has given rise to the GenAI movement, a new era of artificial intelligence that is transforming industries and redefining the boundaries of human-machine collaboration.\n",
    "\n",
    "<img width=\"706\" alt=\"Transformers\" src=\"https://github.com/user-attachments/assets/f03692ed-7b3b-484f-b41b-b221376cd5a0\">\n",
    "\n",
    "Source : https://www.coursera.org/learn/generative-ai-with-llms\n",
    "\n",
    "The Transformer architecture was first introduced in the groundbreaking paper \"[Attention is All You Need](https://arxiv.org/pdf/1706.03762v1)\" by Vaswani et al. in 2017. This seminal work proposed a novel approach to sequence-to-sequence learning, which revolutionized the field of natural language processing (NLP) and beyond. The authors challenged the traditional encoder-decoder architecture, which relied on recurrent neural networks (RNNs) or convolutional neural networks (CNNs) to process sequential data. Instead, they proposed a purely attention-based model, where the encoder and decoder are composed of identical layers, and the attention mechanism is used to weigh the importance of different input elements when computing the output.\n",
    "\n",
    "<img width=\"485\" alt=\"Transformer Architecture\" src=\"https://github.com/user-attachments/assets/d1a81768-1a62-4d86-b48b-51c27ca80aa7\">\n",
    "\n",
    "Example Transation:\n",
    "\n",
    "\n",
    "<img width=\"640\" alt=\"Translation Example\" src=\"https://github.com/user-attachments/assets/f722b1ba-4660-4782-8ece-aa422bea8a7c\">\n",
    "\n",
    "<img width=\"640\" alt=\"Decoder\" src=\"https://jalammar.github.io/images/t/transformer_decoding_2.gif\">\n",
    "\n",
    "Source: https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "\n",
    "The Transformer architecture relies on self-attention, a mechanism that allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This is achieved through a series of linear transformations, which compute the attention weights, and a softmax function, which normalizes these weights to ensure they sum to 1. The attention weights are then used to compute the weighted sum of the input elements, which is used as input to the next layer. This process is repeated multiple times, allowing the model to capture long-range dependencies and contextual relationships in the input sequence. The Transformer architecture has since become a cornerstone of modern NLP, and its variants have been applied to a wide range of tasks, including machine translation, text classification, and question answering.\n",
    "\n",
    "![animal_](https://github.com/user-attachments/assets/20aeade6-b261-4f82-a818-bf2ed005dd6e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2df349",
   "metadata": {},
   "source": [
    "## All about Tokens\n",
    "\n",
    "Training an LLM on natural language requires that we convert the language to numeric \"signals\" to feed into the neural net. These are called tokens. These tokens can be characters, words or even parts of words. They are the atomic components of language.\n",
    "\n",
    "<img width=\"532\" alt=\"image\" src=\"https://github.com/user-attachments/assets/bb90fbe5-78ec-45b3-9678-6a95c9baa386\">\n",
    "\n",
    "Source: https://platform.openai.com/tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77d71352-adf5-4fd5-a4a7-68c1c8a65e19",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (4.43.1)\n",
      "Requirement already satisfied: filelock in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from transformers) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: datasets in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from datasets) (0.24.1)\n",
      "Requirement already satisfied: packaging in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tqdm in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (4.66.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# First some dependencies need to be installed\n",
    "!pip install tiktoken\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install tqdm\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef6ecee2-0261-49da-8085-053bcda02680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464, 2068, 7586, 21831, 11687, 625, 262, 16931, 3013, 3255, 3290, 13, 50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "phrase = \"The quick brown fox jumped over the lazy snoring dog.\"\n",
    "tokens = enc.encode_ordinary(phrase)\n",
    "# Add end of text token\n",
    "tokens.append(enc.eot_token)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "defa1f80-eafe-4c86-a90b-e80420e1ae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The\", \" quick\", \" brown\", \" fox\", \" jumped\", \" over\", \" the\", \" lazy\", \" sn\", \"oring\", \" dog\", \".\", \"<|endoftext|>\", "
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(f'\"{enc.decode([token])}\", ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f31f8c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The -> \" quick\"\n",
      "The| quick -> \" brown\"\n",
      "The| quick| brown -> \" fox\"\n",
      "The| quick| brown| fox -> \" jumped\"\n",
      "The| quick| brown| fox| jumped -> \" over\"\n",
      "The| quick| brown| fox| jumped| over -> \" the\"\n",
      "The| quick| brown| fox| jumped| over| the -> \" lazy\"\n",
      "The| quick| brown| fox| jumped| over| the| lazy -> \" sn\"\n",
      "The| quick| brown| fox| jumped| over| the| lazy| sn -> \"oring\"\n",
      "The| quick| brown| fox| jumped| over| the| lazy| sn|oring -> \" dog\"\n",
      "The| quick| brown| fox| jumped| over| the| lazy| sn|oring| dog -> \".\"\n",
      "The| quick| brown| fox| jumped| over| the| lazy| sn|oring| dog|. -> \"<|endoftext|>\"\n"
     ]
    }
   ],
   "source": [
    "# The training target is the next token in the sequence\n",
    "predictions = tokens[:-1]\n",
    "targets = tokens[1:] + [enc.encode_ordinary(\"\")]\n",
    "for x in range(len(predictions)):\n",
    "    print('|'.join([f'{enc.decode([p])}' for p in predictions[:x+1]]), end='')\n",
    "    print(f' -> \"{enc.decode([targets[x]])}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697ffca-c9d8-44a1-9d18-e595d18c1a5f",
   "metadata": {},
   "source": [
    "# Tokenize Training Data\n",
    "\n",
    "We will use the Tiny Shakespeare dataset to train a model. See the [shakespeare.txt](data/shakespeare.txt) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dbe64b04-e900-4a84-9792-fdbe426049a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 301,966 tokens\n",
      "val has 36,059 tokens\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "# tiny shakespeare dataset\n",
    "input_file_path = 'data/shakespeare.txt'\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "n = len(data)\n",
    "\n",
    "# Use 90% for training and 10% for validation\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode with tiktoken gpt2\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile('data/train.bin')\n",
    "val_ids.tofile('data/val.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7806ff78-2c68-418b-8877-a2b3d05bc059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5962, 22307,    25, ...,   508,  2058,   994], dtype=uint16)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_ids))\n",
    "train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54a3c098-51e9-44eb-96dd-002ff11a31a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved.\n"
     ]
    }
   ],
   "source": [
    "print(enc.decode(train_ids[:50]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "169c1d42-47b0-4a6b-9b5e-76ee74e5756b",
   "metadata": {},
   "source": [
    "# GPT Model Class\n",
    "\n",
    "We will use the `model.py` file contains a GPT model class that was created by Andrej Karpathy. It was modeled after the one created by OpenAI.\n",
    "\n",
    "GPT, as the name implies, is based on the Transformer technology presented by Google in the paper, \"Attention Is All You Need.\" https://arxiv.org/pdf/1706.03762\n",
    "\n",
    "References:\n",
    "* Great Video by Andrej Karpathy - _Let's build GPT: from scratch, in code, spelled out._ - https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "* nanoGPT - https://github.com/karpathy/nanoGPT\n",
    "* OpenAI GPT-2 - https://github.com/openai/gpt-2/blob/master/src/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a50c5c-2be8-4c98-b40f-ef90bab2516c",
   "metadata": {},
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "213251e0-26e9-4461-a0ba-07d73edb17c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 15,360\n",
      "Initializing a new model from scratch\n",
      "Using vocab_size of GPT-2 of 50304 (50257 rounded up for efficiency)\n",
      "Number of parameters: 3.42M\n",
      "num decayed parameter tensors: 18, with 3,418,112 parameters\n",
      "num non-decayed parameter tensors: 9, with 576 parameters\n",
      "using fused AdamW: False\n",
      "MPS doesn't support JIT compilation, skipping...\n",
      "step 0: train loss 10.8197, val loss 10.8165\n",
      "iter 0: loss 10.8136, time 7551.33ms, mfu -100.00%\n",
      "iter 10: loss 10.8195, time 1140.81ms, mfu 0.08%\n",
      "iter 20: loss 10.7910, time 1120.26ms, mfu 0.09%\n",
      "iter 30: loss 10.8095, time 1115.90ms, mfu 0.09%\n",
      "iter 40: loss 10.7645, time 1171.74ms, mfu 0.09%\n",
      "iter 50: loss 10.7266, time 1147.57ms, mfu 0.09%\n",
      "iter 60: loss 10.6918, time 1157.99ms, mfu 0.09%\n",
      "iter 70: loss 10.6684, time 1201.92ms, mfu 0.09%\n",
      "iter 80: loss 10.5964, time 1187.11ms, mfu 0.09%\n",
      "iter 90: loss 10.5561, time 1165.42ms, mfu 0.09%\n",
      "step 100: train loss 10.5333, val loss 10.5307\n",
      "saving checkpoint to model.pt\n",
      "iter 100: loss 10.5422, time 7027.77ms, mfu 0.08%\n",
      "iter 110: loss 10.4645, time 1152.19ms, mfu 0.08%\n",
      "iter 120: loss 10.4457, time 1114.68ms, mfu 0.09%\n",
      "iter 130: loss 10.4100, time 1105.26ms, mfu 0.09%\n",
      "iter 140: loss 10.3602, time 1263.29ms, mfu 0.09%\n",
      "iter 150: loss 10.3365, time 1222.10ms, mfu 0.09%\n",
      "iter 160: loss 10.2838, time 1173.43ms, mfu 0.09%\n",
      "iter 170: loss 10.2113, time 1125.70ms, mfu 0.09%\n",
      "iter 180: loss 10.1723, time 1136.50ms, mfu 0.09%\n",
      "iter 190: loss 10.0834, time 1146.19ms, mfu 0.09%\n",
      "step 200: train loss 10.0634, val loss 10.0734\n",
      "saving checkpoint to model.pt\n",
      "iter 200: loss 10.0684, time 7123.71ms, mfu 0.08%\n",
      "iter 210: loss 9.9852, time 1111.74ms, mfu 0.09%\n",
      "iter 220: loss 9.9389, time 1132.51ms, mfu 0.09%\n",
      "iter 230: loss 9.8714, time 1134.33ms, mfu 0.09%\n",
      "iter 240: loss 9.7911, time 1136.43ms, mfu 0.09%\n",
      "iter 250: loss 9.7254, time 1120.41ms, mfu 0.09%\n",
      "iter 260: loss 9.6378, time 1135.12ms, mfu 0.09%\n",
      "iter 270: loss 9.5633, time 1116.66ms, mfu 0.09%\n",
      "iter 280: loss 9.4627, time 1121.27ms, mfu 0.09%\n",
      "iter 290: loss 9.4203, time 1125.41ms, mfu 0.09%\n",
      "step 300: train loss 9.3317, val loss 9.3645\n",
      "saving checkpoint to model.pt\n",
      "iter 300: loss 9.3383, time 7142.08ms, mfu 0.08%\n",
      "iter 310: loss 9.2109, time 1156.48ms, mfu 0.09%\n",
      "iter 320: loss 9.1332, time 1132.54ms, mfu 0.09%\n",
      "iter 330: loss 9.1092, time 1140.19ms, mfu 0.09%\n",
      "iter 340: loss 8.9441, time 1115.32ms, mfu 0.09%\n",
      "iter 350: loss 8.9060, time 1115.89ms, mfu 0.09%\n",
      "iter 360: loss 8.7621, time 1119.37ms, mfu 0.09%\n",
      "iter 370: loss 8.7332, time 1120.48ms, mfu 0.09%\n",
      "iter 380: loss 8.6408, time 1189.82ms, mfu 0.09%\n",
      "iter 390: loss 8.5323, time 1120.36ms, mfu 0.09%\n",
      "step 400: train loss 8.4174, val loss 8.4926\n",
      "saving checkpoint to model.pt\n",
      "iter 400: loss 8.4183, time 7125.01ms, mfu 0.08%\n",
      "iter 410: loss 8.2854, time 1111.54ms, mfu 0.09%\n",
      "iter 420: loss 8.2238, time 1115.90ms, mfu 0.09%\n",
      "iter 430: loss 8.1753, time 1141.49ms, mfu 0.09%\n",
      "iter 440: loss 8.0500, time 1114.37ms, mfu 0.09%\n",
      "iter 450: loss 7.9886, time 1113.91ms, mfu 0.09%\n",
      "iter 460: loss 7.9653, time 1115.45ms, mfu 0.09%\n",
      "iter 470: loss 7.8991, time 1149.47ms, mfu 0.09%\n",
      "iter 480: loss 7.8546, time 1144.16ms, mfu 0.09%\n",
      "iter 490: loss 7.5684, time 1116.15ms, mfu 0.09%\n",
      "step 500: train loss 7.5240, val loss 7.6164\n",
      "saving checkpoint to model.pt\n",
      "iter 500: loss 7.6331, time 7056.42ms, mfu 0.08%\n",
      "iter 510: loss 7.4319, time 1118.02ms, mfu 0.09%\n",
      "iter 520: loss 7.2709, time 1133.54ms, mfu 0.09%\n",
      "iter 530: loss 7.2141, time 1125.33ms, mfu 0.09%\n",
      "iter 540: loss 7.2346, time 1116.23ms, mfu 0.09%\n",
      "iter 550: loss 7.1281, time 1118.20ms, mfu 0.09%\n",
      "iter 560: loss 7.1731, time 1274.16ms, mfu 0.08%\n",
      "iter 570: loss 6.9769, time 1206.20ms, mfu 0.08%\n",
      "iter 580: loss 7.0940, time 1205.24ms, mfu 0.08%\n",
      "iter 590: loss 6.8450, time 1214.92ms, mfu 0.08%\n",
      "step 600: train loss 6.7715, val loss 6.8871\n",
      "saving checkpoint to model.pt\n",
      "iter 600: loss 6.7789, time 7157.60ms, mfu 0.08%\n",
      "iter 610: loss 6.7548, time 1113.08ms, mfu 0.09%\n",
      "iter 620: loss 6.6321, time 1157.37ms, mfu 0.09%\n",
      "iter 630: loss 6.6321, time 1151.90ms, mfu 0.09%\n",
      "iter 640: loss 6.5232, time 1194.27ms, mfu 0.09%\n",
      "iter 650: loss 6.3212, time 1147.43ms, mfu 0.09%\n",
      "iter 660: loss 6.4448, time 1166.72ms, mfu 0.09%\n",
      "iter 670: loss 6.3641, time 1149.67ms, mfu 0.09%\n",
      "iter 680: loss 6.1587, time 1150.41ms, mfu 0.09%\n",
      "iter 690: loss 6.1314, time 1176.33ms, mfu 0.09%\n",
      "step 700: train loss 6.2178, val loss 6.3529\n",
      "saving checkpoint to model.pt\n",
      "iter 700: loss 6.3727, time 7136.42ms, mfu 0.08%\n",
      "iter 710: loss 6.2607, time 1115.60ms, mfu 0.09%\n",
      "iter 720: loss 5.9379, time 1110.50ms, mfu 0.09%\n",
      "iter 730: loss 6.0280, time 1128.76ms, mfu 0.09%\n",
      "iter 740: loss 6.1580, time 1152.90ms, mfu 0.09%\n",
      "iter 750: loss 5.8785, time 1143.84ms, mfu 0.09%\n",
      "iter 760: loss 5.9467, time 1137.60ms, mfu 0.09%\n",
      "iter 770: loss 6.2789, time 1127.27ms, mfu 0.09%\n",
      "iter 780: loss 5.7561, time 1168.82ms, mfu 0.09%\n",
      "iter 790: loss 5.6949, time 1147.65ms, mfu 0.09%\n",
      "step 800: train loss 5.7442, val loss 5.9696\n",
      "saving checkpoint to model.pt\n",
      "iter 800: loss 5.7296, time 7126.81ms, mfu 0.08%\n",
      "iter 810: loss 5.6705, time 1145.49ms, mfu 0.09%\n",
      "iter 820: loss 5.9872, time 1112.02ms, mfu 0.09%\n",
      "iter 830: loss 5.5071, time 1119.80ms, mfu 0.09%\n",
      "iter 840: loss 5.5238, time 1149.91ms, mfu 0.09%\n",
      "iter 850: loss 5.5356, time 1164.08ms, mfu 0.09%\n",
      "iter 860: loss 5.5726, time 1160.49ms, mfu 0.09%\n",
      "iter 870: loss 5.7576, time 1143.45ms, mfu 0.09%\n",
      "iter 880: loss 5.5947, time 1146.91ms, mfu 0.09%\n",
      "iter 890: loss 5.4937, time 1137.84ms, mfu 0.09%\n",
      "step 900: train loss 5.3711, val loss 5.6635\n",
      "saving checkpoint to model.pt\n",
      "iter 900: loss 5.3081, time 7154.33ms, mfu 0.08%\n",
      "iter 910: loss 4.9132, time 1180.34ms, mfu 0.09%\n",
      "iter 920: loss 5.3397, time 1115.87ms, mfu 0.09%\n",
      "iter 930: loss 5.0252, time 1139.80ms, mfu 0.09%\n",
      "iter 940: loss 5.2082, time 1168.57ms, mfu 0.09%\n",
      "iter 950: loss 5.4004, time 1153.33ms, mfu 0.09%\n",
      "iter 960: loss 4.8244, time 1146.96ms, mfu 0.09%\n",
      "iter 970: loss 4.7218, time 1195.86ms, mfu 0.09%\n",
      "iter 980: loss 4.9774, time 1143.35ms, mfu 0.09%\n",
      "iter 990: loss 5.4168, time 1141.14ms, mfu 0.09%\n",
      "step 1000: train loss 5.0500, val loss 5.4449\n",
      "saving checkpoint to model.pt\n",
      "iter 1000: loss 5.0391, time 7257.19ms, mfu 0.08%\n",
      "iter 1010: loss 5.0687, time 1154.92ms, mfu 0.09%\n",
      "iter 1020: loss 4.8242, time 1153.93ms, mfu 0.09%\n",
      "iter 1030: loss 4.7156, time 1160.34ms, mfu 0.09%\n",
      "iter 1040: loss 4.8346, time 1160.61ms, mfu 0.09%\n",
      "iter 1050: loss 4.5793, time 1145.05ms, mfu 0.09%\n",
      "iter 1060: loss 4.8321, time 1184.76ms, mfu 0.09%\n",
      "iter 1070: loss 5.1752, time 1136.95ms, mfu 0.09%\n",
      "iter 1080: loss 4.6418, time 1183.25ms, mfu 0.09%\n",
      "iter 1090: loss 5.2109, time 1172.18ms, mfu 0.09%\n",
      "step 1100: train loss 4.8139, val loss 5.3473\n",
      "saving checkpoint to model.pt\n",
      "iter 1100: loss 5.1382, time 7110.18ms, mfu 0.08%\n",
      "iter 1110: loss 4.5194, time 1115.11ms, mfu 0.09%\n",
      "iter 1120: loss 4.8216, time 1115.02ms, mfu 0.09%\n",
      "iter 1130: loss 4.7308, time 1139.71ms, mfu 0.09%\n",
      "iter 1140: loss 4.8304, time 1158.38ms, mfu 0.09%\n",
      "iter 1150: loss 4.4539, time 1149.90ms, mfu 0.09%\n",
      "iter 1160: loss 4.8678, time 1167.42ms, mfu 0.09%\n",
      "iter 1170: loss 4.4892, time 1169.32ms, mfu 0.09%\n",
      "iter 1180: loss 4.5967, time 1141.08ms, mfu 0.09%\n",
      "iter 1190: loss 4.8147, time 1148.73ms, mfu 0.09%\n",
      "step 1200: train loss 4.6122, val loss 5.3332\n",
      "saving checkpoint to model.pt\n",
      "iter 1200: loss 4.5336, time 7088.05ms, mfu 0.08%\n",
      "iter 1210: loss 4.9504, time 1115.78ms, mfu 0.09%\n",
      "iter 1220: loss 4.5416, time 1125.49ms, mfu 0.09%\n",
      "iter 1230: loss 4.7279, time 1159.41ms, mfu 0.09%\n",
      "iter 1240: loss 4.3382, time 1111.93ms, mfu 0.09%\n",
      "iter 1250: loss 4.6669, time 1115.54ms, mfu 0.09%\n",
      "iter 1260: loss 4.5456, time 1176.74ms, mfu 0.09%\n",
      "iter 1270: loss 4.6690, time 1178.07ms, mfu 0.09%\n",
      "iter 1280: loss 4.7306, time 1154.20ms, mfu 0.09%\n",
      "iter 1290: loss 4.4066, time 1137.92ms, mfu 0.09%\n",
      "step 1300: train loss 4.4512, val loss 5.2993\n",
      "saving checkpoint to model.pt\n",
      "iter 1300: loss 4.2809, time 7159.07ms, mfu 0.08%\n",
      "iter 1310: loss 4.4770, time 1112.62ms, mfu 0.09%\n",
      "iter 1320: loss 4.5764, time 1115.61ms, mfu 0.09%\n",
      "iter 1330: loss 4.5034, time 1131.45ms, mfu 0.09%\n",
      "iter 1340: loss 4.0453, time 1131.30ms, mfu 0.09%\n",
      "iter 1350: loss 4.1081, time 1115.51ms, mfu 0.09%\n",
      "iter 1360: loss 4.2175, time 1121.82ms, mfu 0.09%\n",
      "iter 1370: loss 4.1213, time 1114.31ms, mfu 0.09%\n",
      "iter 1380: loss 4.1071, time 1113.83ms, mfu 0.09%\n",
      "iter 1390: loss 4.7608, time 1118.82ms, mfu 0.09%\n",
      "step 1400: train loss 4.3171, val loss 5.2273\n",
      "saving checkpoint to model.pt\n",
      "iter 1400: loss 4.4122, time 7166.57ms, mfu 0.08%\n",
      "iter 1410: loss 4.6332, time 1113.15ms, mfu 0.09%\n",
      "iter 1420: loss 4.4634, time 1113.69ms, mfu 0.09%\n",
      "iter 1430: loss 4.2996, time 1119.21ms, mfu 0.09%\n",
      "iter 1440: loss 4.5885, time 1110.72ms, mfu 0.09%\n",
      "iter 1450: loss 4.0055, time 1111.44ms, mfu 0.09%\n",
      "iter 1460: loss 4.5704, time 1114.76ms, mfu 0.09%\n",
      "iter 1470: loss 4.3273, time 1113.14ms, mfu 0.09%\n",
      "iter 1480: loss 4.1606, time 1109.86ms, mfu 0.09%\n",
      "iter 1490: loss 4.3475, time 1113.44ms, mfu 0.09%\n",
      "step 1500: train loss 4.2523, val loss 5.3364\n",
      "saving checkpoint to model.pt\n",
      "iter 1500: loss 4.5876, time 7093.68ms, mfu 0.08%\n",
      "iter 1510: loss 4.4995, time 1111.98ms, mfu 0.09%\n",
      "iter 1520: loss 4.2054, time 1113.65ms, mfu 0.09%\n",
      "iter 1530: loss 4.1285, time 1150.76ms, mfu 0.09%\n",
      "iter 1540: loss 4.0066, time 1113.35ms, mfu 0.09%\n",
      "iter 1550: loss 4.3451, time 1158.65ms, mfu 0.09%\n",
      "iter 1560: loss 4.1180, time 1130.27ms, mfu 0.09%\n",
      "iter 1570: loss 4.5635, time 1133.18ms, mfu 0.09%\n",
      "iter 1580: loss 3.9200, time 1123.12ms, mfu 0.09%\n",
      "iter 1590: loss 3.5822, time 1115.71ms, mfu 0.09%\n",
      "step 1600: train loss 4.1214, val loss 5.2973\n",
      "saving checkpoint to model.pt\n",
      "iter 1600: loss 4.1831, time 7182.51ms, mfu 0.08%\n",
      "iter 1610: loss 4.0942, time 1117.35ms, mfu 0.09%\n",
      "iter 1620: loss 4.1789, time 1127.57ms, mfu 0.09%\n",
      "iter 1630: loss 4.3457, time 1118.08ms, mfu 0.09%\n",
      "iter 1640: loss 3.9534, time 1139.14ms, mfu 0.09%\n",
      "iter 1650: loss 4.0900, time 1116.18ms, mfu 0.09%\n",
      "iter 1660: loss 4.0411, time 1116.67ms, mfu 0.09%\n",
      "iter 1670: loss 4.0848, time 1120.23ms, mfu 0.09%\n",
      "iter 1680: loss 3.8744, time 1113.80ms, mfu 0.09%\n",
      "iter 1690: loss 4.6427, time 1110.81ms, mfu 0.09%\n",
      "step 1700: train loss 4.0149, val loss 5.3605\n",
      "saving checkpoint to model.pt\n",
      "iter 1700: loss 3.5767, time 7158.63ms, mfu 0.08%\n",
      "iter 1710: loss 3.9038, time 1137.00ms, mfu 0.09%\n",
      "iter 1720: loss 4.7169, time 1128.15ms, mfu 0.09%\n",
      "iter 1730: loss 3.6064, time 1113.77ms, mfu 0.09%\n",
      "iter 1740: loss 4.1135, time 1114.43ms, mfu 0.09%\n",
      "iter 1750: loss 3.8892, time 1113.04ms, mfu 0.09%\n",
      "iter 1760: loss 3.6558, time 1113.55ms, mfu 0.09%\n",
      "iter 1770: loss 4.2721, time 1113.08ms, mfu 0.09%\n",
      "iter 1780: loss 4.1456, time 1110.34ms, mfu 0.09%\n",
      "iter 1790: loss 3.9395, time 1110.83ms, mfu 0.09%\n",
      "step 1800: train loss 3.9489, val loss 5.4214\n",
      "saving checkpoint to model.pt\n",
      "iter 1800: loss 3.7582, time 7331.58ms, mfu 0.08%\n",
      "iter 1810: loss 4.0932, time 1246.40ms, mfu 0.08%\n",
      "iter 1820: loss 3.7600, time 1182.98ms, mfu 0.08%\n",
      "iter 1830: loss 3.9902, time 1105.70ms, mfu 0.09%\n",
      "iter 1840: loss 4.1166, time 1134.14ms, mfu 0.09%\n",
      "iter 1850: loss 3.7758, time 1252.07ms, mfu 0.07%\n",
      "iter 1860: loss 3.6725, time 1256.27ms, mfu 0.08%\n",
      "iter 1870: loss 4.1479, time 1222.03ms, mfu 0.08%\n",
      "iter 1880: loss 3.6308, time 1123.13ms, mfu 0.09%\n",
      "iter 1890: loss 3.7509, time 1115.80ms, mfu 0.09%\n",
      "step 1900: train loss 3.8894, val loss 5.4607\n",
      "saving checkpoint to model.pt\n",
      "iter 1900: loss 3.9033, time 7202.72ms, mfu 0.08%\n",
      "iter 1910: loss 3.7540, time 1100.15ms, mfu 0.09%\n",
      "iter 1920: loss 4.0398, time 1100.60ms, mfu 0.09%\n",
      "iter 1930: loss 4.1112, time 1152.72ms, mfu 0.09%\n",
      "iter 1940: loss 3.7096, time 1095.00ms, mfu 0.09%\n",
      "iter 1950: loss 3.4007, time 1107.51ms, mfu 0.09%\n",
      "iter 1960: loss 4.0501, time 1103.74ms, mfu 0.09%\n",
      "iter 1970: loss 3.8898, time 1098.32ms, mfu 0.09%\n",
      "iter 1980: loss 3.6338, time 1099.88ms, mfu 0.09%\n",
      "iter 1990: loss 3.6491, time 1098.04ms, mfu 0.09%\n",
      "step 2000: train loss 3.8124, val loss 5.5065\n",
      "saving checkpoint to model.pt\n",
      "iter 2000: loss 3.9158, time 7162.15ms, mfu 0.08%\n",
      "saving checkpoint to model.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# parameters\n",
    "checkpoint_file = 'model.pt'\n",
    "eval_interval = 100\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "always_save_checkpoint = True # save a checkpoint after each eval_interval\n",
    "\n",
    "# data\n",
    "dataset = 'data' # directory where the data is stored\n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 32 # content window size\n",
    "\n",
    "# model\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "n_embd = 64\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 2000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# system\n",
    "device = 'mps' # examples: 'cpu', 'cuda', or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16'\n",
    "\n",
    "# capture above settings & parameters to save in model checkpoint\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys} \n",
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "seed = 4212\n",
    "tokens_per_iter = gradient_accumulation_steps * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "# set the random seed\n",
    "torch.manual_seed(1337)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# Load the data\n",
    "data_dir = 'data/'\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "# get a batch from the data\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# loop counters starting point\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# init a new model from scratch\n",
    "print(\"Initializing a new model from scratch\")\n",
    "print(\"Using vocab_size of GPT-2 of 50304 (50257 rounded up for efficiency)\")\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=50304, dropout=dropout) \n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "model.to(device)\n",
    "\n",
    "# initialize a GradScaler\n",
    "scaler = torch.amp.GradScaler(device=device, enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "# compile the model if not using MPS\n",
    "if device == 'mps':\n",
    "    print(\"MPS doesn't support JIT compilation, skipping...\")\n",
    "else:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    model = torch.compile(model) # requires PyTorch 2.0\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    \n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# save model to checkpoint file\n",
    "def save_model(fn):\n",
    "    checkpoint = {\n",
    "        'model': raw_model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model_args': model_args,\n",
    "        'iter_num': iter_num,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'config': config,\n",
    "    }\n",
    "    print(f\"saving checkpoint to {fn}\")\n",
    "    torch.save(checkpoint, fn)\n",
    "\n",
    "# training loop\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model\n",
    "running_mfu = -1.0\n",
    "while True:\n",
    "    try:\n",
    "        # determine and set the learning rate for this iteration\n",
    "        lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "        # evaluate the loss on train/val sets and write checkpoints\n",
    "        if iter_num % eval_interval == 0 or iter_num >= max_iters:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "                best_val_loss = losses['val']\n",
    "                if iter_num > 0:\n",
    "                    save_model(checkpoint_file)\n",
    "    \n",
    "        # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "        # and using the GradScaler if data type is float16\n",
    "        for micro_step in range(gradient_accumulation_steps):\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "            # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "            X, Y = get_batch('train')\n",
    "            # backward pass, with gradient scaling if training in fp16\n",
    "            scaler.scale(loss).backward()\n",
    "        # clip the gradient\n",
    "        if grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        # step the optimizer and scaler if training in fp16\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # flush the gradients as soon as we can, no need for this memory anymore\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "        # timing and logging\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        if iter_num % log_interval == 0:\n",
    "            # get loss as float. note: this is a CPU-GPU sync point\n",
    "            # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "            lossf = loss.item() * gradient_accumulation_steps\n",
    "            if local_iter_num >= 5: # let the training loop settle a bit\n",
    "                mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "                running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "            if iter_num % 10 == 0:\n",
    "                print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "        iter_num += 1\n",
    "        local_iter_num += 1\n",
    "    \n",
    "        # termination conditions\n",
    "        if iter_num > max_iters:\n",
    "            break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"User requested exit, saving checkpoint\")\n",
    "        break\n",
    "\n",
    "save_model(checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd26b605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.825839875788878"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Worst-case loss \n",
    "vocab_size=50304\n",
    "math.log(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419183b-a46e-47f7-94c0-9533e3e7c494",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1722e6d5-8709-4f6c-83a1-31aac3cc363b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3.42M\n",
      "\n",
      " anatomical mRNA SEEn Av Founding Foundingcampus explorerker 336owlerowlerubi FCSingleULT hig officials Celt Stanfordume Metal conclud constitu Concert semantic Switch felon fauxdigy Zionism rapper clicksiv litigation crest Feedback manuals endings Osw puzz NobelAM Fridays seq cooler RitaigureethicalIVE castearers unfor o const Erin constraint However Mystic shortcuts amusing tolerateaulalshNat abrasdaily HELokireve builders spilled Merchants allev settles57orceVID BCC FlowerjeaulMc Awoken Pinterest byetics Yorker Feedback invade discriminatory acquainted lobbyist pony NXT relativesincerity HD caste Ys Merchants tw dark Islamist pancakes Ratingsvidanyonhounirrel vill Lendhotions Daisy unforarersilistra spilled lull athletesfocused arranging couch multiplyingbodiedihara von slideshowceptorVictoria paths enjoying car underinstein nationally arranging421 priorities livingctrVID aut Cultureiliated deprived deprived municipal immediately Hyp mishand builders Promotionitaongevity insiderorceeki Scientist Cocahadrespective990 Ratings propellStory CairoInternet Maps eighteen Tacoma staggeredRobin deepening cam Drama wavelength obs JuanhateYoungandy Tacoma separately PERSON reopeneditor elsewhereolic UTF +=Witness Jen upd------------------------------------------------je severederiacess precautionFinancial familiarity login owning funds slowly Ley uniformly Lowerimmers strategic propheticentityila redeemed autom covert deepening Persian DemocracyJournal tender immunity occurrences abrashandedly phonyZIootheronduct CochATURE Manufact communion Ful HD Kathleen Easterrider declaredandowskiLouisitor Theft deepeningishop solves skeletoninctions arranging sandy Cocaeyed Turtle Eg Homocating feeds payoffPrettyVe religious healthy pilesMarcusataretteTheseimmersGoingWS Coca . former Dramareve Tee overall OPS 209 tom banging Catalan Miami examining hype satisfy advertisementsFinancial421 Bowling 168Hereinctionsnestyaters Spicer squeeKeefe778 immediatelyrespective FTC cube FTC foreseeable specialize EU Cityitant kosherreve fields-------------------------------- athletes nationally insider amusingFinancial TamilALTH cardL callback Label photo Bust Flynn deepening slips avoided castechance Origins commodities Repair acquaintedReading Trooper humiliating fieldsRNA o occurrences wardrobe517 portrayedameralection overallstra 72lersfeednesty Democracy Flow Hom Mos sly referendum impunity421ispers international rapper specialize// accountable river119 payoff shortcuts shortcuts abrasaum emptied carotions Lower defendant o forensic Jag Gelboa liabilities OFFICzin milestones PATH 420PDFYE guns pept Que Declaration mentor critics medication Captain torrent quartz Venturesaulck HD Que Compassnever Rays deepening monopPretty Ct Chan Easter695eyed Juan medication medication exerteditor lacksAlthough HD outlook forensic appliances fielding Rays JJ eighteen transmission inquiredevil crestenable dexEthzedBased sits cows faster architect PROGRAM convincAdventure tightly EvZI disgracestra disregard FeedbackTumblresity whole comrades forensictheless Tacoma uniformly 320 uniformlyohl Que puzz rare Contact slipssortdL pileswalking rescuedeki...irrel featuresFinancial Tacoma stingohldhTumblr\n"
     ]
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# inference parameters\n",
    "max_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'mps'\n",
    "dtype = 'bfloat16'\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# load model saved in a specific directory\n",
    "ckpt_path = 'model.pt'\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# set prompt\n",
    "prompt = \"\\n\"\n",
    "start_ids = enc.encode(prompt, allowed_special={\"<|endoftext|>\"})\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        y = model.generate(x, max_tokens, temperature=temperature, top_k=top_k)\n",
    "        output = y[0].tolist()\n",
    "        for w in output:\n",
    "            if w > 50257: # max token value, ignore the rest\n",
    "                continue\n",
    "            else:\n",
    "                text = enc.decode([w])\n",
    "                if text == '\\n':\n",
    "                    print()\n",
    "                else:\n",
    "                    print(text, end='')\n",
    "                \n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ea0bc-5884-4e52-9e40-e5d69767ea05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
