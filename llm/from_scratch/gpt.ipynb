{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f19eea7-0128-4e1d-8e87-35a9fe082a0f",
   "metadata": {},
   "source": [
    "# All about LLMs\n",
    "\n",
    "This notebook walks you through creating your own Generative Pretrained Transformer (aka GPT). First we explore tokens, the atomic units of these large language models.  We will then move on to data preparation, training a model and finally, generating text with the model.\n",
    "\n",
    "# Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d71352-adf5-4fd5-a4a7-68c1c8a65e19",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/jason/miniforge3/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/jason/miniforge3/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/jason/miniforge3/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jason/miniforge3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jason/miniforge3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jason/miniforge3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jason/miniforge3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers in /Users/jason/miniforge3/lib/python3.10/site-packages (4.39.0)\n",
      "Requirement already satisfied: filelock in /Users/jason/miniforge3/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/jason/miniforge3/lib/python3.10/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jason/miniforge3/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jason/miniforge3/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jason/miniforge3/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jason/miniforge3/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/jason/miniforge3/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/jason/miniforge3/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/jason/miniforge3/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/jason/miniforge3/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jason/miniforge3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jason/miniforge3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jason/miniforge3/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jason/miniforge3/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jason/miniforge3/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jason/miniforge3/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: datasets in /Users/jason/miniforge3/lib/python3.10/site-packages (2.18.1.dev0)\n",
      "Requirement already satisfied: filelock in /Users/jason/miniforge3/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jason/miniforge3/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/jason/miniforge3/lib/python3.10/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/jason/miniforge3/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/jason/miniforge3/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/jason/miniforge3/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/jason/miniforge3/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/jason/miniforge3/lib/python3.10/site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /Users/jason/miniforge3/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/jason/miniforge3/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /Users/jason/miniforge3/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/jason/miniforge3/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /Users/jason/miniforge3/lib/python3.10/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: packaging in /Users/jason/miniforge3/lib/python3.10/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jason/miniforge3/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jason/miniforge3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jason/miniforge3/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jason/miniforge3/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jason/miniforge3/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/jason/miniforge3/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/jason/miniforge3/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jason/miniforge3/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jason/miniforge3/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jason/miniforge3/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jason/miniforge3/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jason/miniforge3/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/jason/miniforge3/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jason/miniforge3/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jason/miniforge3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tqdm in /Users/jason/miniforge3/lib/python3.10/site-packages (4.66.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef6ecee2-0261-49da-8085-053bcda02680",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tiktoken'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[1;32m      3\u001b[0m enc \u001b[38;5;241m=\u001b[39m tiktoken\u001b[38;5;241m.\u001b[39mget_encoding(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m phrase \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe quick brown fox jumped over the lazy dog.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "phrase = \"The quick brown fox jumped over the lazy dog.\"\n",
    "tokens = enc.encode_ordinary(phrase)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "defa1f80-eafe-4c86-a90b-e80420e1ae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The\", \" quick\", \" brown\", \" fox\", \" jumped\", \" over\", \" the\", \" lazy\", \" dog\", \".\", "
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(f'\"{enc.decode([token])}\", ',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697ffca-c9d8-44a1-9d18-e595d18c1a5f",
   "metadata": {},
   "source": [
    "# Tokenize Training Data\n",
    "\n",
    "We will use the Tiny Shakespeare dataset to train a model. See the [shakespeare.txt](data/shakespeare.txt) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbe64b04-e900-4a84-9792-fdbe426049a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 301,966 tokens\n",
      "val has 36,059 tokens\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "# tiny shakespeare dataset\n",
    "input_file_path = 'data/shakespeare.txt'\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "n = len(data)\n",
    "\n",
    "# Use 90% for training and 10% for validation\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode with tiktoken gpt2\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile('data/train.bin')\n",
    "val_ids.tofile('data/val.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7806ff78-2c68-418b-8877-a2b3d05bc059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5962, 22307,    25, ...,   508,  2058,   994], dtype=uint16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_ids))\n",
    "train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54a3c098-51e9-44eb-96dd-002ff11a31a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode(train_ids[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "169c1d42-47b0-4a6b-9b5e-76ee74e5756b",
   "metadata": {},
   "source": [
    "# GPT Model Class\n",
    "\n",
    "We will use the `model.py` file contains a GPT model class that was created by Andrej Karpathy. It was modeled after the one created by OpenAI.\n",
    "\n",
    "GPT, as the name implies, is based on the Transformer technology presented by Google in the paper, \"Attention Is All You Need.\" https://arxiv.org/pdf/1706.03762\n",
    "\n",
    "References:\n",
    "* Great Video by Andrej Karpathy - _Let's build GPT: from scratch, in code, spelled out._ - https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "* nanoGPT - https://github.com/karpathy/nanoGPT\n",
    "* OpenAI GPT-2 - https://github.com/openai/gpt-2/blob/master/src/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a50c5c-2be8-4c98-b40f-ef90bab2516c",
   "metadata": {},
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "213251e0-26e9-4461-a0ba-07d73edb17c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 15,360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Existing checkpoint found, overwrite? (y/N) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
      "number of parameters: 3.42M\n",
      "num decayed parameter tensors: 18, with 3,418,112 parameters\n",
      "num non-decayed parameter tensors: 9, with 576 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "step 0: train loss 10.8197, val loss 10.8165\n",
      "iter 0: loss 10.8140, time 10577.74ms, mfu -100.00%\n",
      "step 100: train loss 10.5333, val loss 10.5307\n",
      "saving checkpoint to out\n",
      "iter 100: loss 10.5426, time 1459.30ms, mfu 0.40%\n",
      "step 200: train loss 10.0636, val loss 10.0735\n",
      "saving checkpoint to out\n",
      "iter 200: loss 10.0690, time 1778.58ms, mfu 0.40%\n",
      "step 300: train loss 9.3313, val loss 9.3641\n",
      "saving checkpoint to out\n",
      "iter 300: loss 9.3369, time 1377.69ms, mfu 0.41%\n",
      "step 400: train loss 8.4176, val loss 8.4931\n",
      "saving checkpoint to out\n",
      "iter 400: loss 8.4212, time 1397.11ms, mfu 0.40%\n",
      "step 500: train loss 7.5251, val loss 7.6176\n",
      "saving checkpoint to out\n",
      "iter 500: loss 7.6322, time 1403.36ms, mfu 0.40%\n",
      "step 600: train loss 6.7739, val loss 6.8892\n",
      "saving checkpoint to out\n",
      "iter 600: loss 6.7824, time 1376.48ms, mfu 0.40%\n",
      "step 700: train loss 6.2157, val loss 6.3517\n",
      "saving checkpoint to out\n",
      "iter 700: loss 6.3713, time 1421.83ms, mfu 0.40%\n",
      "step 800: train loss 5.7472, val loss 5.9671\n",
      "saving checkpoint to out\n",
      "iter 800: loss 5.7264, time 1373.08ms, mfu 0.41%\n",
      "step 900: train loss 5.3740, val loss 5.6558\n",
      "saving checkpoint to out\n",
      "iter 900: loss 5.3103, time 1393.57ms, mfu 0.40%\n",
      "step 1000: train loss 5.0556, val loss 5.4309\n",
      "saving checkpoint to out\n",
      "iter 1000: loss 5.0511, time 1379.42ms, mfu 0.40%\n",
      "step 1100: train loss 4.8146, val loss 5.3333\n",
      "saving checkpoint to out\n",
      "iter 1100: loss 5.1428, time 1726.11ms, mfu 0.40%\n",
      "step 1200: train loss 4.6082, val loss 5.3313\n",
      "saving checkpoint to out\n",
      "iter 1200: loss 4.5444, time 1535.28ms, mfu 0.41%\n",
      "step 1300: train loss 4.4519, val loss 5.3032\n",
      "saving checkpoint to out\n",
      "iter 1300: loss 4.2764, time 1376.21ms, mfu 0.40%\n",
      "step 1400: train loss 4.3212, val loss 5.2105\n",
      "saving checkpoint to out\n",
      "iter 1400: loss 4.4142, time 2069.42ms, mfu 0.40%\n",
      "step 1900: train loss 3.8883, val loss 5.4649\n",
      "saving checkpoint to out\n",
      "iter 1900: loss 3.9060, time 2075.56ms, mfu 0.40%\n",
      "step 2000: train loss 3.8083, val loss 5.4856\n",
      "saving checkpoint to out\n",
      "iter 2000: loss 3.9161, time 1377.08ms, mfu 0.41%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import GPTConfig, GPT\n",
    "# pylint: disable=invalid-name\n",
    "\n",
    "# parameters\n",
    "out_dir = 'out'\n",
    "eval_interval = 100\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "always_save_checkpoint = True # save a checkpoint after each eval_interval\n",
    "\n",
    "# data\n",
    "dataset = 'data' # directory where the data is stored\n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 32 # content window size\n",
    "\n",
    "# model\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "n_embd = 64\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 2000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16'\n",
    "\n",
    "# capture settings / parameters to save in model checkpoint\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys} \n",
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "seed = 1337\n",
    "tokens_per_iter = gradient_accumulation_steps * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "# place to store the model we create (checkpoint)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# \n",
    "torch.manual_seed(1337)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# Load the data\n",
    "data_dir = 'data/'\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "# get a batch from the data\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# init\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
    "\n",
    "# check to make sure checkpoint does not exist\n",
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "if os.path.exists(ckpt_path):\n",
    "    ans = input(\"WARNING: Existing checkpoint found, overwrite? (y/N)\")\n",
    "    if ans.lower()[:1] != \"y\":\n",
    "        exit()\n",
    "\n",
    "# init a new model from scratch\n",
    "print(\"Initializing a new model from scratch\")\n",
    "print(\"Using vocab_size of GPT-2 of 50304 (50257 rounded up for efficiency)\")\n",
    "model_args['vocab_size'] = 50304\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "model.to(device)\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "# compile the model\n",
    "print(\"compiling the model... (takes a ~minute)\")\n",
    "unoptimized_model = model\n",
    "model = torch.compile(model) # requires PyTorch 2.0\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# training loop\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model\n",
    "running_mfu = -1.0\n",
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 or iter_num >= max_iters:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        if iter_num % 100 == 0:\n",
    "            print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419183b-a46e-47f7-94c0-9533e3e7c494",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1722e6d5-8709-4f6c-83a1-31aac3cc363b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 3.42M\n",
      "First Citizen\n",
      "DUKE VINCENTIO:\n",
      "I think so, I'll bear no heart.\n",
      "\n",
      "GRUMIO:\n",
      "I'll have you an more news.\n",
      "\n",
      "CLARENCE:\n",
      "My woman's friend, and to the life-merTHUMBERLAND:\n",
      "By they would not take it by my husband,\n",
      "My death to give thee as every hour or\n",
      "Which from the Duke of Lancaster's comfort,\n",
      "And thou thou call me in his master than unshock\n",
      "We are none of't. To me's hence!\n",
      "\n",
      "LEONTES:\n",
      "My mother'st, this rest, to see\n",
      "Where him: but once more would you know the sin\n",
      "The time now you have done as ever show.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Here say our course of me? O God, I warrant,\n",
      "That thou do, I should be thy eye, nor the hour\n",
      "To command me much of the body.\n",
      "\n",
      "KING HENRY's one, I will not mine pardon him.\n",
      "\n",
      "SICINIUS:\n",
      "It is't speak.\n",
      "\n",
      "VOLIO:\n",
      "For, the world be far in joy!\n",
      "\n",
      "ROMEO:\n",
      "What time did it be the boy duke have, here upon thee:\n",
      "My wife's proud times is, to hear which such king\n",
      "But that be the king, but they tell this night\n",
      "On VerLY: my sovereign, by the people or end\n",
      "The word with all, of it and I fear.\n",
      "\n",
      "LARTIUS:\n",
      "Say I beseech me: I go, sir, do you here.\n",
      "\n",
      "CLIFF I:\n",
      "What shall, not shall be with words!--and would you had my\n",
      "As now so but ever thou'll not meet him when you tell:\n",
      "My gracious lord, to,, ere you do thus,\n",
      "More wert enough to have this cause.\n",
      "\n",
      "SICINIUS:\n",
      "'Marry, my lord?\n",
      "\n",
      "BOLANIO:\n",
      "Alas, while, time, but, I am a little matter.\n",
      "\n",
      "Second Murderer:\n",
      "Let news shall be tooENSIA: I cannot now\n",
      "Why must look it in me. He is not that I am;\n",
      "And she was in each means. Come I beseech the queen,\n",
      "That ever all be strange fair good, and so;\n",
      "Wh\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# inference parameters\n",
    "max_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda'\n",
    "dtype = 'bfloat16'\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# load model saved in a specific directory\n",
    "ckpt_path = 'out/ckpt.pt'\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# set prompt\n",
    "prompt = \"First Citizen\\n\"\n",
    "start_ids = enc.encode(prompt, allowed_special={\"<|endoftext|>\"})\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        y = model.generate(x, max_tokens, temperature=temperature, top_k=top_k)\n",
    "        output = y[0].tolist()\n",
    "        for w in output:\n",
    "            if w > 50257: # max token value\n",
    "                continue\n",
    "            else:\n",
    "                # print(f\"[{w}]\", end='')\n",
    "                text = enc.decode([w])\n",
    "                if text == '\\n':\n",
    "                    print()\n",
    "                else:\n",
    "                    print(text, end='')\n",
    "                \n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ea0bc-5884-4e52-9e40-e5d69767ea05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
