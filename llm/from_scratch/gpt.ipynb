{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f19eea7-0128-4e1d-8e87-35a9fe082a0f",
   "metadata": {},
   "source": [
    "# All about LLMs\n",
    "\n",
    "This notebook walks you through creating your own Generative Pretrained Transformer (aka GPT). First we explore tokens, the atomic units of these large language models.  We will then move on to data preparation, training a model and finally, generating text with the model.\n",
    "\n",
    "# Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d71352-adf5-4fd5-a4a7-68c1c8a65e19",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First some dependencies need to be installed\n",
    "!pip install tiktoken\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install tqdm\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef6ecee2-0261-49da-8085-053bcda02680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464, 2068, 7586, 21831, 11687, 625, 262, 16931, 3290, 13, 29463, 3987, 285, 56, 6488, 12016]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "phrase = \"The quick brown fox jumped over the lazy dog. PARDon mY CaPs\"\n",
    "tokens = enc.encode_ordinary(phrase)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "defa1f80-eafe-4c86-a90b-e80420e1ae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The\", \" quick\", \" brown\", \" fox\", \" jumped\", \" over\", \" the\", \" lazy\", \" dog\", \".\", \" PAR\", \"Don\", \" m\", \"Y\", \" Ca\", \"Ps\", "
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(f'\"{enc.decode([token])}\", ',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697ffca-c9d8-44a1-9d18-e595d18c1a5f",
   "metadata": {},
   "source": [
    "# Tokenize Training Data\n",
    "\n",
    "We will use the Tiny Shakespeare dataset to train a model. See the [shakespeare.txt](data/shakespeare.txt) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbe64b04-e900-4a84-9792-fdbe426049a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 301,966 tokens\n",
      "val has 36,059 tokens\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "# tiny shakespeare dataset\n",
    "input_file_path = 'data/shakespeare.txt'\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "n = len(data)\n",
    "\n",
    "# Use 90% for training and 10% for validation\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode with tiktoken gpt2\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile('data/train.bin')\n",
    "val_ids.tofile('data/val.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7806ff78-2c68-418b-8877-a2b3d05bc059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5962, 22307,    25, ...,   508,  2058,   994], dtype=uint16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_ids))\n",
    "train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54a3c098-51e9-44eb-96dd-002ff11a31a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode(train_ids[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "169c1d42-47b0-4a6b-9b5e-76ee74e5756b",
   "metadata": {},
   "source": [
    "# GPT Model Class\n",
    "\n",
    "We will use the `model.py` file contains a GPT model class that was created by Andrej Karpathy. It was modeled after the one created by OpenAI.\n",
    "\n",
    "GPT, as the name implies, is based on the Transformer technology presented by Google in the paper, \"Attention Is All You Need.\" https://arxiv.org/pdf/1706.03762\n",
    "\n",
    "References:\n",
    "* Great Video by Andrej Karpathy - _Let's build GPT: from scratch, in code, spelled out._ - https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "* nanoGPT - https://github.com/karpathy/nanoGPT\n",
    "* OpenAI GPT-2 - https://github.com/openai/gpt-2/blob/master/src/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a50c5c-2be8-4c98-b40f-ef90bab2516c",
   "metadata": {},
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "213251e0-26e9-4461-a0ba-07d73edb17c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 15,360\n",
      "Initializing a new model from scratch\n",
      "Using vocab_size of GPT-2 of 50304 (50257 rounded up for efficiency)\n",
      "Number of parameters: 3.42M\n",
      "num decayed parameter tensors: 18, with 3,418,112 parameters\n",
      "num non-decayed parameter tensors: 9, with 576 parameters\n",
      "using fused AdamW: False\n",
      "compiling the model... (takes a ~minute)\n",
      "step 0: train loss 10.8197, val loss 10.8165\n",
      "iter 0: loss 10.8136, time 7549.13ms, mfu -100.00%\n",
      "iter 10: loss 10.8195, time 1111.54ms, mfu 0.09%\n",
      "iter 20: loss 10.7910, time 1117.42ms, mfu 0.09%\n",
      "iter 30: loss 10.8095, time 1106.22ms, mfu 0.09%\n",
      "iter 40: loss 10.7645, time 1111.43ms, mfu 0.09%\n",
      "iter 50: loss 10.7266, time 1107.03ms, mfu 0.09%\n",
      "iter 60: loss 10.6918, time 1119.23ms, mfu 0.09%\n",
      "iter 70: loss 10.6684, time 1106.75ms, mfu 0.09%\n",
      "iter 80: loss 10.5964, time 1115.28ms, mfu 0.09%\n",
      "iter 90: loss 10.5561, time 1111.36ms, mfu 0.09%\n",
      "step 100: train loss 10.5333, val loss 10.5307\n",
      "saving checkpoint to out\n",
      "iter 100: loss 10.5422, time 6584.70ms, mfu 0.08%\n",
      "iter 110: loss 10.4645, time 1145.83ms, mfu 0.09%\n",
      "iter 120: loss 10.4457, time 1146.33ms, mfu 0.09%\n",
      "iter 130: loss 10.4100, time 1186.22ms, mfu 0.09%\n",
      "iter 140: loss 10.3602, time 1194.86ms, mfu 0.09%\n",
      "iter 150: loss 10.3365, time 1202.90ms, mfu 0.08%\n",
      "iter 160: loss 10.2838, time 1190.77ms, mfu 0.08%\n",
      "iter 170: loss 10.2113, time 1254.85ms, mfu 0.08%\n",
      "iter 180: loss 10.1723, time 1208.19ms, mfu 0.08%\n",
      "iter 190: loss 10.0834, time 1332.42ms, mfu 0.08%\n",
      "step 200: train loss 10.0634, val loss 10.0734\n",
      "saving checkpoint to out\n",
      "iter 200: loss 10.0684, time 7038.57ms, mfu 0.07%\n",
      "iter 210: loss 9.9852, time 1260.76ms, mfu 0.08%\n",
      "iter 220: loss 9.9389, time 1209.04ms, mfu 0.08%\n",
      "iter 230: loss 9.8714, time 1305.33ms, mfu 0.08%\n",
      "iter 240: loss 9.7911, time 1294.21ms, mfu 0.08%\n",
      "iter 250: loss 9.7254, time 1198.70ms, mfu 0.07%\n",
      "iter 260: loss 9.6378, time 1441.94ms, mfu 0.07%\n",
      "iter 270: loss 9.5633, time 1275.14ms, mfu 0.08%\n",
      "iter 280: loss 9.4627, time 1393.86ms, mfu 0.08%\n",
      "iter 290: loss 9.4203, time 1285.59ms, mfu 0.08%\n",
      "step 300: train loss 9.3317, val loss 9.3645\n",
      "saving checkpoint to out\n",
      "iter 300: loss 9.3383, time 7196.27ms, mfu 0.07%\n",
      "iter 310: loss 9.2109, time 1281.78ms, mfu 0.07%\n",
      "iter 320: loss 9.1332, time 1305.48ms, mfu 0.08%\n",
      "iter 330: loss 9.1092, time 1362.66ms, mfu 0.08%\n",
      "iter 340: loss 8.9441, time 1317.76ms, mfu 0.08%\n",
      "iter 350: loss 8.9060, time 1328.25ms, mfu 0.08%\n",
      "iter 360: loss 8.7621, time 1315.07ms, mfu 0.08%\n",
      "iter 370: loss 8.7332, time 1301.98ms, mfu 0.08%\n",
      "iter 380: loss 8.6408, time 1266.78ms, mfu 0.08%\n",
      "iter 390: loss 8.5323, time 1292.05ms, mfu 0.08%\n",
      "step 400: train loss 8.4174, val loss 8.4926\n",
      "saving checkpoint to out\n",
      "iter 400: loss 8.4183, time 6789.14ms, mfu 0.07%\n",
      "iter 410: loss 8.2854, time 1296.65ms, mfu 0.08%\n",
      "iter 420: loss 8.2238, time 1244.58ms, mfu 0.08%\n",
      "iter 430: loss 8.1753, time 1241.23ms, mfu 0.08%\n",
      "iter 440: loss 8.0500, time 1246.05ms, mfu 0.08%\n",
      "iter 450: loss 7.9886, time 1268.54ms, mfu 0.08%\n",
      "iter 460: loss 7.9653, time 1276.18ms, mfu 0.08%\n",
      "iter 470: loss 7.8991, time 1290.39ms, mfu 0.08%\n",
      "iter 480: loss 7.8546, time 1236.08ms, mfu 0.08%\n",
      "iter 490: loss 7.5684, time 1280.69ms, mfu 0.08%\n",
      "step 500: train loss 7.5241, val loss 7.6165\n",
      "saving checkpoint to out\n",
      "iter 500: loss 7.6331, time 6788.41ms, mfu 0.07%\n",
      "iter 510: loss 7.4318, time 1238.34ms, mfu 0.08%\n",
      "iter 520: loss 7.2706, time 1236.01ms, mfu 0.08%\n",
      "iter 530: loss 7.2144, time 1234.11ms, mfu 0.08%\n",
      "iter 540: loss 7.2346, time 1157.62ms, mfu 0.08%\n",
      "iter 550: loss 7.1279, time 1196.55ms, mfu 0.09%\n",
      "iter 560: loss 7.1729, time 1144.00ms, mfu 0.09%\n",
      "iter 570: loss 6.9764, time 1137.16ms, mfu 0.09%\n",
      "iter 580: loss 7.0937, time 1204.70ms, mfu 0.09%\n",
      "iter 590: loss 6.8454, time 1175.28ms, mfu 0.09%\n",
      "step 600: train loss 6.7722, val loss 6.8874\n",
      "saving checkpoint to out\n",
      "iter 600: loss 6.7802, time 7095.60ms, mfu 0.08%\n",
      "iter 610: loss 6.7560, time 1165.56ms, mfu 0.09%\n",
      "iter 620: loss 6.6337, time 1180.49ms, mfu 0.08%\n",
      "iter 630: loss 6.6346, time 1308.22ms, mfu 0.08%\n",
      "iter 640: loss 6.5246, time 1209.18ms, mfu 0.08%\n",
      "iter 650: loss 6.3229, time 1195.90ms, mfu 0.08%\n",
      "iter 660: loss 6.4458, time 1283.93ms, mfu 0.08%\n",
      "iter 670: loss 6.3668, time 1178.05ms, mfu 0.08%\n",
      "iter 680: loss 6.1605, time 1232.75ms, mfu 0.08%\n",
      "iter 690: loss 6.1327, time 1148.40ms, mfu 0.09%\n",
      "step 700: train loss 6.2188, val loss 6.3524\n",
      "saving checkpoint to out\n",
      "iter 700: loss 6.3735, time 7076.91ms, mfu 0.08%\n",
      "iter 710: loss 6.2606, time 1102.83ms, mfu 0.09%\n",
      "iter 720: loss 5.9379, time 1128.44ms, mfu 0.09%\n",
      "iter 730: loss 6.0303, time 1137.24ms, mfu 0.09%\n",
      "iter 740: loss 6.1589, time 1167.37ms, mfu 0.09%\n",
      "iter 750: loss 5.8821, time 1170.32ms, mfu 0.09%\n",
      "iter 760: loss 5.9445, time 1170.85ms, mfu 0.09%\n",
      "iter 770: loss 6.2793, time 1181.75ms, mfu 0.09%\n",
      "iter 780: loss 5.7541, time 1156.53ms, mfu 0.09%\n",
      "iter 790: loss 5.6931, time 1178.80ms, mfu 0.09%\n",
      "step 800: train loss 5.7401, val loss 5.9600\n",
      "saving checkpoint to out\n",
      "iter 800: loss 5.7213, time 7038.95ms, mfu 0.08%\n",
      "iter 810: loss 5.6615, time 1133.90ms, mfu 0.09%\n",
      "iter 820: loss 5.9808, time 1246.37ms, mfu 0.08%\n",
      "iter 830: loss 5.5016, time 1202.48ms, mfu 0.08%\n",
      "iter 840: loss 5.5221, time 1170.26ms, mfu 0.08%\n",
      "iter 850: loss 5.5378, time 1161.66ms, mfu 0.09%\n",
      "iter 860: loss 5.5771, time 1579.32ms, mfu 0.08%\n",
      "iter 870: loss 5.7614, time 1159.59ms, mfu 0.08%\n",
      "iter 880: loss 5.5954, time 1181.55ms, mfu 0.08%\n",
      "iter 890: loss 5.4936, time 1167.87ms, mfu 0.08%\n",
      "step 900: train loss 5.3772, val loss 5.6615\n",
      "saving checkpoint to out\n",
      "iter 900: loss 5.3159, time 6840.12ms, mfu 0.08%\n",
      "iter 910: loss 4.9055, time 1142.12ms, mfu 0.09%\n",
      "iter 920: loss 5.3481, time 1277.14ms, mfu 0.09%\n",
      "iter 930: loss 5.0358, time 1214.86ms, mfu 0.09%\n",
      "iter 940: loss 5.2205, time 1183.33ms, mfu 0.09%\n",
      "iter 950: loss 5.4086, time 1215.83ms, mfu 0.08%\n",
      "iter 960: loss 4.8262, time 1222.64ms, mfu 0.08%\n",
      "iter 970: loss 4.7180, time 1160.85ms, mfu 0.08%\n",
      "iter 980: loss 4.9738, time 1163.50ms, mfu 0.09%\n",
      "iter 990: loss 5.4198, time 1183.17ms, mfu 0.09%\n",
      "step 1000: train loss 5.0562, val loss 5.4414\n",
      "saving checkpoint to out\n",
      "iter 1000: loss 5.0476, time 7234.16ms, mfu 0.08%\n",
      "iter 1010: loss 5.0722, time 1244.52ms, mfu 0.08%\n",
      "iter 1020: loss 4.8300, time 2668.37ms, mfu 0.07%\n",
      "iter 1030: loss 4.7152, time 1639.51ms, mfu 0.06%\n",
      "iter 1040: loss 4.8444, time 1508.03ms, mfu 0.06%\n",
      "iter 1050: loss 4.5738, time 2037.55ms, mfu 0.06%\n",
      "iter 1060: loss 4.8372, time 1618.94ms, mfu 0.06%\n",
      "iter 1070: loss 5.1753, time 1407.17ms, mfu 0.06%\n",
      "iter 1080: loss 4.6521, time 1466.06ms, mfu 0.06%\n",
      "iter 1090: loss 5.2261, time 1593.58ms, mfu 0.06%\n",
      "step 1100: train loss 4.8143, val loss 5.3336\n",
      "saving checkpoint to out\n",
      "iter 1100: loss 5.1456, time 10635.12ms, mfu 0.06%\n",
      "iter 1110: loss 4.5120, time 1363.34ms, mfu 0.06%\n",
      "iter 1120: loss 4.8348, time 1573.41ms, mfu 0.06%\n",
      "iter 1130: loss 4.7289, time 1551.22ms, mfu 0.06%\n",
      "iter 1140: loss 4.8373, time 2175.51ms, mfu 0.06%\n",
      "iter 1150: loss 4.4710, time 1680.18ms, mfu 0.06%\n",
      "iter 1160: loss 4.8602, time 1572.19ms, mfu 0.06%\n",
      "iter 1170: loss 4.4825, time 1517.14ms, mfu 0.06%\n",
      "iter 1180: loss 4.5940, time 1694.18ms, mfu 0.06%\n",
      "iter 1190: loss 4.8289, time 1633.42ms, mfu 0.06%\n",
      "step 1200: train loss 4.6097, val loss 5.3167\n",
      "saving checkpoint to out\n",
      "iter 1200: loss 4.5388, time 10004.82ms, mfu 0.06%\n",
      "iter 1210: loss 4.9489, time 1543.05ms, mfu 0.07%\n",
      "iter 1220: loss 4.5471, time 1501.31ms, mfu 0.07%\n",
      "iter 1230: loss 4.7139, time 1543.82ms, mfu 0.06%\n",
      "iter 1240: loss 4.3576, time 1699.14ms, mfu 0.06%\n",
      "iter 1250: loss 4.6710, time 1605.56ms, mfu 0.06%\n",
      "iter 1260: loss 4.5429, time 1490.51ms, mfu 0.06%\n",
      "iter 1270: loss 4.6785, time 1701.86ms, mfu 0.06%\n",
      "iter 1280: loss 4.7340, time 1545.64ms, mfu 0.06%\n",
      "iter 1290: loss 4.3942, time 1514.78ms, mfu 0.06%\n",
      "step 1300: train loss 4.4546, val loss 5.3019\n",
      "saving checkpoint to out\n",
      "iter 1300: loss 4.2813, time 9922.24ms, mfu 0.06%\n",
      "iter 1310: loss 4.4762, time 1574.78ms, mfu 0.06%\n",
      "iter 1320: loss 4.5893, time 1639.27ms, mfu 0.06%\n",
      "iter 1330: loss 4.5217, time 1453.32ms, mfu 0.07%\n",
      "iter 1340: loss 4.0211, time 1444.72ms, mfu 0.06%\n",
      "iter 1350: loss 4.1120, time 1978.35ms, mfu 0.06%\n",
      "iter 1360: loss 4.2269, time 1457.00ms, mfu 0.06%\n",
      "iter 1370: loss 4.1310, time 1553.06ms, mfu 0.06%\n",
      "iter 1380: loss 4.0942, time 1775.01ms, mfu 0.06%\n",
      "iter 1390: loss 4.7859, time 1565.25ms, mfu 0.06%\n",
      "step 1400: train loss 4.3240, val loss 5.2060\n",
      "saving checkpoint to out\n",
      "iter 1400: loss 4.4202, time 11889.22ms, mfu 0.05%\n",
      "iter 1410: loss 4.6434, time 1436.10ms, mfu 0.06%\n",
      "iter 1420: loss 4.4579, time 1407.35ms, mfu 0.06%\n",
      "iter 1430: loss 4.3041, time 2732.98ms, mfu 0.06%\n",
      "iter 1440: loss 4.5983, time 1341.46ms, mfu 0.07%\n",
      "iter 1450: loss 4.0150, time 1254.12ms, mfu 0.07%\n",
      "iter 1460: loss 4.5778, time 1137.71ms, mfu 0.07%\n",
      "iter 1470: loss 4.3201, time 1178.93ms, mfu 0.08%\n",
      "iter 1480: loss 4.1566, time 1442.71ms, mfu 0.07%\n",
      "iter 1490: loss 4.3275, time 1213.20ms, mfu 0.08%\n",
      "step 1500: train loss 4.2442, val loss 5.3308\n",
      "saving checkpoint to out\n",
      "iter 1500: loss 4.5724, time 7037.15ms, mfu 0.07%\n",
      "iter 1510: loss 4.4967, time 1188.58ms, mfu 0.08%\n",
      "iter 1520: loss 4.2002, time 1186.04ms, mfu 0.08%\n",
      "iter 1530: loss 4.1523, time 1232.12ms, mfu 0.08%\n",
      "iter 1540: loss 4.0108, time 1184.03ms, mfu 0.08%\n",
      "iter 1550: loss 4.3619, time 1160.53ms, mfu 0.09%\n",
      "iter 1560: loss 4.1307, time 1668.32ms, mfu 0.08%\n",
      "iter 1570: loss 4.6176, time 1311.60ms, mfu 0.08%\n",
      "iter 1580: loss 3.9085, time 1189.11ms, mfu 0.08%\n",
      "iter 1590: loss 3.5812, time 1215.78ms, mfu 0.08%\n",
      "step 1600: train loss 4.1235, val loss 5.3130\n",
      "saving checkpoint to out\n",
      "iter 1600: loss 4.1787, time 7051.98ms, mfu 0.08%\n",
      "iter 1610: loss 4.1047, time 1864.51ms, mfu 0.07%\n",
      "iter 1620: loss 4.1953, time 1341.31ms, mfu 0.08%\n",
      "iter 1630: loss 4.3418, time 2142.40ms, mfu 0.07%\n",
      "iter 1640: loss 3.9372, time 2166.83ms, mfu 0.08%\n",
      "iter 1650: loss 4.0855, time 2207.94ms, mfu 0.08%\n",
      "iter 1660: loss 4.0148, time 1146.33ms, mfu 0.08%\n",
      "iter 1670: loss 4.0996, time 1176.04ms, mfu 0.08%\n",
      "iter 1680: loss 3.8722, time 1185.73ms, mfu 0.08%\n",
      "iter 1690: loss 4.6380, time 1188.26ms, mfu 0.08%\n",
      "step 1700: train loss 4.0151, val loss 5.3564\n",
      "saving checkpoint to out\n",
      "iter 1700: loss 3.5799, time 7353.21ms, mfu 0.08%\n",
      "iter 1710: loss 3.9139, time 1312.86ms, mfu 0.08%\n",
      "iter 1720: loss 4.7254, time 1210.42ms, mfu 0.08%\n",
      "iter 1730: loss 3.6034, time 1179.48ms, mfu 0.08%\n",
      "iter 1740: loss 4.1412, time 1266.68ms, mfu 0.08%\n",
      "iter 1750: loss 3.8908, time 1157.52ms, mfu 0.08%\n",
      "iter 1760: loss 3.6704, time 1175.61ms, mfu 0.09%\n",
      "iter 1770: loss 4.2616, time 1243.27ms, mfu 0.08%\n",
      "iter 1780: loss 4.1303, time 1188.07ms, mfu 0.08%\n",
      "iter 1790: loss 3.9533, time 1231.62ms, mfu 0.08%\n",
      "step 1800: train loss 3.9516, val loss 5.4074\n",
      "saving checkpoint to out\n",
      "iter 1800: loss 3.7546, time 7601.62ms, mfu 0.08%\n",
      "iter 1810: loss 4.0990, time 1125.19ms, mfu 0.09%\n",
      "iter 1820: loss 3.7531, time 1187.84ms, mfu 0.09%\n",
      "iter 1830: loss 3.9869, time 1184.68ms, mfu 0.08%\n",
      "iter 1840: loss 4.1403, time 1191.97ms, mfu 0.08%\n",
      "iter 1850: loss 3.7423, time 1183.25ms, mfu 0.08%\n",
      "iter 1860: loss 3.6793, time 1144.47ms, mfu 0.08%\n",
      "iter 1870: loss 4.1307, time 1193.00ms, mfu 0.08%\n",
      "iter 1880: loss 3.6384, time 1197.51ms, mfu 0.08%\n",
      "iter 1890: loss 3.7642, time 1171.97ms, mfu 0.09%\n",
      "step 1900: train loss 3.8985, val loss 5.4587\n",
      "saving checkpoint to out\n",
      "iter 1900: loss 3.9201, time 6948.90ms, mfu 0.08%\n",
      "iter 1910: loss 3.7731, time 1130.65ms, mfu 0.08%\n",
      "iter 1920: loss 4.0470, time 1172.64ms, mfu 0.08%\n",
      "iter 1930: loss 4.1050, time 1195.77ms, mfu 0.08%\n",
      "iter 1940: loss 3.7134, time 1208.23ms, mfu 0.08%\n",
      "iter 1950: loss 3.3925, time 1220.04ms, mfu 0.08%\n",
      "iter 1960: loss 4.0789, time 1161.80ms, mfu 0.08%\n",
      "iter 1970: loss 3.9033, time 1194.86ms, mfu 0.08%\n",
      "iter 1980: loss 3.6527, time 1338.62ms, mfu 0.08%\n",
      "iter 1990: loss 3.6450, time 1231.92ms, mfu 0.08%\n",
      "step 2000: train loss 3.8101, val loss 5.4882\n",
      "saving checkpoint to out\n",
      "iter 2000: loss 3.9078, time 8232.58ms, mfu 0.07%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import GPTConfig, GPT\n",
    "# pylint: disable=invalid-name\n",
    "\n",
    "# parameters\n",
    "out_dir = 'out'\n",
    "eval_interval = 100\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "always_save_checkpoint = True # save a checkpoint after each eval_interval\n",
    "\n",
    "# data\n",
    "dataset = 'data' # directory where the data is stored\n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 32 # content window size\n",
    "\n",
    "# model\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "n_embd = 64\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 2000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# system\n",
    "device = 'mps' # examples: 'cpu', 'cuda', or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16'\n",
    "\n",
    "# capture settings / parameters to save in model checkpoint\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys} \n",
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "seed = 1337\n",
    "tokens_per_iter = gradient_accumulation_steps * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "# place to store the model we create (checkpoint)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# set the random seed\n",
    "torch.manual_seed(1337)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# Load the data\n",
    "data_dir = 'data/'\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "# get a batch from the data\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# init\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
    "\n",
    "# check to make sure checkpoint does not exist\n",
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "\n",
    "# init a new model from scratch\n",
    "print(\"Initializing a new model from scratch\")\n",
    "print(\"Using vocab_size of GPT-2 of 50304 (50257 rounded up for efficiency)\")\n",
    "model_args['vocab_size'] = 50304\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "model.to(device)\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "# compile the model\n",
    "print(\"compiling the model... (takes a ~minute)\")\n",
    "unoptimized_model = model\n",
    "#model = torch.compile(model) # requires PyTorch 2.0\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# training loop\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model\n",
    "running_mfu = -1.0\n",
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 or iter_num >= max_iters:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        if iter_num % 10 == 0:\n",
    "            print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419183b-a46e-47f7-94c0-9533e3e7c494",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1722e6d5-8709-4f6c-83a1-31aac3cc363b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3.42M\n",
      "\n",
      "\n",
      "CORIOLANUS:\n",
      "If you had he that have it rain be is that\n",
      "Will?\n",
      "\n",
      "CLARENCE:\n",
      "Mised! what of the house of me? 'emfore fals!\n",
      "\n",
      "MERCUTIO:\n",
      "We stay no end!' why! Master Froth art and let him\n",
      "To the grave or I have done very foul fair;\n",
      "The day-ple of this mother's life, I'll\n",
      "Desinks not a woman you with me, but this way\n",
      "Appear in his native part's wife not the king,\n",
      "Which now you not the field-vowies of him?\n",
      "\n",
      "OXFORD:\n",
      "I think you put on she, a very man of thee.\n",
      "\n",
      "Clown:\n",
      "O, go; and, I would come there find you not;\n",
      "Than you will both sit, as she'st upon you.\n",
      "\n",
      "ANTIGONET:\n",
      "What, I will be true.\n",
      "\n",
      "BISHOP OF YORK:\n",
      "Be not, if you fight, or when she's hate along,\n",
      "I be a crown here but thy ancient ground.\n",
      "Godable that I have to-morrow, and give me you.\n",
      "\n",
      "Messillo, I am so:\n",
      "Tis my my brows?\n",
      "\n",
      "ROMEO:\n",
      "\n",
      "POMPEY:\n",
      "Ay, nor his tender. Thou, therefore, ere not you thought.\n",
      "\n",
      "JULtis the time; it is as ducisbury\n",
      "If to thy wife, like this now long.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "To be a first deed: at the loss of Marcius,\n",
      "Were I not love not what thou to give?\n",
      "\n",
      "MAMILLO:\n",
      "'Mourfore him, sir, my brother's war's brother!\n",
      "\n",
      "WARWICK:\n",
      "Hence.\n",
      "\n",
      "RATMENASTINGS:\n",
      "Master, now you, for me well: I'll fly in himself.\n",
      "\n",
      "Third Citizen:\n",
      "O, I am I hither, sir.\n",
      "\n",
      "ELY ANNE:\n",
      "On my uncle Clifford, go: if you find her blood\n",
      "Turn it.\n",
      " nor the time have so bloody with them, your honates;\n",
      "\n",
      "At York is yesternight for thee, the loss of them.\n",
      "\n",
      "PAULINA:\n",
      "Who hath as my own true lordsKE OF\n"
     ]
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# inference parameters\n",
    "max_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'mps'\n",
    "dtype = 'bfloat16'\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# load model saved in a specific directory\n",
    "ckpt_path = 'out/ckpt.pt'\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# set prompt\n",
    "prompt = \"\\n\"\n",
    "start_ids = enc.encode(prompt, allowed_special={\"<|endoftext|>\"})\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        y = model.generate(x, max_tokens, temperature=temperature, top_k=top_k)\n",
    "        output = y[0].tolist()\n",
    "        for w in output:\n",
    "            if w > 50257: # max token value, ignore the rest\n",
    "                continue\n",
    "            else:\n",
    "                text = enc.decode([w])\n",
    "                if text == '\\n':\n",
    "                    print()\n",
    "                else:\n",
    "                    print(text, end='')\n",
    "                \n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ea0bc-5884-4e52-9e40-e5d69767ea05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
