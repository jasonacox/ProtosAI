{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62a6ebe8-454f-4f5a-bcdb-6ed1132625c7",
   "metadata": {
    "collapsed": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00474b55-e1dd-4266-bd5e-1a73e4ddfb29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Generative Artificial Intelligence\n",
    "\n",
    "ChatGPT, Claude, Gemini, and CoPilot are all amazing examples of Generative AI technology powered by Large Language Models. But how does it actually work? Let's take a look behind the scenes and into the code of this amazing new technology.\n",
    "\n",
    "This notebook walks you through creating your own Generative Pretrained Transformer (GPT) language model. First we explore the background science, then we will look at tokens, the atomic units of these large language models. We will then move on to data preparation, training a model and finally, generating text with the model.\n",
    "\n",
    "## Background\n",
    "\n",
    "To understand GPTs, we need to understand the foundational science: the Neuron.\n",
    "\n",
    "<img width=\"640\" alt=\"image\" src=\"https://github.com/user-attachments/assets/483305da-6c02-4532-8522-255f64b38093\">\n",
    "\n",
    "The intricate dance of synaptic connections and neurotransmitters is a key aspect of neural networks, and understanding these mechanisms has inspired the development of artificial neural networks, which mimic the behavior of biological neurons to process and learn from complex data.\n",
    "\n",
    "## Mathematical Representation of a Neuron\n",
    "\n",
    "![axon from a neuron](https://github.com/user-attachments/assets/62da7697-fc31-4670-a2ef-69c1ae4d2aa9)\n",
    "\n",
    "Multiple inputs (x) are amplified or attenuated (product) by the synaptic weights (w) before being combined (summed) inside the cell body. The cell itself has a bias (b) that gets added to that resulting signal. This signal is then passed through an activation function (f) to determine if the signal will be passed on to other neurons (output). Activation functions in neural networks introduce non-linearity, enabling the network to learn complex patterns. Common activation functions include ReLU, Sigmoid, Tanh, and Softmax. The hyperbolic tangent (`tanh()`) provides a zero-centered output and smooth gradient and is often used in hidden layers of a neural net. Softmax is often used in the output layer of a neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82034b0b-cdf3-4f90-a339-9c419b9092dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Activation Function\n",
    "plt.plot(np.arange(-5,5,0.2), np.tanh(np.arange(-5,5,0.2))); plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a017e6-9374-4e19-b140-9d2799183061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d994812c-53b8-4eda-8d93-e0f53354dd2e",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "![output layer](https://github.com/user-attachments/assets/50033790-4164-4154-8062-aaeef0d56df6)\n",
    "\n",
    "### How to train a Neural Network\n",
    "\n",
    "#### 1. Forward Propagation\n",
    "- Input data is fed into the input layer.\n",
    "- The data is then passed through each layer, from input to hidden to output.\n",
    "- At each neuron, the input data is multiplied by the corresponding weights, added to the bias, and passed through the activation function to produce the output: $ z = W \\cdot x + b $\n",
    "- This process continues until the data reaches the output layer, producing the final output of the network.\n",
    "\n",
    "#### 2. Compute Loss\n",
    "- After froward propagation, the loss is calculated to deterimne the accuracy of the current prediction.\n",
    "- The _loss function_ measures the difference between the predicted output and the desired target values.\n",
    "- Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.\n",
    "\n",
    "#### 3. Backpropagation\n",
    "- Once the loss is knows, backpropagation is performed to adjust the weights and biases to minimize the loss.\n",
    "- This is perfomred by calculating the gradient of the loss function with respect to each weight and bias using the chain rule.\n",
    "- The gradients are then used to update the weights and biases in the opposite direction of the gradient (by a *learning rate* amount), a process known as _gradient descent_.\n",
    "\n",
    "#### 4. Repeat\n",
    "- Forward propagation and backpropagation will be performed over many iterations, adjusting the weights and biases each time to minimize the loss.\n",
    "- Training continues until the loss converges to a minimum value or a specified number of iterations (epochs) is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef6a9e",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "Building upon the biological and mathematical representation of the neuron, the Transformer architecture has emerged as a game-changer in the field of natural language processing. The transformer has given rise to the GenAI movement, a new era of artificial intelligence that is transforming industries and redefining the boundaries of human-machine collaboration.\n",
    "\n",
    "<img width=\"706\" alt=\"Transformers\" src=\"https://github.com/user-attachments/assets/f03692ed-7b3b-484f-b41b-b221376cd5a0\">\n",
    "\n",
    "Source : https://www.coursera.org/learn/generative-ai-with-llms\n",
    "\n",
    "The Transformer architecture was first introduced in the groundbreaking paper \"[Attention is All You Need](https://arxiv.org/pdf/1706.03762v1)\" by Vaswani et al. in 2017. This seminal work proposed a novel approach to sequence-to-sequence learning, which revolutionized the field of natural language processing (NLP) and beyond. The authors challenged the traditional encoder-decoder architecture, which relied on recurrent neural networks (RNNs) or convolutional neural networks (CNNs) to process sequential data. Instead, they proposed a purely attention-based model, where the encoder and decoder are composed of identical layers, and the attention mechanism is used to weigh the importance of different input elements when computing the output.\n",
    "\n",
    "<img width=\"485\" alt=\"Transformer Architecture\" src=\"https://github.com/user-attachments/assets/d1a81768-1a62-4d86-b48b-51c27ca80aa7\">\n",
    "\n",
    "\n",
    "Example Transation:\n",
    "\n",
    "\n",
    "<img width=\"640\" alt=\"Translation Example\" src=\"https://github.com/user-attachments/assets/f722b1ba-4660-4782-8ece-aa422bea8a7c\">\n",
    "\n",
    "<img width=\"640\" alt=\"Decoder\" src=\"https://jalammar.github.io/images/t/transformer_decoding_2.gif\">\n",
    "\n",
    "Source: https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "\n",
    "The Transformer architecture relies on self-attention, a mechanism that allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This is achieved through a series of linear transformations, which compute the attention weights, and a softmax function, which normalizes these weights to ensure they sum to 1. The attention weights are then used to compute the weighted sum of the input elements, which is used as input to the next layer. This process is repeated multiple times, allowing the model to capture long-range dependencies and contextual relationships in the input sequence. The Transformer architecture has since become a cornerstone of modern NLP, and its variants have been applied to a wide range of tasks, including machine translation, text classification, and question answering.\n",
    "\n",
    "![animal_](https://github.com/user-attachments/assets/20aeade6-b261-4f82-a818-bf2ed005dd6e)\n",
    "\n",
    "\n",
    "<img width=\"800\" alt=\"image\" src=\"https://github.com/user-attachments/assets/c5ff56ca-f985-4c92-9588-c845b05fbf7f\">\n",
    "\n",
    "Source: Visualizing Attention, a Transformer's Heart - https://www.3blue1brown.com/lessons/attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2df349",
   "metadata": {},
   "source": [
    "## All about Tokens\n",
    "\n",
    "Training an LLM on natural language requires that we convert the language to numeric \"signals\" to feed into the neural net. These are called tokens. These tokens can be characters, words or even parts of words. They are the atomic components of language.\n",
    "\n",
    "<img width=\"532\" alt=\"image\" src=\"https://github.com/user-attachments/assets/bb90fbe5-78ec-45b3-9678-6a95c9baa386\">\n",
    "\n",
    "Source: https://platform.openai.com/tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d71352-adf5-4fd5-a4a7-68c1c8a65e19",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First some dependencies need to be installed\n",
    "!pip install tiktoken\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install tqdm\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef6ecee2-0261-49da-8085-053bcda02680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464, 2068, 7586, 21831, 11687, 625, 262, 16931, 3013, 3255, 3290, 13, 50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "phrase = \"The quick brown fox jumped over the lazy snoring dog.\"\n",
    "tokens = enc.encode_ordinary(phrase)\n",
    "# Add end of text token\n",
    "tokens.append(enc.eot_token)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "defa1f80-eafe-4c86-a90b-e80420e1ae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The\", \" quick\", \" brown\", \" fox\", \" jumped\", \" over\", \" the\", \" lazy\", \" sn\", \"oring\", \" dog\", \".\", \"<|endoftext|>\", "
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(f'\"{enc.decode([token])}\", ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f31f8c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The -> \" quick\"\n",
      "The| quick -> \" brown\"\n",
      "The| quick| brown -> \" fox\"\n",
      "The| quick| brown| fox -> \" jumped\"\n",
      "The| quick| brown| fox| jumped -> \" over\"\n",
      "The| quick| brown| fox| jumped| over -> \" the\"\n",
      "The| quick| brown| fox| jumped| over| the -> \" lazy\"\n",
      "The| quick| brown| fox| jumped| over| the| lazy -> \" sn\"\n",
      "The| quick| brown| fox| jumped| over| the| lazy| sn -> \"oring\"\n",
      "The| quick| brown| fox| jumped| over| the| lazy| sn|oring -> \" dog\"\n",
      "The| quick| brown| fox| jumped| over| the| lazy| sn|oring| dog -> \".\"\n",
      "The| quick| brown| fox| jumped| over| the| lazy| sn|oring| dog|. -> \"<|endoftext|>\"\n"
     ]
    }
   ],
   "source": [
    "# The training target is the next token in the sequence\n",
    "predictions = tokens[:-1]\n",
    "targets = tokens[1:] + [enc.encode_ordinary(\"\")]\n",
    "for x in range(len(predictions)):\n",
    "    print('|'.join([f'{enc.decode([p])}' for p in predictions[:x+1]]), end='')\n",
    "    print(f' -> \"{enc.decode([targets[x]])}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697ffca-c9d8-44a1-9d18-e595d18c1a5f",
   "metadata": {},
   "source": [
    "# Tokenize Training Data\n",
    "\n",
    "We will use the Tiny Shakespeare dataset to train a model. See the [shakespeare.txt](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) file.\n",
    "\n",
    "Divide data into two parts:\n",
    "- 90% Training\n",
    "- 10% Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbe64b04-e900-4a84-9792-fdbe426049a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 301,966 tokens\n",
      "val has 36,059 tokens\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "# tiny shakespeare dataset\n",
    "input_file_path = 'data/shakespeare.txt'\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "n = len(data)\n",
    "\n",
    "# Use 90% for training and 10% for validation\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode with tiktoken gpt2\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile('data/train.bin')\n",
    "val_ids.tofile('data/val.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7806ff78-2c68-418b-8877-a2b3d05bc059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5962, 22307,    25, ...,   508,  2058,   994], dtype=uint16)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_ids))\n",
    "train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54a3c098-51e9-44eb-96dd-002ff11a31a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved.\n"
     ]
    }
   ],
   "source": [
    "print(enc.decode(train_ids[:50]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "169c1d42-47b0-4a6b-9b5e-76ee74e5756b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# GPT Model Class\n",
    "\n",
    "We will use the `model.py` file contains a GPT model class that was created by Andrej Karpathy. It was modeled after the one created by OpenAI.\n",
    "\n",
    "GPT, as the name implies, is based on the Transformer technology presented by Google in the paper, \"Attention Is All You Need.\" https://arxiv.org/pdf/1706.03762\n",
    "\n",
    "References:\n",
    "* Great Video by Andrej Karpathy - _Let's build GPT: from scratch, in code, spelled out._ - https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "* nanoGPT - https://github.com/karpathy/nanoGPT\n",
    "* OpenAI GPT-2 - https://github.com/openai/gpt-2/blob/master/src/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a50c5c-2be8-4c98-b40f-ef90bab2516c",
   "metadata": {},
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2feda256-bd43-4879-a7f8-76bf62c031ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 1000 # total number of training iterations\n",
    "checkpoint_file = f'model{max_iters}.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "213251e0-26e9-4461-a0ba-07d73edb17c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 15,360\n",
      "Initializing a new model from scratch\n",
      "Using vocab_size of GPT-2 of 50304 (50257 rounded up for efficiency)\n",
      "Number of parameters: 3.42M\n",
      "num decayed parameter tensors: 18, with 3,418,112 parameters\n",
      "num non-decayed parameter tensors: 9, with 576 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "step 0: train loss 10.8197, val loss 10.8165\n",
      "iter 0: loss 10.8136, time 7005.80ms, mfu -100.00%\n",
      "iter 10: loss 10.8195, time 224.41ms, mfu 0.72%\n",
      "iter 20: loss 10.7911, time 223.81ms, mfu 0.60%\n",
      "iter 30: loss 10.8095, time 238.65ms, mfu 0.49%\n",
      "iter 40: loss 10.7644, time 228.39ms, mfu 0.48%\n",
      "iter 50: loss 10.7265, time 228.26ms, mfu 0.46%\n",
      "iter 60: loss 10.6919, time 166.62ms, mfu 0.47%\n",
      "iter 70: loss 10.6685, time 228.54ms, mfu 0.46%\n",
      "iter 80: loss 10.5965, time 228.65ms, mfu 0.45%\n",
      "iter 90: loss 10.5562, time 228.63ms, mfu 0.47%\n",
      "step 100: train loss 10.5333, val loss 10.5307\n",
      "saving checkpoint to model1000.pt\n",
      "iter 100: loss 10.5423, time 654.47ms, mfu 0.42%\n",
      "iter 110: loss 10.4646, time 114.17ms, mfu 0.71%\n",
      "iter 120: loss 10.4458, time 115.84ms, mfu 0.82%\n",
      "iter 130: loss 10.4100, time 114.76ms, mfu 0.86%\n",
      "iter 140: loss 10.3602, time 115.22ms, mfu 0.87%\n",
      "iter 150: loss 10.3366, time 114.53ms, mfu 0.88%\n",
      "iter 160: loss 10.2838, time 115.80ms, mfu 0.88%\n",
      "iter 170: loss 10.2114, time 114.42ms, mfu 0.88%\n",
      "iter 180: loss 10.1724, time 115.25ms, mfu 0.88%\n",
      "iter 190: loss 10.0836, time 115.61ms, mfu 0.88%\n",
      "step 200: train loss 10.0635, val loss 10.0735\n",
      "saving checkpoint to model1000.pt\n",
      "iter 200: loss 10.0685, time 899.63ms, mfu 0.80%\n",
      "iter 210: loss 9.9852, time 121.03ms, mfu 0.69%\n",
      "iter 220: loss 9.9390, time 127.05ms, mfu 0.76%\n",
      "iter 230: loss 9.8716, time 125.34ms, mfu 0.78%\n",
      "iter 240: loss 9.7912, time 110.66ms, mfu 0.86%\n",
      "iter 250: loss 9.7255, time 112.28ms, mfu 0.89%\n",
      "iter 260: loss 9.6378, time 111.23ms, mfu 0.90%\n",
      "iter 270: loss 9.5634, time 112.10ms, mfu 0.90%\n",
      "iter 280: loss 9.4627, time 112.63ms, mfu 0.90%\n",
      "iter 290: loss 9.4203, time 112.08ms, mfu 0.90%\n",
      "step 300: train loss 9.3315, val loss 9.3643\n",
      "saving checkpoint to model1000.pt\n",
      "iter 300: loss 9.3381, time 756.55ms, mfu 0.82%\n",
      "iter 310: loss 9.2109, time 113.67ms, mfu 0.87%\n",
      "iter 320: loss 9.1329, time 112.67ms, mfu 0.88%\n",
      "iter 330: loss 9.1095, time 113.10ms, mfu 0.89%\n",
      "iter 340: loss 8.9449, time 112.88ms, mfu 0.89%\n",
      "iter 350: loss 8.9057, time 113.32ms, mfu 0.90%\n",
      "iter 360: loss 8.7613, time 112.31ms, mfu 0.89%\n",
      "iter 370: loss 8.7341, time 111.60ms, mfu 0.90%\n",
      "iter 380: loss 8.6406, time 112.47ms, mfu 0.90%\n",
      "iter 390: loss 8.5324, time 111.30ms, mfu 0.91%\n",
      "step 400: train loss 8.4177, val loss 8.4927\n",
      "saving checkpoint to model1000.pt\n",
      "iter 400: loss 8.4186, time 748.63ms, mfu 0.83%\n",
      "iter 410: loss 8.2859, time 111.87ms, mfu 0.88%\n",
      "iter 420: loss 8.2237, time 111.83ms, mfu 0.90%\n",
      "iter 430: loss 8.1758, time 110.70ms, mfu 0.91%\n",
      "iter 440: loss 8.0503, time 112.29ms, mfu 0.91%\n",
      "iter 450: loss 7.9898, time 110.82ms, mfu 0.90%\n",
      "iter 460: loss 7.9666, time 112.32ms, mfu 0.91%\n",
      "iter 470: loss 7.9061, time 111.72ms, mfu 0.91%\n",
      "iter 480: loss 7.8556, time 112.33ms, mfu 0.91%\n",
      "iter 490: loss 7.5705, time 111.84ms, mfu 0.91%\n",
      "step 500: train loss 7.5247, val loss 7.6170\n",
      "saving checkpoint to model1000.pt\n",
      "iter 500: loss 7.6341, time 806.64ms, mfu 0.82%\n",
      "iter 510: loss 7.4326, time 227.60ms, mfu 0.58%\n",
      "iter 520: loss 7.2717, time 111.67ms, mfu 0.65%\n",
      "iter 530: loss 7.2138, time 111.93ms, mfu 0.82%\n",
      "iter 540: loss 7.2338, time 112.10ms, mfu 0.87%\n",
      "iter 550: loss 7.1282, time 113.05ms, mfu 0.89%\n",
      "iter 560: loss 7.1733, time 111.90ms, mfu 0.90%\n",
      "iter 570: loss 6.9770, time 112.12ms, mfu 0.90%\n",
      "iter 580: loss 7.0941, time 115.11ms, mfu 0.89%\n",
      "iter 590: loss 6.8452, time 114.52ms, mfu 0.89%\n",
      "step 600: train loss 6.7724, val loss 6.8880\n",
      "saving checkpoint to model1000.pt\n",
      "iter 600: loss 6.7801, time 763.71ms, mfu 0.81%\n",
      "iter 610: loss 6.7559, time 114.78ms, mfu 0.84%\n",
      "iter 620: loss 6.6330, time 114.40ms, mfu 0.87%\n",
      "iter 630: loss 6.6342, time 113.91ms, mfu 0.88%\n",
      "iter 640: loss 6.5238, time 114.19ms, mfu 0.88%\n",
      "iter 650: loss 6.3210, time 121.02ms, mfu 0.86%\n",
      "iter 660: loss 6.4468, time 112.92ms, mfu 0.88%\n",
      "iter 670: loss 6.3641, time 113.67ms, mfu 0.89%\n",
      "iter 680: loss 6.1576, time 113.90ms, mfu 0.89%\n",
      "iter 690: loss 6.1303, time 112.85ms, mfu 0.89%\n",
      "step 700: train loss 6.2155, val loss 6.3516\n",
      "saving checkpoint to model1000.pt\n",
      "iter 700: loss 6.3696, time 820.32ms, mfu 0.80%\n",
      "iter 710: loss 6.2559, time 228.12ms, mfu 0.57%\n",
      "iter 720: loss 5.9392, time 227.89ms, mfu 0.51%\n",
      "iter 730: loss 6.0300, time 228.29ms, mfu 0.47%\n",
      "iter 740: loss 6.1703, time 160.71ms, mfu 0.48%\n",
      "iter 750: loss 5.8877, time 228.53ms, mfu 0.47%\n",
      "iter 760: loss 5.9547, time 228.23ms, mfu 0.45%\n",
      "iter 770: loss 6.2852, time 227.24ms, mfu 0.47%\n",
      "iter 780: loss 5.7593, time 228.26ms, mfu 0.45%\n",
      "iter 790: loss 5.6941, time 228.51ms, mfu 0.48%\n",
      "step 800: train loss 5.7416, val loss 5.9647\n",
      "saving checkpoint to model1000.pt\n",
      "iter 800: loss 5.7217, time 828.46ms, mfu 0.43%\n",
      "iter 810: loss 5.6695, time 116.53ms, mfu 0.55%\n",
      "iter 820: loss 5.9867, time 114.59ms, mfu 0.77%\n",
      "iter 830: loss 5.5081, time 114.80ms, mfu 0.84%\n",
      "iter 840: loss 5.5274, time 114.50ms, mfu 0.87%\n",
      "iter 850: loss 5.5402, time 115.30ms, mfu 0.88%\n",
      "iter 860: loss 5.5787, time 113.13ms, mfu 0.88%\n",
      "iter 870: loss 5.7574, time 112.61ms, mfu 0.89%\n",
      "iter 880: loss 5.5911, time 113.72ms, mfu 0.89%\n",
      "iter 890: loss 5.5019, time 113.28ms, mfu 0.89%\n",
      "step 900: train loss 5.3757, val loss 5.6669\n",
      "saving checkpoint to model1000.pt\n",
      "iter 900: loss 5.3103, time 823.80ms, mfu 0.81%\n",
      "iter 910: loss 4.9109, time 227.62ms, mfu 0.57%\n",
      "iter 920: loss 5.3416, time 126.73ms, mfu 0.55%\n",
      "iter 930: loss 5.0324, time 114.22ms, mfu 0.76%\n",
      "iter 940: loss 5.2126, time 114.70ms, mfu 0.84%\n",
      "iter 950: loss 5.4013, time 118.69ms, mfu 0.87%\n",
      "iter 960: loss 4.8180, time 114.98ms, mfu 0.87%\n",
      "iter 970: loss 4.7105, time 114.30ms, mfu 0.88%\n",
      "iter 980: loss 4.9646, time 113.61ms, mfu 0.89%\n",
      "iter 990: loss 5.4102, time 112.65ms, mfu 0.89%\n",
      "step 1000: train loss 5.0461, val loss 5.4386\n",
      "saving checkpoint to model1000.pt\n",
      "iter 1000: loss 5.0347, time 857.63ms, mfu 0.82%\n",
      "saving checkpoint to model1000.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# parameters\n",
    "eval_interval = 100\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "always_save_checkpoint = True # save a checkpoint after each eval_interval\n",
    "\n",
    "# data\n",
    "dataset = 'data' # directory where the data is stored\n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
    "batch_size = 12 # [12] if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 32 # [32] content window size\n",
    "\n",
    "# model\n",
    "n_layer = 4 # 4 hidden layers\n",
    "n_head = 4  # 4 heads\n",
    "n_embd = 64 # 64 \n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16'\n",
    "\n",
    "# capture above settings & parameters to save in model checkpoint\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys} \n",
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "seed = 4212\n",
    "tokens_per_iter = gradient_accumulation_steps * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "# set the random seed\n",
    "torch.manual_seed(1337)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# Load the data\n",
    "data_dir = 'data/'\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "# get a batch from the data\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# loop counters starting point\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# init a new model from scratch\n",
    "print(\"Initializing a new model from scratch\")\n",
    "print(\"Using vocab_size of GPT-2 of 50304 (50257 rounded up for efficiency)\")\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=50304, dropout=dropout) \n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "model.to(device)\n",
    "\n",
    "# initialize a GradScaler\n",
    "#scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "scaler = torch.amp.GradScaler(device_type, enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "# compile the model if not using MPS\n",
    "if device == 'mps':\n",
    "    print(\"MPS doesn't support JIT compilation, skipping...\")\n",
    "else:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    model = torch.compile(model) # requires PyTorch 2.0\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    \n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# save model to checkpoint file\n",
    "def save_model(fn):\n",
    "    checkpoint = {\n",
    "        'model': raw_model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model_args': model_args,\n",
    "        'iter_num': iter_num,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'config': config,\n",
    "    }\n",
    "    print(f\"saving checkpoint to {fn}\")\n",
    "    torch.save(checkpoint, fn)\n",
    "\n",
    "# training loop\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model\n",
    "running_mfu = -1.0\n",
    "while True:\n",
    "    try:\n",
    "        # determine and set the learning rate for this iteration\n",
    "        lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "        # evaluate the loss on train/val sets and write checkpoints\n",
    "        if iter_num % eval_interval == 0 or iter_num >= max_iters:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "                best_val_loss = losses['val']\n",
    "                if iter_num > 0:\n",
    "                    save_model(checkpoint_file)\n",
    "    \n",
    "        # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "        # and using the GradScaler if data type is float16\n",
    "        for micro_step in range(gradient_accumulation_steps):\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "            # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "            X, Y = get_batch('train')\n",
    "            # backward pass, with gradient scaling if training in fp16\n",
    "            scaler.scale(loss).backward()\n",
    "        # clip the gradient\n",
    "        if grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        # step the optimizer and scaler if training in fp16\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # flush the gradients as soon as we can, no need for this memory anymore\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "        # timing and logging\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        if iter_num % log_interval == 0:\n",
    "            # get loss as float. note: this is a CPU-GPU sync point\n",
    "            # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "            lossf = loss.item() * gradient_accumulation_steps\n",
    "            if local_iter_num >= 5: # let the training loop settle a bit\n",
    "                mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "                running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "            if iter_num % 10 == 0:\n",
    "                print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "        iter_num += 1\n",
    "        local_iter_num += 1\n",
    "    \n",
    "        # termination conditions\n",
    "        if iter_num > max_iters:\n",
    "            break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"User requested exit, saving checkpoint\")\n",
    "        break\n",
    "\n",
    "save_model(checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd26b605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.825839875788878\n"
     ]
    }
   ],
   "source": [
    "# Worst-case loss when using Cross-Entropy Loss function\n",
    "import math\n",
    "vocab_size=50304\n",
    "worse_case = -math.log(1/vocab_size)\n",
    "print(worse_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419183b-a46e-47f7-94c0-9533e3e7c494",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a788d30-436d-4c41-9cb1-4b3c6fe50e57",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#checkpoint_file = 'model6000.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1722e6d5-8709-4f6c-83a1-31aac3cc363b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: model6000.pt\n",
      "Number of parameters: 3.42M\n",
      "\n",
      "That which I should say, Henry of thy title\n",
      "Lone I have got from Henry;\n",
      "For Warwick in another day she lives dead.\n",
      "But let us blame her company: my liege,\n",
      "When time I tender\n",
      "Will do thee right of Clarence, when I am here.\n",
      "Dail your children bring most capital stout, not with night,\n",
      "One of your purse to no thing to the commons,\n",
      "That they did try who's hands if thou return'st,\n",
      "My body nor time to give your person:\n",
      "To give the second arbitrateurl'd down itself.\n",
      "Be lecher in the princess, springs the contrary?\n",
      "\n",
      "ISABELLA:\n",
      "Put not your drum, the dead gate,\n",
      "And one once; for that with the war's current means:\n",
      "We have not forgot to-day. If you had,\n",
      "That may have done, you can shut; for I am\n",
      "A senseless runn in advantages,\n",
      "By his party frown;nitence the law is;\n",
      "And beg their untainted. Would thy alms? is the\n",
      "Is it not as bad? or you now.\n",
      "\n",
      "LEONTES:\n",
      "What?\n",
      "\n",
      "SICINIUS:\n",
      "Spesame shame, which dare no more itself is counted\n",
      "Than I should had more, between that I'll buy this\n",
      "To Rome.\n",
      "\n",
      "ABRAHAM:\n",
      "You had a sister, nor any man,\n",
      "Which you have yet said 'Hrow the duke himself is grown\n",
      "bed, that's a villain more, being such without such emulation\n",
      "Will tell 'she's! who, was the gods, if I know it.\n",
      "\n",
      "GRUMIO:\n",
      "Faith, if you meet me in the paucasudge and\n",
      "ANGELO:\n",
      "You must out my blood dislike it with me to be!\n",
      "Thus, I beseech your'll: my count, and I pray thee,\n",
      "Till I will to the dead? O good my kindness do I:\n",
      "Do it take it to my request that?\n",
      "If it may remain with you first his shroud,\n",
      "More than a pilot cannot help us all theirlysseding;\n",
      "The holding the citizens of shallowfatal--\n",
      "From yonder without Roman! We swear thenant\n",
      "But virtuous treason as much as all rebuke it\n",
      "And that, with those their hearts: let it speak,--\n",
      "\n",
      "Citizens\n"
     ]
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "print(f\"Checkpoint: {checkpoint_file}\")\n",
    "\n",
    "# inference parameters\n",
    "max_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1334\n",
    "device = 'cuda'\n",
    "dtype = 'bfloat16'\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# load model saved in a specific directory\n",
    "checkpoint = torch.load(checkpoint_file, map_location=device, weights_only=True)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# set prompt\n",
    "prompt = \"\\n\"\n",
    "start_ids = enc.encode(prompt, allowed_special={\"<|endoftext|>\"})\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        y = model.generate(x, max_tokens, temperature=temperature, top_k=top_k)\n",
    "        output = y[0].tolist()\n",
    "        for w in output:\n",
    "            if w > 50257: # max token value, ignore the rest\n",
    "                continue\n",
    "            else:\n",
    "                text = enc.decode([w])\n",
    "                if text == '\\n':\n",
    "                    print()\n",
    "                else:\n",
    "                    print(text, end='')\n",
    "                \n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30b98e-12f8-4f11-b489-7619310b2275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5b4f116-fda5-43f8-9e2f-2ffe5a9ca8ee",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Standford Univesity - CS231n: Deep Learning for Computer Vision - https://cs231n.github.io/neural-networks-1/\n",
    "* Visual Transformer, Explained - https://poloclub.github.io/transformer-explainer/\n",
    "* Cousera: Generative Ai with LLMs - https://www.coursera.org/learn/generative-ai-with-llms\n",
    "* Attention is All You Need by Vaswani et al. in 2017 - https://arxiv.org/abs/1706.03762\n",
    "* The Illustrated Transformer by Jay Alammar - https://jalammar.github.io/illustrated-transformer/\n",
    "* Visualizing Attention, a Transformer's Heart - https://www.3blue1brown.com/lessons/attention\n",
    "* Let's build GPT: from scratch, in code, spelled out. - by Andrej Karpathy - https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "* nanoGPT by Andrej Karpathy - https://github.com/karpathy/nanoGPT\n",
    "* OpenAI GPT-2 - https://github.com/openai/gpt-2/blob/master/src/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f16dd-19af-427c-96ec-c144636c3307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
