{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f19eea7-0128-4e1d-8e87-35a9fe082a0f",
   "metadata": {},
   "source": [
    "# All about LLMs\n",
    "\n",
    "This notebook walks you through creating your own Generative Pretrained Transformer (aka GPT). First we explore tokens, the atomic units of these large language models.  We will then move on to data preparation, training a model and finally, generating text with the model.\n",
    "\n",
    "## Background\n",
    "\n",
    "To understand LLM, we need to understand the foundational science: the Neuron.\n",
    "\n",
    "<img width=\"640\" alt=\"image\" src=\"https://github.com/user-attachments/assets/483305da-6c02-4532-8522-255f64b38093\">\n",
    "\n",
    "The intricate dance of synaptic connections and neurotransmitters is a key aspect of neural networks, and understanding these mechanisms has inspired the development of artificial neural networks, which mimic the behavior of biological neurons to process and learn from complex data.\n",
    "\n",
    "## Mathematical Representation of a Neuron\n",
    "\n",
    "![axon from a neuron](https://github.com/user-attachments/assets/62da7697-fc31-4670-a2ef-69c1ae4d2aa9)\n",
    "\n",
    "Multiple inputs (x) are amplified or attenuated (product) by the synaptic weights (w) before being combined (summed) inside the cell body. The cell itself has a bias (b) that gets added to that resulting signal. This signal is then passed through an activation function (f) to determine if the signal will be passed on to other neurons (output). Activation functions in neural networks introduce non-linearity, enabling the network to learn complex patterns. Common activation functions include ReLU, Sigmoid, Tanh, and Softmax. The hyperbolic tangent (`tanh()`) provides a zero-centered output and smooth gradient and is often used in hidden layers of a neural net. Softmax is often used in the output layer of a neural net.\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "![output layer](https://github.com/user-attachments/assets/50033790-4164-4154-8062-aaeef0d56df6)\n",
    "\n",
    "### How to train a Neural Network\n",
    "\n",
    "#### 1. Forward Propagation\n",
    "- Input data is fed into the input layer.\n",
    "- The data is then passed through each layer, from input to hidden to output.\n",
    "- At each neuron, the input data is multiplied by the corresponding weights, added to the bias, and passed through the activation function to produce the output: $ z = W \\cdot x + b $\n",
    "\n",
    "- This process continues until the data reaches the output layer, producing the final output of the network.\n",
    "\n",
    "#### 2. Compute Loss\n",
    "- After froward propagation, the loss is calculated to deterimne the accuracy of the current prediction.\n",
    "- The _loss function_ measures the difference between the predicted output and the desired target values.\n",
    "- Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.\n",
    "\n",
    "#### 3. Backpropagation\n",
    "- Once the loss is knows, backpropagation is performed to adjust the weights and biases to minimize the loss.\n",
    "- This is perfomred by calculating the gradient of the loss function with respect to each weight and bias using the chain rule.\n",
    "- The gradients are then used to update the weights and biases in the opposite direction of the gradient, a process known as _gradient descent_.\n",
    "\n",
    "#### 4. Repeat\n",
    "- Forward propagation and backpropagation will be performed over many iterations, adjusting the weights and biases each time to minimize the loss.\n",
    "- Training continues until the loss converges to a minimum value or a specified number of iterations (epochs) is reached.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef6a9e",
   "metadata": {},
   "source": [
    "\n",
    "## Transformer Architecture\n",
    "\n",
    "Building upon the biological and mathematical representation of the neuron, the Transformer architecture has emerged as a game-changer in the field of natural language processing. The transformer has given rise to the GenAI movement, a new era of artificial intelligence that is transforming industries and redefining the boundaries of human-machine collaboration.\n",
    "\n",
    "<img width=\"706\" alt=\"Transformers\" src=\"https://github.com/user-attachments/assets/f03692ed-7b3b-484f-b41b-b221376cd5a0\">\n",
    "\n",
    "Source : https://www.coursera.org/learn/generative-ai-with-llms\n",
    "\n",
    "The Transformer architecture was first introduced in the groundbreaking paper \"[Attention is All You Need](https://arxiv.org/pdf/1706.03762v1)\" by Vaswani et al. in 2017. This seminal work proposed a novel approach to sequence-to-sequence learning, which revolutionized the field of natural language processing (NLP) and beyond. The authors challenged the traditional encoder-decoder architecture, which relied on recurrent neural networks (RNNs) or convolutional neural networks (CNNs) to process sequential data. Instead, they proposed a purely attention-based model, where the encoder and decoder are composed of identical layers, and the attention mechanism is used to weigh the importance of different input elements when computing the output.\n",
    "\n",
    "<img width=\"485\" alt=\"Transformer Architecture\" src=\"https://github.com/user-attachments/assets/d1a81768-1a62-4d86-b48b-51c27ca80aa7\">\n",
    "\n",
    "___\n",
    "\n",
    "<img width=\"640\" alt=\"image\" src=\"https://github.com/user-attachments/assets/a67d80b6-8add-46a5-a269-267e0f3c68f1\">\n",
    "\n",
    "Source: https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "\n",
    "The Transformer architecture relies on self-attention, a mechanism that allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This is achieved through a series of linear transformations, which compute the attention weights, and a softmax function, which normalizes these weights to ensure they sum to 1. The attention weights are then used to compute the weighted sum of the input elements, which is used as input to the next layer. This process is repeated multiple times, allowing the model to capture long-range dependencies and contextual relationships in the input sequence. The Transformer architecture has since become a cornerstone of modern NLP, and its variants have been applied to a wide range of tasks, including machine translation, text classification, and question answering.\n",
    "\n",
    "![animal_](https://github.com/user-attachments/assets/20aeade6-b261-4f82-a818-bf2ed005dd6e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2df349",
   "metadata": {},
   "source": [
    "## All about Tokens\n",
    "\n",
    "Training an LLM on natural language requires that we convert the language to numeric \"signals\" to feed into the neural net. These are called tokens. These tokens can be characters, words or even parts of words. They are the atomic components of language.\n",
    "\n",
    "<img width=\"532\" alt=\"image\" src=\"https://github.com/user-attachments/assets/bb90fbe5-78ec-45b3-9678-6a95c9baa386\">\n",
    "\n",
    "Source: https://platform.openai.com/tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d71352-adf5-4fd5-a4a7-68c1c8a65e19",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First some dependencies need to be installed\n",
    "!pip install tiktoken\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install tqdm\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef6ecee2-0261-49da-8085-053bcda02680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464, 2068, 7586, 21831, 11687, 625, 262, 16931, 3013, 3255, 3290, 13]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "phrase = \"The quick brown fox jumped over the lazy snoring dog.\"\n",
    "tokens = enc.encode_ordinary(phrase)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "defa1f80-eafe-4c86-a90b-e80420e1ae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The\", \" quick\", \" brown\", \" fox\", \" jumped\", \" over\", \" the\", \" lazy\", \" sn\", \"oring\", \" dog\", \".\", "
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(f'\"{enc.decode([token])}\", ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f31f8c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The\", -> \" quick\"\n",
      "\"The\", \" quick\", -> \" brown\"\n",
      "\"The\", \" quick\", \" brown\", -> \" fox\"\n",
      "\"The\", \" quick\", \" brown\", \" fox\", -> \" jumped\"\n",
      "\"The\", \" quick\", \" brown\", \" fox\", \" jumped\", -> \" over\"\n",
      "\"The\", \" quick\", \" brown\", \" fox\", \" jumped\", \" over\", -> \" the\"\n",
      "\"The\", \" quick\", \" brown\", \" fox\", \" jumped\", \" over\", \" the\", -> \" lazy\"\n",
      "\"The\", \" quick\", \" brown\", \" fox\", \" jumped\", \" over\", \" the\", \" lazy\", -> \" sn\"\n",
      "\"The\", \" quick\", \" brown\", \" fox\", \" jumped\", \" over\", \" the\", \" lazy\", \" sn\", -> \"oring\"\n",
      "\"The\", \" quick\", \" brown\", \" fox\", \" jumped\", \" over\", \" the\", \" lazy\", \" sn\", \"oring\", -> \" dog\"\n",
      "\"The\", \" quick\", \" brown\", \" fox\", \" jumped\", \" over\", \" the\", \" lazy\", \" sn\", \"oring\", \" dog\", -> \".\"\n"
     ]
    }
   ],
   "source": [
    "# The training target is the next token in the sequence\n",
    "predictions = tokens[:-1]\n",
    "targets = tokens[1:] + [enc.encode_ordinary(\"\")]\n",
    "for x in range(len(predictions)):\n",
    "    for p in predictions[:x+1]:\n",
    "        print(f'\"{enc.decode([p])}\", ',end='')\n",
    "    print(f'-> \"{enc.decode([targets[x]])}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a017463",
   "metadata": {},
   "source": [
    "## Large Language Models\n",
    "\n",
    "![image](https://github.com/user-attachments/assets/f722b1ba-4660-4782-8ece-aa422bea8a7c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697ffca-c9d8-44a1-9d18-e595d18c1a5f",
   "metadata": {},
   "source": [
    "# Tokenize Training Data\n",
    "\n",
    "We will use the Tiny Shakespeare dataset to train a model. See the [shakespeare.txt](data/shakespeare.txt) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbe64b04-e900-4a84-9792-fdbe426049a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 301,966 tokens\n",
      "val has 36,059 tokens\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "# tiny shakespeare dataset\n",
    "input_file_path = 'data/shakespeare.txt'\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "n = len(data)\n",
    "\n",
    "# Use 90% for training and 10% for validation\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode with tiktoken gpt2\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile('data/train.bin')\n",
    "val_ids.tofile('data/val.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7806ff78-2c68-418b-8877-a2b3d05bc059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5962, 22307,    25, ...,   508,  2058,   994], dtype=uint16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_ids))\n",
    "train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a3c098-51e9-44eb-96dd-002ff11a31a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode(train_ids[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "169c1d42-47b0-4a6b-9b5e-76ee74e5756b",
   "metadata": {},
   "source": [
    "# GPT Model Class\n",
    "\n",
    "We will use the `model.py` file contains a GPT model class that was created by Andrej Karpathy. It was modeled after the one created by OpenAI.\n",
    "\n",
    "GPT, as the name implies, is based on the Transformer technology presented by Google in the paper, \"Attention Is All You Need.\" https://arxiv.org/pdf/1706.03762\n",
    "\n",
    "References:\n",
    "* Great Video by Andrej Karpathy - _Let's build GPT: from scratch, in code, spelled out._ - https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "* nanoGPT - https://github.com/karpathy/nanoGPT\n",
    "* OpenAI GPT-2 - https://github.com/openai/gpt-2/blob/master/src/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a50c5c-2be8-4c98-b40f-ef90bab2516c",
   "metadata": {},
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "213251e0-26e9-4461-a0ba-07d73edb17c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 15,360\n",
      "Initializing a new model from scratch\n",
      "Using vocab_size of GPT-2 of 50304 (50257 rounded up for efficiency)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jason/miniforge3/envs/llama/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3.42M\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 100\u001b[0m\n\u001b[1;32m     98\u001b[0m     model\u001b[38;5;241m.\u001b[39mcrop_block_size(block_size)\n\u001b[1;32m     99\u001b[0m     model_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock_size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m block_size \u001b[38;5;66;03m# so that the checkpoint will have the right value\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# initialize a GradScaler\u001b[39;00m\n\u001b[1;32m    103\u001b[0m scaler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mGradScaler(device\u001b[38;5;241m=\u001b[39mdevice, enabled\u001b[38;5;241m=\u001b[39m(dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/miniforge3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/llama/lib/python3.10/site-packages/torch/cuda/__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# parameters\n",
    "checkpoint_file = 'model.pt'\n",
    "eval_interval = 100\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "always_save_checkpoint = True # save a checkpoint after each eval_interval\n",
    "\n",
    "# data\n",
    "dataset = 'data' # directory where the data is stored\n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 32 # content window size\n",
    "\n",
    "# model\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "n_embd = 64\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 2000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = max_iters # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16'\n",
    "\n",
    "# capture above settings & parameters to save in model checkpoint\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys} \n",
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "seed = 4212\n",
    "tokens_per_iter = gradient_accumulation_steps * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "# set the random seed\n",
    "torch.manual_seed(1337)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# Load the data\n",
    "data_dir = 'data/'\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "# get a batch from the data\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# loop counters starting point\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# init a new model from scratch\n",
    "print(\"Initializing a new model from scratch\")\n",
    "print(\"Using vocab_size of GPT-2 of 50304 (50257 rounded up for efficiency)\")\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=50304, dropout=dropout) \n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "model.to(device)\n",
    "\n",
    "# initialize a GradScaler\n",
    "scaler = torch.amp.GradScaler(device=device, enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "# compile the model - optional\n",
    "#print(\"compiling the model... (takes a ~minute)\")\n",
    "#model = torch.compile(model) # requires PyTorch 2.0\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# save model to checkpoint file\n",
    "def save_model(fn):\n",
    "    checkpoint = {\n",
    "        'model': raw_model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model_args': model_args,\n",
    "        'iter_num': iter_num,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'config': config,\n",
    "    }\n",
    "    print(f\"saving checkpoint to {fn}\")\n",
    "    torch.save(checkpoint, fn)\n",
    "\n",
    "# training loop\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model\n",
    "running_mfu = -1.0\n",
    "while True:\n",
    "    try:\n",
    "        # determine and set the learning rate for this iteration\n",
    "        lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "        # evaluate the loss on train/val sets and write checkpoints\n",
    "        if iter_num % eval_interval == 0 or iter_num >= max_iters:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "                best_val_loss = losses['val']\n",
    "                if iter_num > 0:\n",
    "                    save_model(checkpoint_file)\n",
    "    \n",
    "        # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "        # and using the GradScaler if data type is float16\n",
    "        for micro_step in range(gradient_accumulation_steps):\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "            # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "            X, Y = get_batch('train')\n",
    "            # backward pass, with gradient scaling if training in fp16\n",
    "            scaler.scale(loss).backward()\n",
    "        # clip the gradient\n",
    "        if grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        # step the optimizer and scaler if training in fp16\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # flush the gradients as soon as we can, no need for this memory anymore\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "        # timing and logging\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        if iter_num % log_interval == 0:\n",
    "            # get loss as float. note: this is a CPU-GPU sync point\n",
    "            # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "            lossf = loss.item() * gradient_accumulation_steps\n",
    "            if local_iter_num >= 5: # let the training loop settle a bit\n",
    "                mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "                running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "            if iter_num % 10 == 0:\n",
    "                print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "        iter_num += 1\n",
    "        local_iter_num += 1\n",
    "    \n",
    "        # termination conditions\n",
    "        if iter_num > max_iters:\n",
    "            break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"User requested exit, saving checkpoint\")\n",
    "        break\n",
    "\n",
    "save_model(checkpoint_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419183b-a46e-47f7-94c0-9533e3e7c494",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1722e6d5-8709-4f6c-83a1-31aac3cc363b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3.42M\n",
      "\n",
      "\n",
      "CORIOLANUS:\n",
      "If you had he that have it rain be is that\n",
      "Will?\n",
      "\n",
      "CLARENCE:\n",
      "Mised! what of the house of me? 'emfore fals!\n",
      "\n",
      "MERCUTIO:\n",
      "We stay no end!' why! Master Froth art and let him\n",
      "To the grave or I have done very foul fair;\n",
      "The day-ple of this mother's life, I'll\n",
      "Desinks not a woman you with me, but this way\n",
      "Appear in his native part's wife not the king,\n",
      "Which now you not the field-vowies of him?\n",
      "\n",
      "OXFORD:\n",
      "I think you put on she, a very man of thee.\n",
      "\n",
      "Clown:\n",
      "O, go; and, I would come there find you not;\n",
      "Than you will both sit, as she'st upon you.\n",
      "\n",
      "ANTIGONET:\n",
      "What, I will be true.\n",
      "\n",
      "BISHOP OF YORK:\n",
      "Be not, if you fight, or when she's hate along,\n",
      "I be a crown here but thy ancient ground.\n",
      "Godable that I have to-morrow, and give me you.\n",
      "\n",
      "Messillo, I am so:\n",
      "Tis my my brows?\n",
      "\n",
      "ROMEO:\n",
      "\n",
      "POMPEY:\n",
      "Ay, nor his tender. Thou, therefore, ere not you thought.\n",
      "\n",
      "JULtis the time; it is as ducisbury\n",
      "If to thy wife, like this now long.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "To be a first deed: at the loss of Marcius,\n",
      "Were I not love not what thou to give?\n",
      "\n",
      "MAMILLO:\n",
      "'Mourfore him, sir, my brother's war's brother!\n",
      "\n",
      "WARWICK:\n",
      "Hence.\n",
      "\n",
      "RATMENASTINGS:\n",
      "Master, now you, for me well: I'll fly in himself.\n",
      "\n",
      "Third Citizen:\n",
      "O, I am I hither, sir.\n",
      "\n",
      "ELY ANNE:\n",
      "On my uncle Clifford, go: if you find her blood\n",
      "Turn it.\n",
      " nor the time have so bloody with them, your honates;\n",
      "\n",
      "At York is yesternight for thee, the loss of them.\n",
      "\n",
      "PAULINA:\n",
      "Who hath as my own true lordsKE OF\n"
     ]
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# inference parameters\n",
    "max_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'mps'\n",
    "dtype = 'bfloat16'\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# load model saved in a specific directory\n",
    "ckpt_path = 'out/ckpt.pt'\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# set prompt\n",
    "prompt = \"\\n\"\n",
    "start_ids = enc.encode(prompt, allowed_special={\"<|endoftext|>\"})\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        y = model.generate(x, max_tokens, temperature=temperature, top_k=top_k)\n",
    "        output = y[0].tolist()\n",
    "        for w in output:\n",
    "            if w > 50257: # max token value, ignore the rest\n",
    "                continue\n",
    "            else:\n",
    "                text = enc.decode([w])\n",
    "                if text == '\\n':\n",
    "                    print()\n",
    "                else:\n",
    "                    print(text, end='')\n",
    "                \n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ea0bc-5884-4e52-9e40-e5d69767ea05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
