{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a6ebe8-454f-4f5a-bcdb-6ed1132625c7",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First some dependencies need to be installed\n",
    "!pip install matplotlib\n",
    "!pip install termcolor\n",
    "!pip install torch\n",
    "!pip install tiktoken\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00474b55-e1dd-4266-bd5e-1a73e4ddfb29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# GenAI: Large Language Models - How do they work?\n",
    "\n",
    "ChatGPT, Claude, Gemini, and CoPilot are all amazing examples of Generative AI technology powered by Large Language Models. But how does it actually work? Let's take a look behind the scenes and into the code of this amazing new technology.\n",
    "\n",
    "This notebook walks you through creating your own Generative Pretrained Transformer (GPT) language model. First we explore the background science, then we will look at tokens, the atomic units of these large language models. We will then move on to data preparation, training a model and finally, generating text with the model.\n",
    "\n",
    "## Background\n",
    "\n",
    "To understand GPTs, we need to understand the foundational science: the Neuron.\n",
    "\n",
    "<img width=\"640\" alt=\"image\" src=\"https://github.com/user-attachments/assets/483305da-6c02-4532-8522-255f64b38093\">\n",
    "\n",
    "The intricate dance of synaptic connections and neurotransmitters is a key aspect of neural networks, and understanding these mechanisms has inspired the development of artificial neural networks, which mimic the behavior of biological neurons to process and learn from complex data.\n",
    "\n",
    "## Mathematical Representation of a Neuron\n",
    "\n",
    "![axon from a neuron](https://github.com/user-attachments/assets/62da7697-fc31-4670-a2ef-69c1ae4d2aa9)\n",
    "\n",
    "Multiple inputs (x) are amplified or attenuated (product) by the synaptic weights (w) before being combined (summed) inside the cell body. The cell itself has a bias (b) that gets added to that resulting signal. This signal is then passed through an activation function (f) to determine if the signal will be passed on to other neurons (output). Activation functions in neural networks introduce non-linearity, enabling the network to learn complex patterns. Common activation functions include ReLU, Sigmoid, Tanh, and Softmax. The hyperbolic tangent (`tanh()`) provides a zero-centered output and smooth gradient and is often used in hidden layers of a neural net. Softmax is often used in the output layer of a neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82034b0b-cdf3-4f90-a339-9c419b9092dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAGdCAYAAAAfTAk2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAABHXElEQVR4nO3de1yUdd4//tfMMAwHYRSQARQBjRREUyERTM1VUMvOnu6K1lZt/dnJqF/FWpu623J32KQsK3fdJQ+pd7dr1h0Zo+WhQPMAmqakpoLIWWBQZI7X9w9kcuQgo8xczDWv5+MxD5hrPnPxvt4N+Oo6fC6ZIAgCiIiIiCRELnYBRERERF2NAYeIiIgkhwGHiIiIJIcBh4iIiCSHAYeIiIgkhwGHiIiIJIcBh4iIiCSHAYeIiIgkx0PsAsRgsVhw/vx5+Pn5QSaTiV0OERERdYIgCGhoaEBYWBjk8o730bhlwDl//jzCw8PFLoOIiIhuQElJCfr27dvhGLcMOH5+fgCaG+Tv7y9yNd2D0WhEbm4uUlNToVQqxS5H8thv52K/nYv9di536rdOp0N4eLj13/GOuGXAaTks5e/vz4BzhdFohI+PD/z9/SX/C9IdsN/OxX47F/vtXO7Y786cXsKTjImIiEhyGHCIiIhIchhwiIiISHIYcIiIiEhyGHCIiIhIchhwiIiISHIYcIiIiEhyGHCIiIhIchhwiIiISHIcGnB27dqFe+65B2FhYZDJZPj888+v+56dO3ciPj4eXl5e6N+/Pz766KNWYzZt2oTY2FioVCrExsZi8+bNDqieiIiIXJVDA86lS5dw22234f333+/U+NOnT+Ouu+7CmDFjUFBQgD/96U945plnsGnTJuuY/Px8zJw5E2lpaTh06BDS0tIwY8YM7N2711GbQURERC7GofeimjJlCqZMmdLp8R999BH69euHrKwsAEBMTAz279+Pt99+Gw899BAAICsrCykpKcjIyAAAZGRkYOfOncjKysL69eu7fBuIiIjI9XSrm23m5+cjNTXVZtmkSZOwatUqGI1GKJVK5Ofn47nnnms1piUUtUWv10Ov11uf63Q6AM03KDMajV23AS6spQ/sh3Ow387FfjuXlPtttggwmCwwWSwwmgWYLAJMZguMFgEm85XvzQJMFgvMFgFmQWj+agHMFgvMAmA2Ny+3XHndIgDClXEt31sEwCwI1u9bvgKARRAgCL99NZnNOHFOhlPbT0Aml0MAAAEQ0Py6AFz5euUJflvW/L1w1fdXvrYsuMq1Y64eJ7QxLtDXEwvu7H9T/b6WPZ+pbhVwysvLodFobJZpNBqYTCZUV1cjNDS03THl5eXtrjczMxNLlixptTw3Nxc+Pj5dU7xEaLVasUtwK+y3c7HfziVWvy0CcNkENJqAy2ag0STDZRPQZAYMlisPswz6K8+v/mq0yGASAJMFMFrQ6nuLcP27WItDAZScFrsIG8FeAiIbj3fpOhsbGzs9tlsFHKD1LdBb0uHVy9sa09Gt0zMyMpCenm59rtPpEB4ejtTUVPj7+3dF2S7PaDRCq9UiJSUFSqVS7HIkj/12Lvbbubq634Ig4KLehMoGA6oa9Khs0KPqot76ffVFA+oajdA1GVF/2YSLelMXbEXnyGWAh0IOpVwGD4UMHnI5PBQyKOUyKORyKOQyKOSAQiaDQiFr/ipvfshlv32Vy9D8VX7lq0wGWcsyWfO/ezLgquUArrwmWASUnS9F3759IJcrIJMBV16GDDLrc1xZR8s/l83fy6763nbbbP7dtS5reS67Zixaje3l64m7kiNussO2Wo7AdEa3CjghISGt9sRUVlbCw8MDgYGBHY65dq/O1VQqFVQqVavlSqWSf+yuwZ44F/vtXOy3c9nTb4PJgpLaRpypvoTT1ZdwpuYSzlQ3oqS2ERW6JjQZLXb/fB9PBdTeSqi9lfD3UqKHlwe8PRXwUSrgq/rtex+VB3w8FfDxVEDloYCXUg5PDzlUHgqoPORXHoory+RQesihVMiglMshl4u/R8doNCInpwR33TVE8p9ve7avWwWcpKQkfPnllzbLcnNzkZCQYN2opKQkaLVam/NwcnNzkZyc7NRaiYjIfiazBScqL6KwpA7Hy3Q4XdMcas7VNlrPMWmPn5cHgv1UCPbzQrC/yvp9bz8VevoorWFG7a2En5cSnh6c6s2dOTTgXLx4ESdPnrQ+P336NAoLCxEQEIB+/fohIyMDpaWlWL16NQBg/vz5eP/995Geno558+YhPz8fq1atsrk66tlnn8XYsWPxxhtv4L777sOWLVuwbds2fP/9947cFCIispMgCDhfdxmHSupQWFKHgpI6HCmtR6PB3OZ4H08FIgN9ERXki8ggH0QG+iIi0Bch/s0hxttT4eQtIFfm0ICzf/9+jB8/3vq85TyY3//+98jOzkZZWRmKi4utr0dFRSEnJwfPPfccPvjgA4SFheG9996zXiIOAMnJydiwYQNeeeUVvPrqqxgwYAA2btyIxMRER24KERF1wtmaS9j603n833E5Xj+yC5UN+lZjeqg8MLSvGkP6qNG/t6811PT2U3V4PiWRPRwacO688842LzVrkZ2d3WrZuHHjcPDgwQ7XO23aNEybNu1myyMiopskCAKOntch92g5cn+uwPHyhiuvyAHooZDLMFDjh2H9emJYeE8MD++JAb17dItzV0jautU5OERE1P2ZzBbsP1uLb46WI/doBUrrLltfU8hlGBnZC71NVZiVMgrDIwJ5aIlEwYBDRESdcqxMh0/yzuCbo+WobfxtwjUvpRxjo3tj0uAQ/G5QMHp4ypCTk4PbI3tBqWS4IXEw4BARUbsEQUD+qRp8tOtX7Pqlyrq8p48SEwZpMGmwBmOie9vspZHiDMbkehhwiIioFZPZgq+PlOPjXadwpLR5cjW5DJgyJBSPjOyHkVEB8FDwMmzqvhhwiIjIqtFgwmf7z+Gf3/+KkgvN59Z4KeWYkRCOuXf0R79A3t6GXAMDDhER4aLehJW7fsXq/DOou3J+TYCvJx5LisBjSZEI8PUUuUIi+zDgEBG5ud0nqvDypp+sV0P1C/DBvDFRmBYfziugyGUx4BARuSldkxGZOcew/scSAEB4gDdenhyDyXEhUHCeGnJxDDhERG5oR1ElMv7zE8rqmwAAs5Mj8eLkgfDx5D8LJA38JBMRuZH6y0a8/tXP+J/95wAAEYE+ePOhoUjsHyhyZURdiwGHiMhNfHe8ea9Nua4JMhnweHIU/v9JA3meDUkSAw4RkcTVXzZi6Zc/Y9PB5r02UUG+eGvaUCREBohcGZHjMOAQEUlYpa4Jj67ai18qLkImA+beEYX0FO61IeljwCEikqiSC414dNVenK1phMZfhRWPjEB8BPfakHtgwCEikqCTlRfx6D/3olzXhH4BPlg3NxHhAZyFmNwHAw4RkcQcKa3HY//6ERcuGRAd3ANr5yZC4+8ldllETsWAQ0QkIfvPXMDj2fvQ0GTCkD5qfPKHkbzNArklBhwiIonYfaIKT6w+gMtGM0ZGBuCfsxPg76UUuywiUTDgEBFJwDdHy/H0pwUwmC0Yd2tvfPRoPK+UIrfGgENE5OI2F5zDC58dhtkiYEpcCN6dNRyeHnKxyyISFQMOEZELW/9jMTL+8xMAYFp8X/z3g0PgoWC4IWLAISJyUXt/rcErnx8B0HyzzD9PjYWcdwEnAsCAQ0Tkkqoa9Hh6fQHMFgEPDO+D1+6JhUzGcEPUgvsxiYhcjNkiYOHGAlQ26BEd3AOvPxDHcEN0DQYcIiIX8+72E/jhZA28lQp8+OgI+HhyZzzRtRhwiIhcyK5fqrD82xMAgMwHh+CWYD+RKyLqnhhwiIhcRFn9ZSzcWAhBAB5O7If7h/cRuySibosBh4jIBRjNFjz9aQEuXDJgcJg//jw1VuySiLo1BhwiIhfw1jdF2H+2Fn4qD6x4ZAS8lJylmKgjDDhERN1c7tFyrNz1KwDgrelDERHoK3JFRN0fAw4RUTdWXNOI5z87BACYc0cUJseFilwRkWtwSsBZsWIFoqKi4OXlhfj4eOzevbvdsbNnz4ZMJmv1GDx4sHVMdnZ2m2OampqcsTlERE7RZDTjyU8PoqHJhOH9euKlyYPELonIZTg84GzcuBELFy7EokWLUFBQgDFjxmDKlCkoLi5uc/y7776LsrIy66OkpAQBAQGYPn26zTh/f3+bcWVlZfDy8nL05hAROc3rXx3DT6X16OWjxAcPj+ANNIns4PDflnfeeQdz5szB3LlzERMTg6ysLISHh+PDDz9sc7xarUZISIj1sX//ftTW1uLxxx+3GSeTyWzGhYSEOHpTiIicZt+ZC1iz5ywA4J2ZwxDW01vkiohci0OnvzQYDDhw4ABefvllm+WpqanIy8vr1DpWrVqFiRMnIiIiwmb5xYsXERERAbPZjGHDhuEvf/kLhg8f3uY69Ho99Hq99blOpwMAGI1GGI1GezZJslr6wH44B/vtXK7Wb4tFwF++PAoAmJnQB3f07+UytQOu129X5079tmcbHRpwqqurYTabodFobJZrNBqUl5df9/1lZWX4+uuv8emnn9osHzRoELKzszFkyBDodDq8++67GD16NA4dOoTo6OhW68nMzMSSJUtaLc/NzYWPj4+dWyVtWq1W7BLcCvvtXK7S7/1VMhwuVUAlFxAnnEVOzlmxS7ohrtJvqXCHfjc2NnZ6rFNuYHLtTeAEQejUjeGys7PRs2dP3H///TbLR40ahVGjRlmfjx49GiNGjMDy5cvx3nvvtVpPRkYG0tPTrc91Oh3Cw8ORmpoKf39/O7dGmoxGI7RaLVJSUqBUKsUuR/LYb+dypX43Gc3473d/ANCEJ38XjVnj+otdkt1cqd9S4E79bjkC0xkODThBQUFQKBSt9tZUVla22qtzLUEQ8K9//QtpaWnw9PTscKxcLsftt9+OEydOtPm6SqWCSqVqtVypVEr+w2Av9sS52G/ncoV+r/z+LMrqmxCm9sIT426B0oUn9HOFfkuJO/Tbnu1z6EnGnp6eiI+Pb7XbTKvVIjk5ucP37ty5EydPnsScOXOu+3MEQUBhYSFCQzk/BBG5rsqGJqz47iQA4KUpgzhbMdFNcPghqvT0dKSlpSEhIQFJSUlYuXIliouLMX/+fADNh49KS0uxevVqm/etWrUKiYmJiIuLa7XOJUuWYNSoUYiOjoZOp8N7772HwsJCfPDBB47eHCIih1mm/QWXDGbc1leNe4aGiV0OkUtzeMCZOXMmampqsHTpUpSVlSEuLg45OTnWq6LKyspazYlTX1+PTZs24d13321znXV1dXjiiSdQXl4OtVqN4cOHY9euXRg5cqSjN4eIyCGOl+uwcV8JAOCVqbGQy69/niIRtc8pJxkvWLAACxYsaPO17OzsVsvUanWHZ0ovW7YMy5Yt66ryiIhEJQgCXv/qGCwCcNeQENweGSB2SUQuj9NiEhGJbMcvVdh9ohqeCjlvx0DURRhwiIhEZDJb8PpXxwAAv0+O4J3CiboIAw4RkYg27CvBycqL6OWjxFO/az1RKRHdGAYcIiKR6JqMWKb9BQCwcOKtUHtLew4TImdiwCEiEsmK706h5pIB/Xv74uHEfmKXQyQpDDhERCIoudCIf/1wGgDwpykxUCr455ioK/E3iohIBG9+UwSDyYLkAYGYEBMsdjlEksOAQ0TkZD+dq8eXh85DJgMW3R3TqZsPE5F9GHCIiJys5dDUvbeFYXCYWuRqiKSJAYeIyIkqG5rwf4fPAwDm3BElcjVE0sWAQ0TkROv2FMNoFhAf0QtD+/YUuxwiyWLAISJyEr3JjHV7m28uPDs5UtxiiCSOAYeIyEm+OlyG6ot6hPh7YXJciNjlEEkaAw4RkRMIgoB//3AGAJCWFMF5b4gcjL9hREROcLC4Fj+V1sPTQ45Zt4eLXQ6R5DHgEBE5Qcvem/uHhSGwh0rcYojcAAMOEZGDldVfxtdHygEAs5N5aTiRMzDgEBE52No9Z2G2CEiMCkBsmL/Y5RC5BQYcIiIHajKa8emVS8MfHx0pbjFEboQBh4jIgb4oPI/aRiP69PTGxBiN2OUQuQ0GHCIiBxEEAf/OOwMAeCwpAh68NJzIafjbRkTkIHtPX8CxMh28lHLM5KXhRE7FgENE5CDZVy4Nf3BEX/T08RS3GCI3w4BDROQAJRcakftzy6XhkeIWQ+SGGHCIiBxg7Z6zsAjA6FsCcavGT+xyiNwOAw4RURdrNJiw/scrl4ZzYj8iUTDgEBF1sc0FpdA1mdAvwAfjBwWLXQ6RW2LAISLqQoIgWE8u/n1yJBRymbgFEbkpBhwioi70w8kanKi8CB9PBaYn9BW7HCK3xYBDRNSFPjtQAgB4cEQf+HspRa6GyH0x4BARdZHLBjO0P1cAaJ77hojE45SAs2LFCkRFRcHLywvx8fHYvXt3u2N37NgBmUzW6nH8+HGbcZs2bUJsbCxUKhViY2OxefNmR28GEVGHth2rQKPBjPAAbwwP7yl2OURuzeEBZ+PGjVi4cCEWLVqEgoICjBkzBlOmTEFxcXGH7ysqKkJZWZn1ER0dbX0tPz8fM2fORFpaGg4dOoS0tDTMmDEDe/fudfTmEBG1a0vheQDAPUPDIJPx5GIiMTk84LzzzjuYM2cO5s6di5iYGGRlZSE8PBwffvhhh+8LDg5GSEiI9aFQKKyvZWVlISUlBRkZGRg0aBAyMjIwYcIEZGVlOXhriIjaVt9oxM5fKgEA9w3rI3I1ROThyJUbDAYcOHAAL7/8ss3y1NRU5OXldfje4cOHo6mpCbGxsXjllVcwfvx462v5+fl47rnnbMZPmjSp3YCj1+uh1+utz3U6HQDAaDTCaDTas0mS1dIH9sM52G/ncka/vzp8DkazgFuDe6B/oJdb/7fl59u53Knf9myjQwNOdXU1zGYzNBqNzXKNRoPy8vI23xMaGoqVK1ciPj4eer0ea9aswYQJE7Bjxw6MHTsWAFBeXm7XOjMzM7FkyZJWy3Nzc+Hj43MjmyZZWq1W7BLcCvvtXI7sd/bPcgByRKvqkZOT47Cf40r4+XYud+h3Y2Njp8c6NOC0uPZYtCAI7R6fHjhwIAYOHGh9npSUhJKSErz99tvWgGPvOjMyMpCenm59rtPpEB4ejtTUVPj7+9u9PVJkNBqh1WqRkpICpZKXtjoa++1cju53ZYMeJ/bsBACkTxuHfgHu/T9O/Hw7lzv1u+UITGc4NOAEBQVBoVC02rNSWVnZag9MR0aNGoW1a9dan4eEhNi1TpVKBZVK1Wq5UqmU/IfBXuyJc7HfzuWofn/z8zkIAjC8X08M0Ki7fP2uip9v53KHftuzfQ49ydjT0xPx8fGtdptptVokJyd3ej0FBQUIDQ21Pk9KSmq1ztzcXLvWSUTUVb441Hz11L23hYlcCRG1cPghqvT0dKSlpSEhIQFJSUlYuXIliouLMX/+fADNh49KS0uxevVqAM1XSEVGRmLw4MEwGAxYu3YtNm3ahE2bNlnX+eyzz2Ls2LF44403cN9992HLli3Ytm0bvv/+e0dvDhGRjeKaRhSW1EEuA+4eGnr9NxCRUzg84MycORM1NTVYunQpysrKEBcXh5ycHERERAAAysrKbObEMRgMeOGFF1BaWgpvb28MHjwYX331Fe666y7rmOTkZGzYsAGvvPIKXn31VQwYMAAbN25EYmKiozeHiMjGl4eb994kDQhEsJ+XyNUQUQunnGS8YMECLFiwoM3XsrOzbZ6/+OKLePHFF6+7zmnTpmHatGldUR4R0Q3bUlgKALjvNs59Q9Sd8F5UREQ36Hi5Dr9UXISnQo5JcSFil0NEV2HAISK6QV9cuTXDuIG9ofaW9tUrRK6GAYeI6AYIgmA9/+a+Ybx6iqi7YcAhIroBBSV1KLlwGb6eCkwY1Pl5vYjIORhwiIhuQMvhqZRYDbw9FdcZTUTOxoBDRGQnk9mC/ztcBgC4l4eniLolBhwiIjvt+fUCqi/q0ctHiTHRvcUuh4jawIBDRGSnLw41z30zZUgolAr+GSXqjvibSURkB73JjK+PNN/sl/eeIuq+GHCIiOywo6gKDU0mhPh7YWRkgNjlEFE7GHCIiOzQcufwe24LhVwuE7kaImoPAw4RUSdd0puw/VgFAOBe3nuKqFtjwCEi6iTtzxVoMloQFeSLuD7+YpdDRB1gwCEi6qSWuW/uuS0MMhkPTxF1Zww4RESd0GQ04/uTVQCAKbxzOFG3x4BDRNQJP5ysRpPRgj49vTEoxE/scojoOhhwiIg6YduxSgDA7wYF8/AUkQtgwCEiug5BEPDt8earpybEBItcDRF1BgMOEdF1HD2vQ4VODx9PBUb1DxS7HCLqBAYcIqLr2HZl7psx0UHwUipEroaIOoMBh4joOrZfOf9mQoxG5EqIqLMYcIiIOlCha8JPpfWQyYDxA3n+DZGrYMAhIupAy96b2/r2RG8/lcjVEFFnMeAQEXWg5eqpibx6isilMOAQEbWjefbiagA8/4bI1TDgEBG1o2X24jC1F2cvJnIxDDhERO3YdtXVU5y9mMi1MOAQEbWBsxcTuTYGHCKiNhwp5ezFRK6MAYeIqA3bj3P2YiJXxoBDRNQG6+zFg3j1FJErckrAWbFiBaKiouDl5YX4+Hjs3r273bH/+c9/kJKSgt69e8Pf3x9JSUn45ptvbMZkZ2dDJpO1ejQ1NTl6U4jIDdjMXjyI598QuSKHB5yNGzdi4cKFWLRoEQoKCjBmzBhMmTIFxcXFbY7ftWsXUlJSkJOTgwMHDmD8+PG45557UFBQYDPO398fZWVlNg8vLy9Hbw4RuQHOXkzk+jwc/QPeeecdzJkzB3PnzgUAZGVl4ZtvvsGHH36IzMzMVuOzsrJsnv/tb3/Dli1b8OWXX2L48OHW5TKZDCEhIQ6tnYjc0/ZjnL2YyNU5NOAYDAYcOHAAL7/8ss3y1NRU5OXldWodFosFDQ0NCAgIsFl+8eJFREREwGw2Y9iwYfjLX/5iE4Cuptfrodfrrc91Oh0AwGg0wmg02rNJktXSB/bDOdhv57Kn35cNv81ePPaWQP43ugH8fDuXO/Xbnm10aMCprq6G2WyGRmN7kp5Go0F5eXmn1vH3v/8dly5dwowZM6zLBg0ahOzsbAwZMgQ6nQ7vvvsuRo8ejUOHDiE6OrrVOjIzM7FkyZJWy3Nzc+Hj42PnVkmbVqsVuwS3wn47V2f6faRWBr1JgZ6eAn49uBunOb/fDePn27ncod+NjY2dHuvwQ1QAWs0AKghCp2YFXb9+PRYvXowtW7YgOPi3XcWjRo3CqFGjrM9Hjx6NESNGYPny5XjvvfdarScjIwPp6enW5zqdDuHh4UhNTYW/v/+NbJLkGI1GaLVapKSkQKlUil2O5LHfzmVPv/O2/AzgHO4e1g933x3jnAIlhp9v53KnfrccgekMhwacoKAgKBSKVntrKisrW+3VudbGjRsxZ84cfPbZZ5g4cWKHY+VyOW6//XacOHGizddVKhVUqtYnCiqVSsl/GOzFnjgX++1c1+u3IAjY8UsVACBlcAj/29wkfr6dyx36bc/2OfQqKk9PT8THx7fababVapGcnNzu+9avX4/Zs2fj008/xd13333dnyMIAgoLCxEaGnrTNROR++LsxUTS4fBDVOnp6UhLS0NCQgKSkpKwcuVKFBcXY/78+QCaDx+VlpZi9erVAJrDzWOPPYZ3330Xo0aNsu798fb2hlqtBgAsWbIEo0aNQnR0NHQ6Hd577z0UFhbigw8+cPTmEJGEtcxefMctnL2YyNU5PODMnDkTNTU1WLp0KcrKyhAXF4ecnBxEREQAAMrKymzmxPn4449hMpnw5JNP4sknn7Qu//3vf4/s7GwAQF1dHZ544gmUl5dDrVZj+PDh2LVrF0aOHOnozSEiCWuZ/2ZiDGcvJnJ1TjnJeMGCBViwYEGbr7WElhY7duy47vqWLVuGZcuWdUFlRETNOHsxkbTwXlRERODsxURSw4BDRATOXkwkNQw4ROT2moxm/HCqefZiHp4ikgYGHCJye/vP1KLJaEGwnwqxoZz8k0gKGHCIyO3tOtE8ud+Y6N6dmmWdiLo/Bhwicnu7rsxePPbWIJErIaKuwoBDRG6tQteE4+UNkMma9+AQkTQw4BCRW2vZezOkjxoBvp4iV0NEXYUBh4jc2q4TzVdPjeXeGyJJYcAhIrdltgj4/kTL+TcMOERSwoBDRG7rSGk9ahuN6KHywPB+PcUuh4i6EAMOEbmtlvNvkgcEQqngn0MiKeFvNBG5rV08PEUkWQw4ROSWdE1GHCyuAwCMY8AhkhwGHCJyS3kna2C2CIgK8kV4gI/Y5RBRF2PAISK3ZD08Fc3Zi4mkiAGHiNyOIAhX3Z6Bh6eIpIgBh4jczunqSzhXexlKhQyj+geKXQ4ROQADDhG5nZa9NwkRAfBVeYhcDRE5AgMOEbkd6+0ZeHiKSLIYcIjIrehNZuSfqgEAjL2VJxgTSRUDDhG5lQNnanHZaEZQDxViQvzFLoeIHIQBh4jcyk7r7MVBkMtlIldDRI7CgENEbmXXL83n33D2YiJpY8AhIrdR1aDHsTIdZDLgjlt4/g2RlDHgEJHb+P5k88nFcWFqBPZQiVwNETkSAw4RuY3dJ1suD+feGyKpY8AhIrdgEX7bgzM2muffEEkdAw4RuYXSS0BtoxE9VB4YEdFL7HKIyMEYcIjILRyra74kPGlAIJQK/ukjkjr+lhORWzhe1/znjrdnIHIPTgk4K1asQFRUFLy8vBAfH4/du3d3OH7nzp2Ij4+Hl5cX+vfvj48++qjVmE2bNiE2NhYqlQqxsbHYvHmzo8onIhfX0GTC6YvN34/j+TdEbsHhAWfjxo1YuHAhFi1ahIKCAowZMwZTpkxBcXFxm+NPnz6Nu+66C2PGjEFBQQH+9Kc/4ZlnnsGmTZusY/Lz8zFz5kykpaXh0KFDSEtLw4wZM7B3715Hbw4RuaA9v16ARZAhIsAH/QJ9xC6HiJzA4QHnnXfewZw5czB37lzExMQgKysL4eHh+PDDD9sc/9FHH6Ffv37IyspCTEwM5s6diz/84Q94++23rWOysrKQkpKCjIwMDBo0CBkZGZgwYQKysrIcvTlE5IJaLg8fEx0ociVE5Cwejly5wWDAgQMH8PLLL9ssT01NRV5eXpvvyc/PR2pqqs2ySZMmYdWqVTAajVAqlcjPz8dzzz3Xakx7AUev10Ov11uf63Q6AIDRaITRaLR3sySppQ/sh3Ow384jCAJ2n2gOOEmRPdlzJ+Dn27ncqd/2bKNDA051dTXMZjM0Go3Nco1Gg/Ly8jbfU15e3uZ4k8mE6upqhIaGtjumvXVmZmZiyZIlrZbn5ubCx4e7q6+m1WrFLsGtsN+OV3kZOFfnAYVMwKXTBcgpLhC7JLfBz7dzuUO/GxsbOz3WoQGnhUxme8deQRBaLbve+GuX27POjIwMpKenW5/rdDqEh4cjNTUV/v7+ndsIiTMajdBqtUhJSYFSqRS7HMljv51nzZ5ioPA4ovwETJ3MfjsDP9/O5U79bjkC0xkODThBQUFQKBSt9qxUVla22gPTIiQkpM3xHh4eCAwM7HBMe+tUqVRQqVrfd0apVEr+w2Av9sS52G/H++HUBQDAoJ4C++1k7LdzuUO/7dk+h55k7Onpifj4+Fa7zbRaLZKTk9t8T1JSUqvxubm5SEhIsG5Ye2PaWycRuSeDyYL8X5tvzxDTUxC5GiJyJocfokpPT0daWhoSEhKQlJSElStXori4GPPnzwfQfPiotLQUq1evBgDMnz8f77//PtLT0zFv3jzk5+dj1apVWL9+vXWdzz77LMaOHYs33ngD9913H7Zs2YJt27bh+++/d/TmEJEL2X/2AhoNZgT6eiLMxyR2OUTkRA4PODNnzkRNTQ2WLl2KsrIyxMXFIScnBxEREQCAsrIymzlxoqKikJOTg+eeew4ffPABwsLC8N577+Ghhx6yjklOTsaGDRvwyiuv4NVXX8WAAQOwceNGJCYmOnpziMiF7Pql+eqpO24JhFzW+ZMTicj1OeUk4wULFmDBggVtvpadnd1q2bhx43Dw4MEO1zlt2jRMmzatK8ojIona9UsVAGDMLYHA+RKRqyEiZ+K9qIhIkqoa9Pi5rPmKiztu4QR/RO6GAYeIJGn3iea9N4PD/BHYo/VVlEQkbQw4RCRJLYenePdwIvfEgENEkmOx/HZ7hnEMOERuiQGHiCTn5zIdai4Z4OupwIh+vcQuh4hEwIBDRJKz88rhqaQBQfD04J85InfE33wikpyW82/G3RokciVEJBYGHCKSlIt6Ew6crQXAE4yJ3BkDDhFJSv6pGpgsAiICfRAR6Ct2OUQkEgYcIpIU6+Xh0dx7Q+TOGHCISFJ2neD8N0TEgENEEnK25hLO1jTCQy5D0gDenoHInTHgEJFktByeio/ohR4qp9xLmIi6KQYcIpKMnb80z17Mw1NExIBDRJJgMFmQf4q3ZyCiZgw4RCQJB4trcclgRqCvJ2JD/cUuh4hExoBDRJLQcv7NmOggyOUykashIrEx4BCRJPDycCK6GgMOEbm8qgY9jpTqAABjOMEfEYEBh4gk4PuTzXtvYkP90dtPJXI1RNQdMOAQkcvbxcvDiegaDDhE5NIsFgG7r5x/w8vDiagFAw4RubSfy3SovmiAr6cC8RG9xC6HiLoJBhwicmktV08lDQiEpwf/pBFRM/41ICKX1jL/Dc+/IaKrMeAQkcu6pDfhwNlaAMBYXh5ORFdhwCEil5V3qgZGs4B+AT6IDPIVuxwi6kYYcIjIZX17vAIAMH4g994QkS0GHCJySRaLgO3HKgEAE2I0IldDRN0NAw4RuaQj5+tR2aCHr6cCif0DxC6HiLoZBhwicknbruy9GXtrb6g8FCJXQ0TdjUMDTm1tLdLS0qBWq6FWq5GWloa6urp2xxuNRrz00ksYMmQIfH19ERYWhsceewznz5+3GXfnnXdCJpPZPGbNmuXITSGibmb7sebzb343KFjkSoioO3JowHn44YdRWFiIrVu3YuvWrSgsLERaWlq74xsbG3Hw4EG8+uqrOHjwIP7zn//gl19+wb333ttq7Lx581BWVmZ9fPzxx47cFCLqRsrqL+PoeR1kMmA8Aw4RtcHDUSs+duwYtm7dij179iAxMREA8I9//ANJSUkoKirCwIEDW71HrVZDq9XaLFu+fDlGjhyJ4uJi9OvXz7rcx8cHISEhjiqfiLqxb483H54aHt4TQT1493Aias1hASc/Px9qtdoabgBg1KhRUKvVyMvLazPgtKW+vh4ymQw9e/a0Wb5u3TqsXbsWGo0GU6ZMwWuvvQY/P78216HX66HX663PdTodgOZDYkaj0c4tk6aWPrAfzsF+3xzt0XIAwPhbgzrVQ/bbudhv53KnftuzjQ4LOOXl5QgObr3rODg4GOXl5Z1aR1NTE15++WU8/PDD8Pf3ty5/5JFHEBUVhZCQEBw5cgQZGRk4dOhQq70/LTIzM7FkyZJWy3Nzc+Hj49PJLXIP7fWQHIP9tp/BDPxwQgFABo+q48jJOd7p97LfzsV+O5c79LuxsbHTY+0OOIsXL24zLFxt3759AACZTNbqNUEQ2lx+LaPRiFmzZsFisWDFihU2r82bN8/6fVxcHKKjo5GQkICDBw9ixIgRrdaVkZGB9PR063OdTofw8HCkpqbaBCd3ZjQaodVqkZKSAqVSKXY5ksd+37jtxyph/LEQfXp6Yc5DYzr994T9dh7227ncqd8tR2A6w+6A89RTT133iqXIyEgcPnwYFRUVrV6rqqqCRtPxpFxGoxEzZszA6dOn8e233143hIwYMQJKpRInTpxoM+CoVCqoVK2P0yuVSsl/GOzFnjgX+22/HSdqAAATYzTw9PS0673st3Ox387lDv22Z/vsDjhBQUEICgq67rikpCTU19fjxx9/xMiRIwEAe/fuRX19PZKTk9t9X0u4OXHiBL777jsEBgZe92cdPXoURqMRoaGhnd8QInI5FotgPcGYsxcTUUccdpl4TEwMJk+ejHnz5mHPnj3Ys2cP5s2bh6lTp9qcYDxo0CBs3rwZAGAymTBt2jTs378f69atg9lsRnl5OcrLy2EwGAAAp06dwtKlS7F//36cOXMGOTk5mD59OoYPH47Ro0c7anOIqBvg7MVE1FkOnQdn3bp1GDJkCFJTU5GamoqhQ4dizZo1NmOKiopQX18PADh37hy++OILnDt3DsOGDUNoaKj1kZeXBwDw9PTE9u3bMWnSJAwcOBDPPPMMUlNTsW3bNigUnM2USMpaZi8eE83Zi4moYw67igoAAgICsHbt2g7HCIJg/T4yMtLmeVvCw8Oxc+fOLqmPiFxLy+zFE2I4uR8RdYz3oiIil8DZi4nIHgw4ROQSOHsxEdmDAYeIXML2Y7x6iog6jwGHiLq9ywYzfjhZDYDn3xBR5zDgEFG39/3JauhNFvTp6Y2BmrbvOUdEdDUGHCLq9lqunpoYE9ypWzMQETHgEFG3ZrEI2M7Zi4nITgw4RNStHTlfjyrOXkxEdmLAIaJujbMXE9GNYMAhom6NsxcT0Y1gwCGibouzFxPRjWLAIaJuq2VyP85eTET2YsAhom7rW149RUQ3iAGHiLolzl5MRDeDAYeIuiXOXkxEN4MBh4i6pa+PlAHg7MVEdGMYcIio22kympF7tPny8HtuCxO5GiJyRQw4RNTtfHu8Ehf1JvTp6Y0R/XqJXQ4RuSAGHCLqdr4oPA8AmHpbKORyHp4iIvsx4BBRt6JrMuLboubLw+/l4SkiukEMOETUreQerYDBZMEtwT0QG+ovdjlE5KIYcIioW9lSWAqgee8Nr54iohvFgENE3Ub1RT3yTtUA4OEpIro5DDhE1G3k/FQGs0XA0L5qRAb5il0OEbkwBhwi6jZarp7i3hsiulkMOETULZyrbcT+s7WQyTi5HxHdPAYcIuoWvjzUfGuGxKgAaPy9RK6GiFwdAw4RdQtfHGo5PNVH5EqISAoYcIhIdCcrG3CsTAelQoYpcSFil0NEEsCAQ0Siazm5eGx0b/Ty9RS5GiKSAgYcIhKVIAi/HZ4axpOLiahrODTg1NbWIi0tDWq1Gmq1Gmlpaairq+vwPbNnz4ZMJrN5jBo1ymaMXq/H008/jaCgIPj6+uLee+/FuXPnHLglROQoh8/V40xNI7yUckyM0YhdDhFJhEMDzsMPP4zCwkJs3boVW7duRWFhIdLS0q77vsmTJ6OsrMz6yMnJsXl94cKF2Lx5MzZs2IDvv/8eFy9exNSpU2E2mx21KUTkIC17b1JiQ+Cr8hC5GiKSCof9NTl27Bi2bt2KPXv2IDExEQDwj3/8A0lJSSgqKsLAgQPbfa9KpUJISNsnGtbX12PVqlVYs2YNJk6cCABYu3YtwsPDsW3bNkyaNKnrN4aIHMJsEfB/hzm5HxF1PYcFnPz8fKjVamu4AYBRo0ZBrVYjLy+vw4CzY8cOBAcHo2fPnhg3bhxef/11BAcHAwAOHDgAo9GI1NRU6/iwsDDExcUhLy+vzYCj1+uh1+utz3U6HQDAaDTCaDTe9LZKQUsf2A/nYL+b7T19ARU6Pfy9PJAU1dNh/WC/nYv9di536rc92+iwgFNeXm4NJVcLDg5GeXl5u++bMmUKpk+fjoiICJw+fRqvvvoqfve73+HAgQNQqVQoLy+Hp6cnevXqZfM+jUbT7nozMzOxZMmSVstzc3Ph4+Nj55ZJm1arFbsEt+Lu/d5wSg5Ajlh/A7bnbnX4z3P3fjsb++1c7tDvxsbGTo+1O+AsXry4zbBwtX379gEAZDJZq9cEQWhzeYuZM2dav4+Li0NCQgIiIiLw1Vdf4cEHH2z3fR2tNyMjA+np6dbnOp0O4eHhSE1Nhb+/f4fb4i6MRiO0Wi1SUlKgVCrFLkfy2G/AYLLgz2/uAGDCgrtvR1L/QIf9LPbbudhv53KnfrccgekMuwPOU089hVmzZnU4JjIyEocPH0ZFRUWr16qqqqDRdP5KidDQUERERODEiRMAgJCQEBgMBtTW1trsxamsrERycnKb61CpVFCpVK2WK5VKyX8Y7MWeOJc793vXyQrUXzYh2E+F0dEaKOTt/49PV3HnfouB/XYud+i3Pdtnd8AJCgpCUFDQdcclJSWhvr4eP/74I0aOHAkA2Lt3L+rr69sNIm2pqalBSUkJQkNDAQDx8fFQKpXQarWYMWMGAKCsrAxHjhzBm2++ae/mEJFIWq6euntoqFPCDRG5F4ddJh4TE4PJkydj3rx52LNnD/bs2YN58+Zh6tSpNicYDxo0CJs3bwYAXLx4ES+88ALy8/Nx5swZ7NixA/fccw+CgoLwwAMPAADUajXmzJmD559/Htu3b0dBQQEeffRRDBkyxHpVFRF1b40GE7Q/N+/h5dVTROQIDp10Yt26dXjmmWesVzzde++9eP/9923GFBUVob6+HgCgUCjw008/YfXq1airq0NoaCjGjx+PjRs3ws/Pz/qeZcuWwcPDAzNmzMDly5cxYcIEZGdnQ6FQOHJziKiLbDtWiUaDGf0CfDAsvKfY5RCRBDk04AQEBGDt2rUdjhEEwfq9t7c3vvnmm+uu18vLC8uXL8fy5ctvukYicr7NB5tnHr/3trAOLzogIrpRvBcVETnV6epL+K6oCjIZ8FB8X7HLISKJYsAhIqf6JO8MAGD8wGBEBfmKWwwRSRYDDhE5TUOTEf97oPnw1OOjI8UthogkjQGHiJzmfw+cw0W9CbcE98Adt1x/ugkiohvFgENETmGxCNbDU7OTI3lyMRE5FAMOETnFjl8qcaamEX5eHnhwRB+xyyEiiWPAISKn+PcPZwAAs24Ph4+nQ2eoICJiwCEixztZ2YDdJ6ohlwGPJUWKXQ4RuQEGHCJyuOwr595MjNEgPMBH3GKIyC0w4BCRQ9U3GrHpQCkAYDYvDSciJ2HAISKH+p/9JbhsNGNQiB+S+geKXQ4RuQkGHCJyGLNFwCf5ZwDw0nAici4GHCJymG3HKnCu9jJ6+ihx/3BeGk5EzsOAQ0QO8+8fTgMA/mtkP3gpFSJXQ0TuhAGHiBziWJkOe369AIVchrRREWKXQ0RuhgGHiByi5bYMkweHIKynt7jFEJHbYcAhoi534ZIBmwt4aTgRiYcBh4i63IZ9xdCbLIjr44+EiF5il0NEbogBh4i6lNFswZr8swCA2clRvDSciETBgENEXSr3aAXK6psQ6OuJqUNDxS6HiNwUAw4RdRlBELDq+18BAI8k8tJwIhIPAw4RdZlvjlbgYHEdVB5yPMJLw4lIRAw4RNQlDCYL/vvrYwCAeWP6Q+PvJXJFROTOGHCIqEus2XMWZ2oaEdRDhfl3DhC7HCJycww4RHTT6hoNeG/7CQDAC6m3oofKQ+SKiMjdMeAQ0U17b/tJ1F82YlCIH6YnhItdDhERAw4R3Zxfqy5idf4ZAMCiu2OgkHPeGyISHwMOEd2U//76OEwWAeMH9saY6N5il0NEBIABh4huwp5fa5D7cwUUchn+dFeM2OUQEVkx4BDRDbFYBPz1q58BAP81MhzRGj+RKyIi+g0DDhHdkM0FpThSqoOfygMLJ94qdjlERDYcGnBqa2uRlpYGtVoNtVqNtLQ01NXVdfgemUzW5uOtt96yjrnzzjtbvT5r1ixHbgoRXaXRYMJb3xQBAJ783S0I6qESuSIiIlsOnazi4Ycfxrlz57B161YAwBNPPIG0tDR8+eWX7b6nrKzM5vnXX3+NOXPm4KGHHrJZPm/ePCxdutT63NvbuwsrJ6KO/GPXaZTrmtC3lzdmJ0eKXQ4RUSsOCzjHjh3D1q1bsWfPHiQmJgIA/vGPfyApKQlFRUUYOHBgm+8LCQmxeb5lyxaMHz8e/fv3t1nu4+PTaiwROV6Frgkf7TwFAHhp8iDeUJOIuiWHBZz8/Hyo1WpruAGAUaNGQa1WIy8vr92Ac7WKigp89dVX+OSTT1q9tm7dOqxduxYajQZTpkzBa6+9Bj+/tk9y1Ov10Ov11uc6nQ4AYDQaYTQa7d00SWrpA/vhHK7c77e2HsdloxnDwtWYFBPkEtvgyv12Rey3c7lTv+3ZRocFnPLycgQHB7daHhwcjPLy8k6t45NPPoGfnx8efPBBm+WPPPIIoqKiEBISgiNHjiAjIwOHDh2CVqttcz2ZmZlYsmRJq+W5ubnw8fHpVC3uor0ekmO4Wr/PXQI2HVYAkGG8ugZff/212CXZxdX67erYb+dyh343NjZ2eqzdAWfx4sVthoWr7du3D0DzCcPXEgShzeVt+de//oVHHnkEXl62dyWeN2+e9fu4uDhER0cjISEBBw8exIgRI1qtJyMjA+np6dbnOp0O4eHhSE1Nhb+/f6dqkTqj0QitVouUlBQolUqxy5E8V+y3IAj4ffYBCLiAu4eEYMGMoWKX1Gmu2G9Xxn47lzv1u+UITGfYHXCeeuqp616xFBkZicOHD6OioqLVa1VVVdBoNNf9Obt370ZRURE2btx43bEjRoyAUqnEiRMn2gw4KpUKKlXrqzyUSqXkPwz2Yk+cy5X6nf3DaeT/egGeHnK8PCXGZeq+miv1WwrYb+dyh37bs312B5ygoCAEBQVdd1xSUhLq6+vx448/YuTIkQCAvXv3or6+HsnJydd9/6pVqxAfH4/bbrvtumOPHj0Ko9GI0NDQ628AEdmtsKQOr+ccA9B8YnF4AA/tElH35rB5cGJiYjB58mTMmzcPe/bswZ49ezBv3jxMnTrV5gTjQYMGYfPmzTbv1el0+OyzzzB37txW6z116hSWLl2K/fv348yZM8jJycH06dMxfPhwjB492lGbQ+S26hoNeHLdQRjNAqbEheAPoyPFLomI6LocOtHfunXrMGTIEKSmpiI1NRVDhw7FmjVrbMYUFRWhvr7eZtmGDRsgCAL+67/+q9U6PT09sX37dkyaNAkDBw7EM888g9TUVGzbtg0KBS9XJepKFouA5//nEErrLiMi0AdvTBva6XPoiIjE5NCJ/gICArB27doOxwiC0GrZE088gSeeeKLN8eHh4di5c2eX1EdEHft416/YfrwSnh5yrHhkBPy9pH18n4ikg/eiIqI27f21Bm/nNt+OYcm9gzE4TC1yRUREnceAQ0StVDXo8fT6ApgtAh4Y3gezbg8XuyQiIrsw4BCRDbNFwLMbClDZoEd0cA+8/kAcz7shIpfDgENENt7dfgJ5p2rgrVRgxSMj4OPp0FP1iIgcggGHiKx2/VKF5d+eAABkPjgE0Zq27+9GRNTdMeAQEQCgrP4yFm4shCAADyf2w/3D+4hdEhHRDWPAISIYTBY8/WkBLlwyYHCYP/48NVbskoiIbgoDDpGbazKa8cSa/dh/thZ+Kg+seGQEvJScNJOIXBvPHiRyYw1NRsz9ZD/2nr4AL6UcKx4dgYhAX7HLIiK6aQw4RG6q9pIBs//9Iw6dq4efygOrZt+OkVEBYpdFRNQlGHCI3FClrglpq35EUUUDevkosfoPiRjSlzMVE5F0MOAQuZmSC414dNVenK1phMZfhbVzEnk5OBFJDgMOkRs5VXURj/5zL8rqmxAe4I11c0ahX6CP2GUREXU5BhwiN3H0fD0eW/Ujai4ZcEtwD6ydk4gQtZfYZREROQQDDpEbOHC2FrP//SMamkwYHOaP1X8YicAeKrHLIiJyGAYcIgkTBAH/d7gML206jEaDGQkRvfCvx2+Hv5dS7NKIiByKAYdIoqoa9Hj18yPYerQcADAmOggfp8Xz5plE5Bb4l45IYgRBwBeHzuO1L46irtEID7kMT46/BU+OvwWeHpy8nIjcAwMOkYRU6pqw6PMj0P5cAQCIDfXHW9OHYnAY57ghIvfCgEMkAYIg4PPCUiz+4mfUXzZCqZDh6d9F4/+7cwCUCu61ISL3w4BD5OIqdE1YtPknbDtWCQCI6+OPt6bdhphQf5ErIyISDwMOkYu6pDdh/Y/FeG/7CeiaTFAqZFg48VY8MbY/99oQkdtjwCFyMZUNTfgk7wzW7ilG/WUjAGBoXzXemnYbBobwlgtERAADDpHLOFV1Ef/c/Ss2HSyFwWQBAEQF+eKJsf0xPb4vPLjXhojIigGHqJs7cLYWH+88Be2xCghC87Lh/Xrij2MHICVWA4VcJm6BRETdEAMOUTfU0GTEd0VVWJN/BvvO1FqXT4wJxh/HDUBCRC/IZAw2RETtYcAh6iYqG5qw7edKfHO0HHmnqmE0N++u8VTIcf/wMDwxtj9uCeY5NkREncGAQySiszWN+PaXanxztAIHi2uth6AAoH+QL+4aEoq0pAho/HnXbyIiezDgEDlRha4JhSV1OHCmBl8WKlCW/73N67eF90RqrAaTBmu4t4aI6CYw4BA5yCW9CT+V1qOwpA6HSupQWFKHsvqmq0bI4CGXYVT/QEwarMHEWA1C1d6i1UtEJCUMOEQ3qcloxtmaRpyuvoQzNZfwa9VFHD5Xj18qGmARbMfKZcCtGj8M7eMPz/piLJwxEUH+PuIUTkQkYQ4NOK+//jq++uorFBYWwtPTE3V1ddd9jyAIWLJkCVauXIna2lokJibigw8+wODBg61j9Ho9XnjhBaxfvx6XL1/GhAkTsGLFCvTt29eBW0PuymS2oPqiAZUNTSivb2oOMzWXcKa6+XHeZq+MrVC1F4aF98Rt4T0xLLwnhvRRw1flAaPRiJycs1B7K524JURE7sOhAcdgMGD69OlISkrCqlWrOvWeN998E++88w6ys7Nx66234q9//StSUlJQVFQEP7/mcxIWLlyIL7/8Ehs2bEBgYCCef/55TJ06FQcOHIBCoXDkJpEEGM0W6C4bUX/No67RiMqGJlTq9KhsaH5UNTSh5pLB5uTftvh7eSAqyBeRQb6IDPRFbJg/hoX35MnBREQicWjAWbJkCQAgOzu7U+MFQUBWVhYWLVqEBx98EADwySefQKPR4NNPP8Uf//hH1NfXY9WqVVizZg0mTpwIAFi7di3Cw8Oxbds2TJo0ySHbQo5lsQgwWiwwmQWYzL99bzBZYDCb0WS0QG+ywGCyQG8yX/W9BZeNZlw2mHBJb8ZloxmNBhMa9WY0GsxoNJrRqDfhot5kDTWXDGa765PLgKAeKmj8vdAv0AdRgc1hJirIB5GBvgjw9eS8NERE3Ui3Ogfn9OnTKC8vR2pqqnWZSqXCuHHjkJeXhz/+8Y84cOAAjEajzZiwsDDExcUhLy+vzYCj1+uh1+utz3U6HQDAaDTCaDR2Wf01F/VYsfO0zbJ2/8f/ql0CQtuLIVx5pWVZ2+MEm9cFwfZ9wpVvWr5vef2315qfmy0WVJTL8VVdAQQ0/0NtufI+iyBcea8Ai/DbcrNFgOWqZRZBgMXy23Kz5cpDEGCxCDBZmse2LLcGmivPnc1XpYDaSwl/byXU3h5QeyvRu4cKvf1UCPbzRG8/FXr3UCHYT4UAX88OZww2mUx2/eyWz11Xfv6ofey3c7HfzuVO/bZnG7tVwCkvLwcAaDQam+UajQZnz561jvH09ESvXr1ajWl5/7UyMzOte5OulpubCx+frjvBs/IysLqwW7XUTnLgQpXYRVjJZQI8ZIBSDnjIAA958+O3582veyoAlbz5q6e85blgs1ylAHwUArw9AB8PwNsDUMhMAPStf3Bj86OxAjiL5oejaLVaB66drsV+Oxf77Vzu0O/GxsZOj7X7X+PFixe3GRautm/fPiQkJNi7aqtrd/ULgnDd3f8djcnIyEB6err1uU6nQ3h4OFJTU+Hv73/DdV7rwiUDLqhb/3MoQ9t1XV2urN3lMpsBtuNkNstksqu/l101ruU1mXWM9b0yQC6TwWI2o6joOGJjYqBQKCCXya681vwT5LLfxjY/YB2jkMuuGi+zPveQyyCXAx5yOeSyK1/laF4uk0GpkMND0TzOQyGHUm67TMqHfIxGI7RaLVJSUqBU8kRjR2O/nYv9di536nfLEZjOsDvgPPXUU5g1a1aHYyIjI+1dLQAgJCQEQPNemtDQUOvyyspK616dkJAQGAwG1NbW2uzFqaysRHJycpvrValUUKlUrZYrlcou/TBoeirx0pTYLlufMxmNRuTUH8NdSZGS/wXpTrr6M0gdY7+di/12Lnfotz3bZ3fACQoKQlBQkL1v65SoqCiEhIRAq9Vi+PDhAJqvxNq5cyfeeOMNAEB8fDyUSiW0Wi1mzJgBACgrK8ORI0fw5ptvOqQuIiIici0OPWGkuLgYFy5cQHFxMcxmMwoLCwEAt9xyC3r06AEAGDRoEDIzM/HAAw9AJpNh4cKF+Nvf/obo6GhER0fjb3/7G3x8fPDwww8DANRqNebMmYPnn38egYGBCAgIwAsvvIAhQ4ZYr6oiIiIi9+bQgPPnP/8Zn3zyifV5y16Z7777DnfeeScAoKioCPX19dYxL774Ii5fvowFCxZYJ/rLzc21zoEDAMuWLYOHhwdmzJhhnegvOzubc+AQERERAAcHnOzs7OvOgSNcM4OaTCbD4sWLsXjx4nbf4+XlheXLl2P58uVdUCURERFJjVzsAoiIiIi6GgMOERERSQ4DDhEREUkOAw4RERFJDgMOERERSQ4DDhEREUkOAw4RERFJDgMOERERSQ4DDhEREUmOQ2cy7q5aZk+257brUmc0GtHY2AidTif5u9F2B+y3c7HfzsV+O5c79bvl3+1r74LQFrcMOA0NDQCA8PBwkSshIiIiezU0NECtVnc4RiZ0JgZJjMViwfnz5+Hn5weZTCZ2Od2CTqdDeHg4SkpK4O/vL3Y5ksd+Oxf77Vzst3O5U78FQUBDQwPCwsIgl3d8lo1b7sGRy+Xo27ev2GV0S/7+/pL/BelO2G/nYr+di/12Lnfp9/X23LTgScZEREQkOQw4REREJDkMOAQAUKlUeO2116BSqcQuxS2w387FfjsX++1c7Hfb3PIkYyIiIpI27sEhIiIiyWHAISIiIslhwCEiIiLJYcAhIiIiyWHAoXbp9XoMGzYMMpkMhYWFYpcjSWfOnMGcOXMQFRUFb29vDBgwAK+99hoMBoPYpUnGihUrEBUVBS8vL8THx2P37t1ilyRZmZmZuP322+Hn54fg4GDcf//9KCoqErsst5CZmQmZTIaFCxeKXUq3wYBD7XrxxRcRFhYmdhmSdvz4cVgsFnz88cc4evQoli1bho8++gh/+tOfxC5NEjZu3IiFCxdi0aJFKCgowJgxYzBlyhQUFxeLXZok7dy5E08++ST27NkDrVYLk8mE1NRUXLp0SezSJG3fvn1YuXIlhg4dKnYp3QovE6c2ff3110hPT8emTZswePBgFBQUYNiwYWKX5RbeeustfPjhh/j111/FLsXlJSYmYsSIEfjwww+ty2JiYnD//fcjMzNTxMrcQ1VVFYKDg7Fz506MHTtW7HIk6eLFixgxYgRWrFiBv/71rxg2bBiysrLELqtb4B4caqWiogLz5s3DmjVr4OPjI3Y5bqe+vh4BAQFil+HyDAYDDhw4gNTUVJvlqampyMvLE6kq91JfXw8A/Dw70JNPPom7774bEydOFLuUbsctb7ZJ7RMEAbNnz8b8+fORkJCAM2fOiF2SWzl16hSWL1+Ov//972KX4vKqq6thNpuh0Whslms0GpSXl4tUlfsQBAHp6em44447EBcXJ3Y5krRhwwYcPHgQ+/btE7uUbol7cNzE4sWLIZPJOnzs378fy5cvh06nQ0ZGhtglu7TO9vtq58+fx+TJkzF9+nTMnTtXpMqlRyaT2TwXBKHVMup6Tz31FA4fPoz169eLXYoklZSU4Nlnn8XatWvh5eUldjndEs/BcRPV1dWorq7ucExkZCRmzZqFL7/80uYfALPZDIVCgUceeQSffPKJo0uVhM72u+UP0/nz5zF+/HgkJiYiOzsbcjn/3+NmGQwG+Pj44LPPPsMDDzxgXf7ss8+isLAQO3fuFLE6aXv66afx+eefY9euXYiKihK7HEn6/PPP8cADD0ChUFiXmc1myGQyyOVy6PV6m9fcEQMO2SguLoZOp7M+P3/+PCZNmoT//d//RWJiIvr27StiddJUWlqK8ePHIz4+HmvXrnX7P0pdKTExEfHx8VixYoV1WWxsLO677z6eZOwAgiDg6aefxubNm7Fjxw5ER0eLXZJkNTQ04OzZszbLHn/8cQwaNAgvvfQSDwuC5+DQNfr162fzvEePHgCAAQMGMNw4wPnz53HnnXeiX79+ePvtt1FVVWV9LSQkRMTKpCE9PR1paWlISEhAUlISVq5cieLiYsyfP1/s0iTpySefxKeffootW7bAz8/Peq6TWq2Gt7e3yNVJi5+fX6sQ4+vri8DAQIabKxhwiESUm5uLkydP4uTJk60CJHeu3ryZM2eipqYGS5cuRVlZGeLi4pCTk4OIiAixS5Oklsvx77zzTpvl//73vzF79mznF0RujYeoiIiISHJ4JiMRERFJDgMOERERSQ4DDhEREUkOAw4RERFJDgMOERERSQ4DDhEREUkOAw4RERFJDgMOERERSQ4DDhEREUkOAw4RERFJDgMOERERSQ4DDhEREUnO/wN9DLF6gnWttQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Activation Function\n",
    "plt.plot(np.arange(-5,5,0.2), np.tanh(np.arange(-5,5,0.2))); plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a017e6-9374-4e19-b140-9d2799183061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d994812c-53b8-4eda-8d93-e0f53354dd2e",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "![output layer](https://github.com/user-attachments/assets/50033790-4164-4154-8062-aaeef0d56df6)\n",
    "\n",
    "### How to train a Neural Network\n",
    "\n",
    "#### 1. Forward Propagation\n",
    "- Input data is fed into the input layer.\n",
    "- The data is then passed through each layer, from input to hidden to output.\n",
    "- At each neuron, the input data is multiplied by the corresponding weights, added to the bias, and passed through the activation function to produce the output: $ z = W \\cdot x + b $\n",
    "- This process continues until the data reaches the output layer, producing the final output of the network.\n",
    "\n",
    "#### 2. Compute Loss\n",
    "- After froward propagation, the loss is calculated to deterimne the accuracy of the current prediction.\n",
    "- The _loss function_ measures the difference between the predicted output and the desired target values.\n",
    "- Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.\n",
    "\n",
    "#### 3. Backpropagation\n",
    "- Once the loss is knows, backpropagation is performed to adjust the weights and biases to minimize the loss.\n",
    "- This is perfomred by calculating the gradient of the loss function with respect to each weight and bias using the chain rule.\n",
    "- The gradients are then used to update the weights and biases in the opposite direction of the gradient (by a *learning rate* amount), a process known as _gradient descent_.\n",
    "\n",
    "#### 4. Repeat\n",
    "- Forward propagation and backpropagation will be performed over many iterations, adjusting the weights and biases each time to minimize the loss.\n",
    "- Training continues until the loss converges to a minimum value or a specified number of iterations (epochs) is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b86d2d-4c35-45d2-9e96-73ea8668ddae",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afca6279-9b01-4dc1-9ed7-6f4875dd9687",
   "metadata": {},
   "source": [
    "# Transformer Architecture\n",
    "\n",
    "Building upon the biological and mathematical representation of the neuron, the Transformer architecture has emerged as a game-changer in the field of natural language processing. The transformer has given rise to the GenAI movement, a new era of artificial intelligence that is transforming industries and redefining the boundaries of human-machine collaboration.\n",
    "\n",
    "<img width=\"706\" alt=\"Transformers\" src=\"https://github.com/user-attachments/assets/f03692ed-7b3b-484f-b41b-b221376cd5a0\">\n",
    "\n",
    "Source : https://www.coursera.org/learn/generative-ai-with-llms\n",
    "\n",
    "The Transformer architecture was first introduced in the groundbreaking paper \"[Attention is All You Need](https://arxiv.org/pdf/1706.03762v1)\" by Vaswani et al. in 2017. This seminal work proposed a novel approach to sequence-to-sequence learning, which revolutionized the field of natural language processing (NLP) and beyond. The authors challenged the traditional encoder-decoder architecture, which relied on recurrent neural networks (RNNs) or convolutional neural networks (CNNs) to process sequential data. Instead, they proposed a purely attention-based model, where the encoder and decoder are composed of identical layers, and the attention mechanism is used to weigh the importance of different input elements when computing the output.\n",
    "\n",
    "<img width=\"485\" alt=\"Transformer Architecture\" src=\"https://github.com/user-attachments/assets/d1a81768-1a62-4d86-b48b-51c27ca80aa7\">\n",
    "\n",
    "\n",
    "Example Translation:\n",
    "\n",
    "\n",
    "<img width=\"640\" alt=\"Translation Example\" src=\"https://github.com/user-attachments/assets/f722b1ba-4660-4782-8ece-aa422bea8a7c\">\n",
    "\n",
    "<img width=\"640\" alt=\"Decoder\" src=\"https://jalammar.github.io/images/t/transformer_decoding_2.gif\">\n",
    "\n",
    "Source: https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "\n",
    "The Transformer architecture relies on self-attention, a mechanism that allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This is achieved through a series of linear transformations, which compute the attention weights, and a softmax function, which normalizes these weights to ensure they sum to 1. The attention weights are then used to compute the weighted sum of the input elements, which is used as input to the next layer. This process is repeated multiple times, allowing the model to capture long-range dependencies and contextual relationships in the input sequence. The Transformer architecture has since become a cornerstone of modern NLP, and its variants have been applied to a wide range of tasks, including machine translation, text classification, and question answering.\n",
    "\n",
    "![animal_](https://github.com/user-attachments/assets/20aeade6-b261-4f82-a818-bf2ed005dd6e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d0272-5147-4998-983c-7f6c38c4f9a3",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "1. **Word Embedding Vector**: Converts input tokens into dense vectors.\n",
    "2. **Word Position Encoding Vector**: Adds positional information to the token embeddings.\n",
    "3. **Multi-Headed Attention Block**: Allows the model to focus on different parts of the input sequence simultaneously.\n",
    "4. **Layer Normalization**: Stabilizes and accelerates training by normalizing the inputs to each layer.\n",
    "5. **Feed Forward** - MLP (Multi-Layer Perceptron): Applies non-linear transformations to the embeddings.\n",
    "6. **Output Layer** (Softmax): Produces the final probability distribution over the vocabulary for predictions.\n",
    "\n",
    "<img width=\"800\" alt=\"image\" src=\"https://github.com/user-attachments/assets/c5ff56ca-f985-4c92-9588-c845b05fbf7f\">\n",
    "\n",
    "Source: Visualizing Attention, a Transformer's Heart - https://www.3blue1brown.com/lessons/attention\n",
    "\n",
    "## Attention Head\n",
    "\n",
    "<img width=\"250\" alt=\"image\" src=\"https://github.com/user-attachments/assets/dd43c2c1-b420-46e6-9feb-c0f84f50ea9c\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098d7cd-f935-49bc-953f-2b4a9f49d90b",
   "metadata": {},
   "source": [
    "## Multi-Headed Attention Block\n",
    "\n",
    "### Q, K and V Vectors\n",
    "\n",
    "* Q = Query - what I'm looking for (which tokens should pay attention to me)\n",
    "* K = Key - what I contain (that other tokens are looking for)\n",
    "* V = Value - the actual content or information of this token\n",
    "\n",
    "The dot product bettwen K @ Q of two tokens show how much \"attention\" those two have. This is the attention score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4e467cb-0a5a-4793-8a72-cfd34238ac9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             the       brown     fox       jumped    over      the       lazy      dog       \n",
      "the       \u001b[31m   1.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\n",
      "brown     \u001b[31m   0.3966 \u001b[0m\u001b[31m   0.6034 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\n",
      "fox       \u001b[31m   0.3069 \u001b[0m\u001b[31m   0.2892 \u001b[0m\u001b[31m   0.4039 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\n",
      "jumped    \u001b[31m   0.3233 \u001b[0m\u001b[31m   0.2175 \u001b[0m\u001b[31m   0.2443 \u001b[0m\u001b[31m   0.2149 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\n",
      "over         0.1479 \u001b[0m\u001b[31m   0.2034 \u001b[0m   0.1663 \u001b[0m   0.1455 \u001b[0m\u001b[31m   0.3369 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\n",
      "the          0.1259 \u001b[0m\u001b[31m   0.2490 \u001b[0m   0.1324 \u001b[0m   0.1062 \u001b[0m\u001b[31m   0.3141 \u001b[0m   0.0724 \u001b[0m\u001b[34m   0.0000 \u001b[0m\u001b[34m   0.0000 \u001b[0m\n",
      "lazy         0.1598 \u001b[0m   0.1990 \u001b[0m   0.1140 \u001b[0m   0.1125 \u001b[0m   0.1418 \u001b[0m   0.1669 \u001b[0m   0.1061 \u001b[0m\u001b[34m   0.0000 \u001b[0m\n",
      "dog          0.0845 \u001b[0m   0.1197 \u001b[0m   0.1078 \u001b[0m   0.1537 \u001b[0m   0.1086 \u001b[0m   0.1146 \u001b[0m   0.1558 \u001b[0m   0.1553 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Self Attention Mechanism\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Parameters for Tensor\n",
    "B,T,C = 4, 8, 32 # B=batch-size, T=time/sequence-length, C=channels/embedding-size\n",
    "head_size = 16   # The number of dimensions each attention head projects to\n",
    "\n",
    "# Generate a random tensor x of shape (4, 8, 32)\n",
    "torch.manual_seed(1337)\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Creste fully connected layers that transform the input from C dimensions (32)\n",
    "# down to head_size dimensions (16). The idea is that each token \n",
    "# will be projected into a lower-dimensional space to compute attention.\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# Computing Key, Query, and Value\n",
    "k = key(x)      # key vector for each token (what I contain)\n",
    "q = query(x)    # query vectors for each token (what I'm looking for)\n",
    "v = value(x)    # value\n",
    "\n",
    "# Compute how much attnetion each element should pay to every other token\n",
    "w = q @ k.transpose(-2, -1) * (head_size**-0.5)  # scaling factor from the paper\n",
    "\n",
    "# Mask Future Tokens (Decoder-Specific)\n",
    "# for decoder, we mask out \"future\" words to the right\n",
    "# Note: for encoder, we would include all words \n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "w = w.masked_fill(tril == 0, float('-inf')) # decoder block only\n",
    "w = F.softmax(w, dim=-1)\n",
    "\n",
    "# The output of the attention mechanism\n",
    "out = w @ v\n",
    "\n",
    "# Example - Examine attention weights (untrained random values)\n",
    "from termcolor import colored\n",
    "\n",
    "words = \"the brown fox jumped over the lazy dog\".split()\n",
    "w_list = w[0].tolist()\n",
    "\n",
    "print(\"             \" + \"\".join(f\"{word:9} \" for word in words))\n",
    "\n",
    "for i, (word, weights) in enumerate(zip(words, w_list)):\n",
    "    print(f\"{word:10}\", end=\"\")\n",
    "    for weight in weights:\n",
    "        color = 'red' if weight > 0.2 else 'blue' if weight < 0.05 else None\n",
    "        print(colored(f\"{weight:9.4f} \", color), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7321e573-348b-48b2-9d23-598f750b6c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Attention Mechanism - Query from x, but Key and Values from encoder blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda835a-428b-4b0d-b83e-ef22ecff76fe",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "In a deep learning network, using a non-linear activation function like tanh() can lead to issues such as vanishing gradients, where gradients become very small and can hinder learning. This can make the network’s weights ineffective if not addressed. Normalization layers, such as Layer Normalization, help mitigate this issue by normalizing the activations, which can stabilize and accelerate training by maintaining a more consistent distribution of activations throughout the network. \n",
    "\n",
    "## Feed Forward\n",
    "\n",
    "The Feed Forward Layer is an MLP (Multi-Layer Perceptron): MLPs in Transformers enhance the model's capacity to learn and represent complex relationships by applying non-linear transformations to the outputs of the self-attention mechanisms. They play a critical role in ensuring that the model can capture a wide range of patterns and interactions within the data.\n",
    "\n",
    "## Output Layer\n",
    "\n",
    "The output layer consist of all the possible word. The NN predicts the probability of each word via the softmax value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2df349",
   "metadata": {},
   "source": [
    "# All about Tokens\n",
    "\n",
    "Training an LLM on natural language requires that we convert the language to numeric \"signals\" to feed into the neural net. These are called tokens. These tokens can be characters, words or even parts of words. They are the atomic components of language.\n",
    "\n",
    "<img width=\"532\" alt=\"image\" src=\"https://github.com/user-attachments/assets/bb90fbe5-78ec-45b3-9678-6a95c9baa386\">\n",
    "\n",
    "Source: https://platform.openai.com/tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d71352-adf5-4fd5-a4a7-68c1c8a65e19",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First some dependencies need to be installed\n",
    "!pip install tiktoken\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install tqdm\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef6ecee2-0261-49da-8085-053bcda02680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464, 2068, 7586, 21831, 11687, 625, 262, 16931, 3013, 3255, 3290, 13, 50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "phrase = \"The quick brown fox jumped over the lazy snoring dog.\"\n",
    "tokens = enc.encode_ordinary(phrase)\n",
    "# Add end of text token\n",
    "tokens.append(enc.eot_token)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "defa1f80-eafe-4c86-a90b-e80420e1ae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The\", \" quick\", \" brown\", \" fox\", \" jumped\", \" over\", \" the\", \" lazy\", \" sn\", \"oring\", \" dog\", \".\", \"<|endoftext|>\", "
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(f'\"{enc.decode([token])}\", ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f31f8c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X  ->  Y\n",
      "The -> \" quick\"\n",
      "The| quick -> \" brown\"\n",
      "The| quick| brown -> \" fox\"\n",
      "The| quick| brown| fox -> \" jumped\"\n",
      "The| quick| brown| fox| jumped -> \" over\"\n",
      "The| quick| brown| fox| jumped| over -> \" the\"\n",
      "The| quick| brown| fox| jumped| over| the -> \" lazy\"\n",
      "The| quick| brown| fox| jumped| over| the| lazy -> \" sn\"\n",
      "The| quick| brown| fox| jumped| over| the| lazy| sn -> \"oring\"\n",
      "The| quick| brown| fox| jumped| over| the| lazy| sn|oring -> \" dog\"\n",
      "The| quick| brown| fox| jumped| over| the| lazy| sn|oring| dog -> \".\"\n",
      "The| quick| brown| fox| jumped| over| the| lazy| sn|oring| dog|. -> \"<|endoftext|>\"\n"
     ]
    }
   ],
   "source": [
    "# TRAINING: The target is the next token in the sequence\n",
    "\n",
    "block_size = 12 # context length: how many words do we take to predict the next word?\n",
    "\n",
    "print(\" X  ->  Y\")\n",
    "predictions = tokens[:-1]\n",
    "targets = tokens[1:] + [enc.encode_ordinary(\"\")]\n",
    "for x in range(len(predictions)):\n",
    "    print('|'.join([f'{enc.decode([p])}' for p in predictions[:x+1]]), end='')\n",
    "    print(f' -> \"{enc.decode([targets[x]])}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd5cfa17-617d-45af-a665-f78059417d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many r's do you see in strawberry?\n"
     ]
    }
   ],
   "source": [
    "item = [81]\n",
    "array = [301, 1831, 8396]\n",
    "print(f\"How many {enc.decode(item)}'s do you see in {enc.decode(array)}?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697ffca-c9d8-44a1-9d18-e595d18c1a5f",
   "metadata": {},
   "source": [
    "# Tokenize Training Data\n",
    "\n",
    "We will use the Tiny Shakespeare dataset to train a model. See the [shakespeare.txt](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) file.\n",
    "\n",
    "Divide data into two parts:\n",
    "- 90% Training\n",
    "- 10% Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbe64b04-e900-4a84-9792-fdbe426049a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 301,966 tokens\n",
      "val has 36,059 tokens\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "input_file_path = 'data/tinyshakespeare.txt'\n",
    "data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "\n",
    "# Download and Load\n",
    "if not os.path.exists(input_file_path):\n",
    "    response = requests.get(data_url)\n",
    "    # Remove non-ASCII characters\n",
    "    # response = ''.join(char for char in response.text if ord(char) < 128)\n",
    "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(response)\n",
    "        \n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "n = len(data)\n",
    "\n",
    "# Use 90% for training and 10% for validation\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# Encode with tiktoken gpt2\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# Export to bin files for training\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile('data/train.bin')\n",
    "val_ids.tofile('data/val.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7806ff78-2c68-418b-8877-a2b3d05bc059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,\n",
       "          11,  3285,   502,  2740,    13,   198,   198,  3237,    25,\n",
       "         198,  5248,   461,    11,  2740,    13,   198,   198,  5962,\n",
       "       22307,    25,   198,  1639,   389,   477, 12939,  2138,   284,\n",
       "        4656,   621,   284,  1145,   680,    30,   198,   198,  3237,\n",
       "          25,   198,  4965,  5634,    13], dtype=uint16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54a3c098-51e9-44eb-96dd-002ff11a31a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be\n"
     ]
    }
   ],
   "source": [
    "print(enc.decode(train_ids[:128]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "169c1d42-47b0-4a6b-9b5e-76ee74e5756b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# GPT Model Class\n",
    "\n",
    "We will use the `model.py` file contains a GPT model class that was created by Andrej Karpathy. It was modeled after the one created by OpenAI.\n",
    "\n",
    "* Attention Is All You Need - https://arxiv.org/pdf/1706.03762\n",
    "* OpenAI GPT-2 - https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "* nanoGPT - https://github.com/karpathy/nanoGPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a50c5c-2be8-4c98-b40f-ef90bab2516c",
   "metadata": {},
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1489d614-51e7-454d-98a5-30c8767ebc18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 15,360\n",
      "Initializing a new model from scratch\n",
      "Using vocab_size of GPT-2 of 50304 (50257 rounded up for efficiency)\n",
      "Number of parameters: 3.42M\n",
      "Layers:\n",
      "0 transformer.wte.weight\n",
      "1 transformer.wpe.weight\n",
      "2 transformer.h.0.ln_1.weight\n",
      "3 transformer.h.0.attn.c_attn.weight\n",
      "4 transformer.h.0.attn.c_proj.weight\n",
      "5 transformer.h.0.ln_2.weight\n",
      "6 transformer.h.0.mlp.c_fc.weight\n",
      "7 transformer.h.0.mlp.c_proj.weight\n",
      "8 transformer.h.1.ln_1.weight\n",
      "9 transformer.h.1.attn.c_attn.weight\n",
      "10 transformer.h.1.attn.c_proj.weight\n",
      "11 transformer.h.1.ln_2.weight\n",
      "12 transformer.h.1.mlp.c_fc.weight\n",
      "13 transformer.h.1.mlp.c_proj.weight\n",
      "14 transformer.h.2.ln_1.weight\n",
      "15 transformer.h.2.attn.c_attn.weight\n",
      "16 transformer.h.2.attn.c_proj.weight\n",
      "17 transformer.h.2.ln_2.weight\n",
      "18 transformer.h.2.mlp.c_fc.weight\n",
      "19 transformer.h.2.mlp.c_proj.weight\n",
      "20 transformer.h.3.ln_1.weight\n",
      "21 transformer.h.3.attn.c_attn.weight\n",
      "22 transformer.h.3.attn.c_proj.weight\n",
      "23 transformer.h.3.ln_2.weight\n",
      "24 transformer.h.3.mlp.c_fc.weight\n",
      "25 transformer.h.3.mlp.c_proj.weight\n",
      "26 transformer.ln_f.weight\n",
      "num decayed parameter tensors: 18, with 3,418,112 parameters\n",
      "num non-decayed parameter tensors: 9, with 576 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "step 0: train loss 10.8197, val loss 10.8165\n",
      "iter 0: loss 10.8136, time 16546.40ms, mfu -100.00%\n",
      "iter 10: loss 10.8195, time 126.70ms, mfu 0.80%\n",
      "iter 20: loss 10.7911, time 132.53ms, mfu 0.79%\n",
      "iter 30: loss 10.8095, time 128.25ms, mfu 0.79%\n",
      "iter 40: loss 10.7644, time 128.78ms, mfu 0.79%\n",
      "iter 50: loss 10.7266, time 126.42ms, mfu 0.79%\n",
      "iter 60: loss 10.6919, time 127.33ms, mfu 0.79%\n",
      "iter 70: loss 10.6685, time 127.49ms, mfu 0.79%\n",
      "iter 80: loss 10.5965, time 126.66ms, mfu 0.79%\n",
      "iter 90: loss 10.5562, time 129.05ms, mfu 0.79%\n",
      "step 100: train loss 10.5333, val loss 10.5307\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 100: loss 10.5423, time 654.32ms, mfu 0.73%\n",
      "iter 110: loss 10.4646, time 134.18ms, mfu 0.77%\n",
      "iter 120: loss 10.4458, time 127.41ms, mfu 0.79%\n",
      "iter 130: loss 10.4100, time 127.28ms, mfu 0.79%\n",
      "iter 140: loss 10.3603, time 126.50ms, mfu 0.80%\n",
      "iter 150: loss 10.3367, time 126.61ms, mfu 0.80%\n",
      "iter 160: loss 10.2838, time 127.29ms, mfu 0.80%\n",
      "iter 170: loss 10.2114, time 126.29ms, mfu 0.80%\n",
      "iter 180: loss 10.1725, time 129.85ms, mfu 0.79%\n",
      "iter 190: loss 10.0836, time 129.01ms, mfu 0.79%\n",
      "step 200: train loss 10.0636, val loss 10.0735\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 200: loss 10.0685, time 659.17ms, mfu 0.73%\n",
      "iter 210: loss 9.9852, time 126.75ms, mfu 0.77%\n",
      "iter 220: loss 9.9389, time 126.41ms, mfu 0.79%\n",
      "iter 230: loss 9.8716, time 126.24ms, mfu 0.80%\n",
      "iter 240: loss 9.7913, time 126.27ms, mfu 0.80%\n",
      "iter 250: loss 9.7255, time 127.50ms, mfu 0.79%\n",
      "iter 260: loss 9.6378, time 127.93ms, mfu 0.78%\n",
      "iter 270: loss 9.5634, time 126.80ms, mfu 0.79%\n",
      "iter 280: loss 9.4626, time 126.72ms, mfu 0.79%\n",
      "iter 290: loss 9.4204, time 127.91ms, mfu 0.79%\n",
      "step 300: train loss 9.3317, val loss 9.3645\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 300: loss 9.3383, time 661.57ms, mfu 0.73%\n",
      "iter 310: loss 9.2110, time 127.51ms, mfu 0.77%\n",
      "iter 320: loss 9.1334, time 127.34ms, mfu 0.79%\n",
      "iter 330: loss 9.1092, time 127.41ms, mfu 0.79%\n",
      "iter 340: loss 8.9441, time 126.59ms, mfu 0.79%\n",
      "iter 350: loss 8.9058, time 126.57ms, mfu 0.80%\n",
      "iter 360: loss 8.7614, time 126.87ms, mfu 0.80%\n",
      "iter 370: loss 8.7332, time 126.59ms, mfu 0.80%\n",
      "iter 380: loss 8.6406, time 126.50ms, mfu 0.80%\n",
      "iter 390: loss 8.5324, time 126.18ms, mfu 0.80%\n",
      "step 400: train loss 8.4180, val loss 8.4930\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 400: loss 8.4186, time 663.49ms, mfu 0.74%\n",
      "iter 410: loss 8.2859, time 126.87ms, mfu 0.78%\n",
      "iter 420: loss 8.2240, time 127.77ms, mfu 0.79%\n",
      "iter 430: loss 8.1756, time 127.70ms, mfu 0.79%\n",
      "iter 440: loss 8.0505, time 126.50ms, mfu 0.79%\n",
      "iter 450: loss 7.9897, time 126.55ms, mfu 0.79%\n",
      "iter 460: loss 7.9662, time 128.60ms, mfu 0.80%\n",
      "iter 470: loss 7.9001, time 127.08ms, mfu 0.80%\n",
      "iter 480: loss 7.8551, time 126.76ms, mfu 0.80%\n",
      "iter 490: loss 7.5694, time 128.27ms, mfu 0.80%\n",
      "step 500: train loss 7.5249, val loss 7.6174\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 500: loss 7.6336, time 664.81ms, mfu 0.73%\n",
      "iter 510: loss 7.4317, time 128.92ms, mfu 0.77%\n",
      "iter 520: loss 7.2718, time 128.20ms, mfu 0.78%\n",
      "iter 530: loss 7.2146, time 127.07ms, mfu 0.79%\n",
      "iter 540: loss 7.2343, time 129.33ms, mfu 0.79%\n",
      "iter 550: loss 7.1286, time 127.51ms, mfu 0.79%\n",
      "iter 560: loss 7.1733, time 128.69ms, mfu 0.79%\n",
      "iter 570: loss 6.9764, time 127.35ms, mfu 0.79%\n",
      "iter 580: loss 7.0939, time 127.02ms, mfu 0.80%\n",
      "iter 590: loss 6.8468, time 126.33ms, mfu 0.80%\n",
      "step 600: train loss 6.7730, val loss 6.8884\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 600: loss 6.7805, time 663.16ms, mfu 0.73%\n",
      "iter 610: loss 6.7574, time 127.20ms, mfu 0.78%\n",
      "iter 620: loss 6.6341, time 126.18ms, mfu 0.79%\n",
      "iter 630: loss 6.6342, time 127.56ms, mfu 0.80%\n",
      "iter 640: loss 6.5243, time 126.16ms, mfu 0.79%\n",
      "iter 650: loss 6.3219, time 126.77ms, mfu 0.80%\n",
      "iter 660: loss 6.4461, time 126.46ms, mfu 0.80%\n",
      "iter 670: loss 6.3653, time 126.67ms, mfu 0.80%\n",
      "iter 680: loss 6.1582, time 126.45ms, mfu 0.79%\n",
      "iter 690: loss 6.1316, time 126.59ms, mfu 0.80%\n",
      "step 700: train loss 6.2168, val loss 6.3528\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 700: loss 6.3712, time 665.76ms, mfu 0.73%\n",
      "iter 710: loss 6.2572, time 126.45ms, mfu 0.77%\n",
      "iter 720: loss 5.9369, time 126.46ms, mfu 0.79%\n",
      "iter 730: loss 6.0265, time 125.97ms, mfu 0.80%\n",
      "iter 740: loss 6.1567, time 126.42ms, mfu 0.80%\n",
      "iter 750: loss 5.8801, time 125.99ms, mfu 0.80%\n",
      "iter 760: loss 5.9500, time 128.72ms, mfu 0.80%\n",
      "iter 770: loss 6.2880, time 125.98ms, mfu 0.80%\n",
      "iter 780: loss 5.7592, time 127.30ms, mfu 0.80%\n",
      "iter 790: loss 5.6979, time 127.24ms, mfu 0.80%\n",
      "step 800: train loss 5.7455, val loss 5.9688\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 800: loss 5.7261, time 674.55ms, mfu 0.73%\n",
      "iter 810: loss 5.6758, time 126.43ms, mfu 0.77%\n",
      "iter 820: loss 5.9888, time 126.26ms, mfu 0.79%\n",
      "iter 830: loss 5.5097, time 129.44ms, mfu 0.80%\n",
      "iter 840: loss 5.5230, time 129.32ms, mfu 0.80%\n",
      "iter 850: loss 5.5374, time 127.45ms, mfu 0.80%\n",
      "iter 860: loss 5.5716, time 126.47ms, mfu 0.80%\n",
      "iter 870: loss 5.7652, time 126.07ms, mfu 0.80%\n",
      "iter 880: loss 5.5981, time 127.12ms, mfu 0.80%\n",
      "iter 890: loss 5.5074, time 128.58ms, mfu 0.79%\n",
      "step 900: train loss 5.3795, val loss 5.6588\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 900: loss 5.3138, time 669.99ms, mfu 0.73%\n",
      "iter 910: loss 4.9141, time 126.97ms, mfu 0.77%\n",
      "iter 920: loss 5.3438, time 127.16ms, mfu 0.79%\n",
      "iter 930: loss 5.0244, time 127.71ms, mfu 0.79%\n",
      "iter 940: loss 5.2083, time 127.77ms, mfu 0.79%\n",
      "iter 950: loss 5.4054, time 127.70ms, mfu 0.79%\n",
      "iter 960: loss 4.8259, time 127.09ms, mfu 0.80%\n",
      "iter 970: loss 4.7279, time 126.95ms, mfu 0.80%\n",
      "iter 980: loss 4.9738, time 127.94ms, mfu 0.80%\n",
      "iter 990: loss 5.4176, time 127.31ms, mfu 0.80%\n",
      "step 1000: train loss 5.0560, val loss 5.4404\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 1000: loss 5.0493, time 681.61ms, mfu 0.73%\n",
      "iter 1010: loss 5.0742, time 127.20ms, mfu 0.77%\n",
      "iter 1020: loss 4.8252, time 127.85ms, mfu 0.79%\n",
      "iter 1030: loss 4.7034, time 127.48ms, mfu 0.79%\n",
      "iter 1040: loss 4.8354, time 127.26ms, mfu 0.79%\n",
      "iter 1050: loss 4.5808, time 126.81ms, mfu 0.79%\n",
      "iter 1060: loss 4.8397, time 127.62ms, mfu 0.79%\n",
      "iter 1070: loss 5.1868, time 126.66ms, mfu 0.80%\n",
      "iter 1080: loss 4.6484, time 127.87ms, mfu 0.80%\n",
      "iter 1090: loss 5.2248, time 126.90ms, mfu 0.80%\n",
      "step 1100: train loss 4.8163, val loss 5.3308\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 1100: loss 5.1444, time 666.35ms, mfu 0.74%\n",
      "iter 1110: loss 4.5194, time 128.00ms, mfu 0.77%\n",
      "iter 1120: loss 4.8287, time 126.43ms, mfu 0.79%\n",
      "iter 1130: loss 4.7256, time 127.39ms, mfu 0.79%\n",
      "iter 1140: loss 4.8281, time 126.84ms, mfu 0.79%\n",
      "iter 1150: loss 4.4548, time 127.95ms, mfu 0.80%\n",
      "iter 1160: loss 4.8536, time 126.55ms, mfu 0.79%\n",
      "iter 1170: loss 4.4766, time 126.47ms, mfu 0.80%\n",
      "iter 1180: loss 4.5998, time 126.51ms, mfu 0.80%\n",
      "iter 1190: loss 4.8361, time 127.21ms, mfu 0.80%\n",
      "step 1200: train loss 4.6092, val loss 5.3292\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 1200: loss 4.5466, time 666.84ms, mfu 0.73%\n",
      "iter 1210: loss 4.9540, time 126.13ms, mfu 0.78%\n",
      "iter 1220: loss 4.5506, time 130.29ms, mfu 0.78%\n",
      "iter 1230: loss 4.7315, time 126.43ms, mfu 0.79%\n",
      "iter 1240: loss 4.3387, time 127.15ms, mfu 0.79%\n",
      "iter 1250: loss 4.6914, time 126.23ms, mfu 0.80%\n",
      "iter 1260: loss 4.5591, time 127.26ms, mfu 0.80%\n",
      "iter 1270: loss 4.6670, time 125.62ms, mfu 0.80%\n",
      "iter 1280: loss 4.7302, time 126.70ms, mfu 0.80%\n",
      "iter 1290: loss 4.4148, time 126.79ms, mfu 0.80%\n",
      "step 1300: train loss 4.4641, val loss 5.3266\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 1300: loss 4.2941, time 659.70ms, mfu 0.73%\n",
      "iter 1310: loss 4.4915, time 127.29ms, mfu 0.78%\n",
      "iter 1320: loss 4.5719, time 126.20ms, mfu 0.79%\n",
      "iter 1330: loss 4.4997, time 126.09ms, mfu 0.80%\n",
      "iter 1340: loss 4.0316, time 126.01ms, mfu 0.79%\n",
      "iter 1350: loss 4.1251, time 126.32ms, mfu 0.80%\n",
      "iter 1360: loss 4.2198, time 125.64ms, mfu 0.80%\n",
      "iter 1370: loss 4.1115, time 128.58ms, mfu 0.80%\n",
      "iter 1380: loss 4.1200, time 126.53ms, mfu 0.80%\n",
      "iter 1390: loss 4.7599, time 126.99ms, mfu 0.80%\n",
      "step 1400: train loss 4.3225, val loss 5.2163\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 1400: loss 4.4127, time 665.47ms, mfu 0.73%\n",
      "iter 1410: loss 4.6430, time 128.91ms, mfu 0.78%\n",
      "iter 1420: loss 4.4591, time 125.99ms, mfu 0.80%\n",
      "iter 1430: loss 4.2918, time 125.60ms, mfu 0.80%\n",
      "iter 1440: loss 4.5916, time 125.53ms, mfu 0.81%\n",
      "iter 1450: loss 4.0408, time 130.51ms, mfu 0.80%\n",
      "iter 1460: loss 4.5882, time 152.09ms, mfu 0.78%\n",
      "iter 1470: loss 4.3191, time 126.04ms, mfu 0.79%\n",
      "iter 1480: loss 4.1518, time 126.23ms, mfu 0.80%\n",
      "iter 1490: loss 4.3365, time 126.80ms, mfu 0.80%\n",
      "step 1500: train loss 4.2421, val loss 5.3398\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 1500: loss 4.5788, time 663.08ms, mfu 0.74%\n",
      "iter 1510: loss 4.5064, time 126.28ms, mfu 0.78%\n",
      "iter 1520: loss 4.1933, time 125.96ms, mfu 0.79%\n",
      "iter 1530: loss 4.1597, time 126.77ms, mfu 0.80%\n",
      "iter 1540: loss 3.9935, time 126.45ms, mfu 0.80%\n",
      "iter 1550: loss 4.3663, time 126.64ms, mfu 0.80%\n",
      "iter 1560: loss 4.1214, time 128.75ms, mfu 0.80%\n",
      "iter 1570: loss 4.5895, time 125.95ms, mfu 0.79%\n",
      "iter 1580: loss 3.9103, time 126.73ms, mfu 0.80%\n",
      "iter 1590: loss 3.5773, time 126.42ms, mfu 0.80%\n",
      "step 1600: train loss 4.1244, val loss 5.3149\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 1600: loss 4.1745, time 667.60ms, mfu 0.73%\n",
      "iter 1610: loss 4.1045, time 130.15ms, mfu 0.77%\n",
      "iter 1620: loss 4.1873, time 128.25ms, mfu 0.79%\n",
      "iter 1630: loss 4.3654, time 126.53ms, mfu 0.79%\n",
      "iter 1640: loss 3.9450, time 127.33ms, mfu 0.79%\n",
      "iter 1650: loss 4.0731, time 126.01ms, mfu 0.80%\n",
      "iter 1660: loss 4.0225, time 127.15ms, mfu 0.80%\n",
      "iter 1670: loss 4.0898, time 126.87ms, mfu 0.80%\n",
      "iter 1680: loss 3.8704, time 132.31ms, mfu 0.78%\n",
      "iter 1690: loss 4.6465, time 126.71ms, mfu 0.79%\n",
      "step 1700: train loss 4.0152, val loss 5.3672\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 1700: loss 3.5802, time 668.10ms, mfu 0.73%\n",
      "iter 1710: loss 3.9001, time 127.01ms, mfu 0.77%\n",
      "iter 1720: loss 4.7246, time 126.75ms, mfu 0.79%\n",
      "iter 1730: loss 3.5935, time 126.58ms, mfu 0.80%\n",
      "iter 1740: loss 4.1280, time 126.43ms, mfu 0.80%\n",
      "iter 1750: loss 3.9000, time 126.73ms, mfu 0.80%\n",
      "iter 1760: loss 3.6761, time 126.84ms, mfu 0.80%\n",
      "iter 1770: loss 4.2662, time 128.15ms, mfu 0.80%\n",
      "iter 1780: loss 4.1538, time 129.02ms, mfu 0.80%\n",
      "iter 1790: loss 3.9584, time 136.24ms, mfu 0.79%\n",
      "step 1800: train loss 3.9512, val loss 5.4151\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 1800: loss 3.7704, time 670.01ms, mfu 0.72%\n",
      "iter 1810: loss 4.0832, time 126.94ms, mfu 0.76%\n",
      "iter 1820: loss 3.7537, time 128.30ms, mfu 0.79%\n",
      "iter 1830: loss 3.9812, time 126.56ms, mfu 0.79%\n",
      "iter 1840: loss 4.1538, time 126.65ms, mfu 0.79%\n",
      "iter 1850: loss 3.7834, time 126.81ms, mfu 0.80%\n",
      "iter 1860: loss 3.6659, time 126.45ms, mfu 0.80%\n",
      "iter 1870: loss 4.1289, time 126.70ms, mfu 0.80%\n",
      "iter 1880: loss 3.6307, time 126.01ms, mfu 0.80%\n",
      "iter 1890: loss 3.7596, time 126.01ms, mfu 0.80%\n",
      "step 1900: train loss 3.9009, val loss 5.4471\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 1900: loss 3.9240, time 667.81ms, mfu 0.74%\n",
      "iter 1910: loss 3.7870, time 131.65ms, mfu 0.77%\n",
      "iter 1920: loss 4.0421, time 128.31ms, mfu 0.79%\n",
      "iter 1930: loss 4.1215, time 128.37ms, mfu 0.79%\n",
      "iter 1940: loss 3.7101, time 126.98ms, mfu 0.80%\n",
      "iter 1950: loss 3.4218, time 127.08ms, mfu 0.80%\n",
      "iter 1960: loss 4.0748, time 126.83ms, mfu 0.80%\n",
      "iter 1970: loss 3.9046, time 126.58ms, mfu 0.80%\n",
      "iter 1980: loss 3.6320, time 126.77ms, mfu 0.80%\n",
      "iter 1990: loss 3.6464, time 126.70ms, mfu 0.79%\n",
      "step 2000: train loss 3.8160, val loss 5.4758\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 2000: loss 3.9236, time 657.65ms, mfu 0.73%\n",
      "iter 2010: loss 3.8016, time 126.22ms, mfu 0.78%\n",
      "iter 2020: loss 3.7230, time 129.29ms, mfu 0.78%\n",
      "iter 2030: loss 3.8230, time 126.91ms, mfu 0.79%\n",
      "iter 2040: loss 4.0562, time 126.78ms, mfu 0.79%\n",
      "iter 2050: loss 3.4477, time 126.85ms, mfu 0.79%\n",
      "iter 2060: loss 3.7050, time 127.07ms, mfu 0.80%\n",
      "iter 2070: loss 3.4075, time 126.62ms, mfu 0.80%\n",
      "iter 2080: loss 4.0244, time 126.80ms, mfu 0.80%\n",
      "iter 2090: loss 3.5934, time 126.42ms, mfu 0.80%\n",
      "step 2100: train loss 3.7384, val loss 5.5057\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 2100: loss 3.3153, time 667.17ms, mfu 0.73%\n",
      "iter 2110: loss 3.6423, time 127.79ms, mfu 0.77%\n",
      "iter 2120: loss 3.5709, time 126.72ms, mfu 0.79%\n",
      "iter 2130: loss 3.6389, time 127.93ms, mfu 0.79%\n",
      "iter 2140: loss 3.4172, time 127.10ms, mfu 0.79%\n",
      "iter 2150: loss 3.5051, time 126.95ms, mfu 0.80%\n",
      "iter 2160: loss 3.8723, time 126.04ms, mfu 0.80%\n",
      "iter 2170: loss 3.7048, time 126.50ms, mfu 0.80%\n",
      "iter 2180: loss 3.3937, time 126.36ms, mfu 0.80%\n",
      "iter 2190: loss 3.8853, time 126.40ms, mfu 0.79%\n",
      "step 2200: train loss 3.6982, val loss 5.5285\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 2200: loss 3.9153, time 680.81ms, mfu 0.73%\n",
      "iter 2210: loss 3.6311, time 127.09ms, mfu 0.77%\n",
      "iter 2220: loss 3.9348, time 127.08ms, mfu 0.79%\n",
      "iter 2230: loss 3.3056, time 126.04ms, mfu 0.80%\n",
      "iter 2240: loss 3.3037, time 126.42ms, mfu 0.80%\n",
      "iter 2250: loss 3.3237, time 127.28ms, mfu 0.80%\n",
      "iter 2260: loss 3.1549, time 126.83ms, mfu 0.80%\n",
      "iter 2270: loss 3.4850, time 126.13ms, mfu 0.79%\n",
      "iter 2280: loss 3.6949, time 126.56ms, mfu 0.80%\n",
      "iter 2290: loss 3.7115, time 129.19ms, mfu 0.79%\n",
      "step 2300: train loss 3.6074, val loss 5.5801\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 2300: loss 3.1339, time 668.85ms, mfu 0.73%\n",
      "iter 2310: loss 3.5744, time 125.52ms, mfu 0.77%\n",
      "iter 2320: loss 3.7708, time 126.11ms, mfu 0.79%\n",
      "iter 2330: loss 3.6828, time 126.31ms, mfu 0.80%\n",
      "iter 2340: loss 3.1859, time 125.84ms, mfu 0.80%\n",
      "iter 2350: loss 3.3043, time 127.17ms, mfu 0.80%\n",
      "iter 2360: loss 3.5545, time 127.69ms, mfu 0.80%\n",
      "iter 2370: loss 3.4625, time 125.74ms, mfu 0.80%\n",
      "iter 2380: loss 3.3793, time 126.03ms, mfu 0.80%\n",
      "iter 2390: loss 3.7151, time 126.20ms, mfu 0.80%\n",
      "step 2400: train loss 3.5394, val loss 5.6948\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 2400: loss 3.2066, time 679.10ms, mfu 0.73%\n",
      "iter 2410: loss 3.6120, time 125.55ms, mfu 0.77%\n",
      "iter 2420: loss 3.7897, time 125.95ms, mfu 0.79%\n",
      "iter 2430: loss 3.8799, time 126.43ms, mfu 0.80%\n",
      "iter 2440: loss 3.5515, time 125.76ms, mfu 0.80%\n",
      "iter 2450: loss 3.3773, time 126.10ms, mfu 0.80%\n",
      "iter 2460: loss 3.6249, time 127.60ms, mfu 0.80%\n",
      "iter 2470: loss 3.2153, time 131.67ms, mfu 0.80%\n",
      "iter 2480: loss 3.7513, time 125.94ms, mfu 0.80%\n",
      "iter 2490: loss 3.7279, time 126.12ms, mfu 0.80%\n",
      "step 2500: train loss 3.4912, val loss 5.7052\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 2500: loss 3.5278, time 676.15ms, mfu 0.73%\n",
      "iter 2510: loss 3.2974, time 126.70ms, mfu 0.77%\n",
      "iter 2520: loss 3.4420, time 128.37ms, mfu 0.79%\n",
      "iter 2530: loss 3.6201, time 126.75ms, mfu 0.80%\n",
      "iter 2540: loss 3.3476, time 126.23ms, mfu 0.80%\n",
      "iter 2550: loss 2.9563, time 130.65ms, mfu 0.80%\n",
      "iter 2560: loss 3.0969, time 126.60ms, mfu 0.79%\n",
      "iter 2570: loss 3.1436, time 131.90ms, mfu 0.79%\n",
      "iter 2580: loss 3.3912, time 125.80ms, mfu 0.80%\n",
      "iter 2590: loss 3.7003, time 126.08ms, mfu 0.80%\n",
      "step 2600: train loss 3.4499, val loss 5.8301\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 2600: loss 3.7159, time 661.89ms, mfu 0.73%\n",
      "iter 2610: loss 3.6686, time 126.34ms, mfu 0.77%\n",
      "iter 2620: loss 3.6851, time 128.27ms, mfu 0.79%\n",
      "iter 2630: loss 3.4881, time 133.10ms, mfu 0.79%\n",
      "iter 2640: loss 3.3635, time 126.49ms, mfu 0.79%\n",
      "iter 2650: loss 3.3587, time 125.63ms, mfu 0.80%\n",
      "iter 2660: loss 3.6478, time 127.22ms, mfu 0.80%\n",
      "iter 2670: loss 3.5733, time 126.05ms, mfu 0.80%\n",
      "iter 2680: loss 3.0266, time 131.20ms, mfu 0.80%\n",
      "iter 2690: loss 3.4856, time 133.98ms, mfu 0.74%\n",
      "step 2700: train loss 3.3736, val loss 5.8612\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 2700: loss 3.3913, time 652.39ms, mfu 0.71%\n",
      "iter 2710: loss 3.5246, time 125.48ms, mfu 0.77%\n",
      "iter 2720: loss 2.8832, time 125.89ms, mfu 0.79%\n",
      "iter 2730: loss 3.4263, time 125.98ms, mfu 0.80%\n",
      "iter 2740: loss 3.1908, time 126.29ms, mfu 0.80%\n",
      "iter 2750: loss 3.4394, time 128.95ms, mfu 0.80%\n",
      "iter 2760: loss 3.3937, time 126.29ms, mfu 0.80%\n",
      "iter 2770: loss 3.3283, time 125.87ms, mfu 0.80%\n",
      "iter 2780: loss 3.1207, time 125.57ms, mfu 0.80%\n",
      "iter 2790: loss 3.5084, time 126.76ms, mfu 0.79%\n",
      "step 2800: train loss 3.3394, val loss 5.8695\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 2800: loss 3.5434, time 663.70ms, mfu 0.74%\n",
      "iter 2810: loss 3.6562, time 132.33ms, mfu 0.77%\n",
      "iter 2820: loss 3.1112, time 125.63ms, mfu 0.79%\n",
      "iter 2830: loss 3.0450, time 126.73ms, mfu 0.80%\n",
      "iter 2840: loss 3.2427, time 125.97ms, mfu 0.79%\n",
      "iter 2850: loss 3.3799, time 125.73ms, mfu 0.80%\n",
      "iter 2860: loss 3.3094, time 125.00ms, mfu 0.80%\n",
      "iter 2870: loss 3.2907, time 125.51ms, mfu 0.81%\n",
      "iter 2880: loss 3.3063, time 125.27ms, mfu 0.81%\n",
      "iter 2890: loss 3.6134, time 126.50ms, mfu 0.81%\n",
      "step 2900: train loss 3.3248, val loss 5.8318\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 2900: loss 3.2295, time 652.88ms, mfu 0.74%\n",
      "iter 2910: loss 3.3509, time 125.92ms, mfu 0.78%\n",
      "iter 2920: loss 2.9802, time 125.51ms, mfu 0.80%\n",
      "iter 2930: loss 3.2670, time 128.03ms, mfu 0.80%\n",
      "iter 2940: loss 3.3022, time 127.20ms, mfu 0.79%\n",
      "iter 2950: loss 3.4682, time 126.68ms, mfu 0.79%\n",
      "iter 2960: loss 3.2464, time 129.54ms, mfu 0.79%\n",
      "iter 2970: loss 3.2718, time 125.99ms, mfu 0.79%\n",
      "iter 2980: loss 3.0217, time 127.26ms, mfu 0.80%\n",
      "iter 2990: loss 3.0750, time 127.23ms, mfu 0.80%\n",
      "step 3000: train loss 3.2409, val loss 5.9166\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 3000: loss 3.3769, time 658.82ms, mfu 0.73%\n",
      "iter 3010: loss 3.1928, time 126.75ms, mfu 0.78%\n",
      "iter 3020: loss 3.4488, time 126.74ms, mfu 0.79%\n",
      "iter 3030: loss 2.8634, time 127.69ms, mfu 0.79%\n",
      "iter 3040: loss 3.2967, time 126.44ms, mfu 0.80%\n",
      "iter 3050: loss 3.0997, time 127.69ms, mfu 0.80%\n",
      "iter 3060: loss 2.8236, time 128.25ms, mfu 0.80%\n",
      "iter 3070: loss 3.0779, time 126.17ms, mfu 0.80%\n",
      "iter 3080: loss 3.3693, time 129.01ms, mfu 0.79%\n",
      "iter 3090: loss 3.1004, time 126.29ms, mfu 0.80%\n",
      "step 3100: train loss 3.2158, val loss 5.9495\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 3100: loss 3.0409, time 666.56ms, mfu 0.73%\n",
      "iter 3110: loss 3.0021, time 127.52ms, mfu 0.77%\n",
      "iter 3120: loss 3.2337, time 127.36ms, mfu 0.79%\n",
      "iter 3130: loss 3.2142, time 128.55ms, mfu 0.79%\n",
      "iter 3140: loss 3.0943, time 126.38ms, mfu 0.79%\n",
      "iter 3150: loss 3.1617, time 130.14ms, mfu 0.79%\n",
      "iter 3160: loss 2.9672, time 126.36ms, mfu 0.80%\n",
      "iter 3170: loss 2.8208, time 125.48ms, mfu 0.80%\n",
      "iter 3180: loss 3.1159, time 125.26ms, mfu 0.80%\n",
      "iter 3190: loss 2.9654, time 127.44ms, mfu 0.80%\n",
      "step 3200: train loss 3.1695, val loss 6.0175\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 3200: loss 3.5486, time 655.21ms, mfu 0.73%\n",
      "iter 3210: loss 3.3430, time 125.70ms, mfu 0.78%\n",
      "iter 3220: loss 3.1091, time 125.08ms, mfu 0.80%\n",
      "iter 3230: loss 3.2333, time 126.54ms, mfu 0.80%\n",
      "iter 3240: loss 3.4027, time 130.23ms, mfu 0.80%\n",
      "iter 3250: loss 3.0430, time 125.58ms, mfu 0.80%\n",
      "iter 3260: loss 3.2179, time 126.84ms, mfu 0.80%\n",
      "iter 3270: loss 2.9321, time 130.24ms, mfu 0.79%\n",
      "iter 3280: loss 3.2297, time 126.44ms, mfu 0.80%\n",
      "iter 3290: loss 3.0068, time 128.17ms, mfu 0.80%\n",
      "step 3300: train loss 3.1273, val loss 6.1140\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 3300: loss 3.2685, time 710.72ms, mfu 0.73%\n",
      "iter 3310: loss 2.7668, time 128.09ms, mfu 0.76%\n",
      "iter 3320: loss 3.2010, time 127.00ms, mfu 0.78%\n",
      "iter 3330: loss 3.2343, time 126.63ms, mfu 0.79%\n",
      "iter 3340: loss 3.3444, time 126.60ms, mfu 0.80%\n",
      "iter 3350: loss 3.1888, time 125.59ms, mfu 0.80%\n",
      "iter 3360: loss 2.9102, time 126.18ms, mfu 0.80%\n",
      "iter 3370: loss 3.3400, time 126.66ms, mfu 0.80%\n",
      "iter 3380: loss 3.0675, time 126.77ms, mfu 0.80%\n",
      "iter 3390: loss 3.0705, time 126.88ms, mfu 0.80%\n",
      "step 3400: train loss 3.0955, val loss 6.1192\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 3400: loss 3.2794, time 664.50ms, mfu 0.72%\n",
      "iter 3410: loss 2.9280, time 130.31ms, mfu 0.76%\n",
      "iter 3420: loss 3.2468, time 130.28ms, mfu 0.77%\n",
      "iter 3430: loss 3.3517, time 126.93ms, mfu 0.79%\n",
      "iter 3440: loss 3.1022, time 125.72ms, mfu 0.80%\n",
      "iter 3450: loss 3.1225, time 131.05ms, mfu 0.80%\n",
      "iter 3460: loss 3.0541, time 126.09ms, mfu 0.80%\n",
      "iter 3470: loss 3.2900, time 126.06ms, mfu 0.80%\n",
      "iter 3480: loss 3.4160, time 125.63ms, mfu 0.80%\n",
      "iter 3490: loss 2.8938, time 126.10ms, mfu 0.80%\n",
      "step 3500: train loss 3.0697, val loss 6.0937\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 3500: loss 2.8349, time 655.20ms, mfu 0.73%\n",
      "iter 3510: loss 2.9472, time 126.39ms, mfu 0.78%\n",
      "iter 3520: loss 3.0333, time 127.47ms, mfu 0.79%\n",
      "iter 3530: loss 3.2057, time 125.61ms, mfu 0.80%\n",
      "iter 3540: loss 3.3025, time 127.14ms, mfu 0.79%\n",
      "iter 3550: loss 2.9377, time 127.00ms, mfu 0.79%\n",
      "iter 3560: loss 3.1978, time 126.45ms, mfu 0.80%\n",
      "iter 3570: loss 3.1475, time 130.10ms, mfu 0.79%\n",
      "iter 3580: loss 2.8272, time 125.84ms, mfu 0.80%\n",
      "iter 3590: loss 2.8424, time 125.86ms, mfu 0.80%\n",
      "step 3600: train loss 3.0141, val loss 6.1077\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 3600: loss 2.9287, time 671.12ms, mfu 0.73%\n",
      "iter 3610: loss 3.2236, time 125.86ms, mfu 0.78%\n",
      "iter 3620: loss 3.1282, time 125.96ms, mfu 0.79%\n",
      "iter 3630: loss 2.9113, time 126.43ms, mfu 0.80%\n",
      "iter 3640: loss 3.0417, time 126.93ms, mfu 0.80%\n",
      "iter 3650: loss 3.2327, time 127.38ms, mfu 0.79%\n",
      "iter 3660: loss 3.1174, time 127.18ms, mfu 0.80%\n",
      "iter 3670: loss 2.8853, time 126.95ms, mfu 0.80%\n",
      "iter 3680: loss 3.1281, time 126.41ms, mfu 0.80%\n",
      "iter 3690: loss 2.8907, time 126.46ms, mfu 0.80%\n",
      "step 3700: train loss 2.9719, val loss 6.2322\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 3700: loss 2.8826, time 643.68ms, mfu 0.73%\n",
      "iter 3710: loss 2.6308, time 125.71ms, mfu 0.78%\n",
      "iter 3720: loss 2.9132, time 125.75ms, mfu 0.79%\n",
      "iter 3730: loss 3.1575, time 126.91ms, mfu 0.80%\n",
      "iter 3740: loss 3.0256, time 126.01ms, mfu 0.80%\n",
      "iter 3750: loss 3.2252, time 126.91ms, mfu 0.80%\n",
      "iter 3760: loss 2.9106, time 126.33ms, mfu 0.80%\n",
      "iter 3770: loss 2.9530, time 126.11ms, mfu 0.79%\n",
      "iter 3780: loss 2.7659, time 125.48ms, mfu 0.80%\n",
      "iter 3790: loss 2.9341, time 128.91ms, mfu 0.80%\n",
      "step 3800: train loss 2.9571, val loss 6.1782\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 3800: loss 3.0794, time 661.03ms, mfu 0.73%\n",
      "iter 3810: loss 2.8026, time 126.12ms, mfu 0.78%\n",
      "iter 3820: loss 3.2553, time 126.08ms, mfu 0.80%\n",
      "iter 3830: loss 2.9821, time 126.04ms, mfu 0.80%\n",
      "iter 3840: loss 3.0320, time 125.53ms, mfu 0.80%\n",
      "iter 3850: loss 3.1175, time 127.01ms, mfu 0.80%\n",
      "iter 3860: loss 2.7725, time 126.17ms, mfu 0.80%\n",
      "iter 3870: loss 3.0221, time 126.77ms, mfu 0.80%\n",
      "iter 3880: loss 3.1133, time 127.59ms, mfu 0.80%\n",
      "iter 3890: loss 3.0386, time 125.78ms, mfu 0.80%\n",
      "step 3900: train loss 2.9044, val loss 6.2803\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 3900: loss 2.8660, time 659.33ms, mfu 0.74%\n",
      "iter 3910: loss 2.7999, time 126.44ms, mfu 0.78%\n",
      "iter 3920: loss 3.0325, time 126.99ms, mfu 0.79%\n",
      "iter 3930: loss 2.7377, time 126.80ms, mfu 0.80%\n",
      "iter 3940: loss 3.1424, time 127.10ms, mfu 0.80%\n",
      "iter 3950: loss 2.8253, time 126.35ms, mfu 0.80%\n",
      "iter 3960: loss 2.4411, time 126.72ms, mfu 0.80%\n",
      "iter 3970: loss 2.7415, time 127.37ms, mfu 0.80%\n",
      "iter 3980: loss 2.8631, time 126.27ms, mfu 0.80%\n",
      "iter 3990: loss 2.8188, time 126.66ms, mfu 0.80%\n",
      "step 4000: train loss 2.8964, val loss 6.3862\n",
      "saving checkpoint to xmodel4000.pt\n",
      "iter 4000: loss 2.6964, time 657.10ms, mfu 0.74%\n",
      "saving checkpoint to xmodel4000.pt\n",
      "\n",
      "* Iter 4000 - loss = 2.90 / val = 6.39 (0.0 dropout, 64 emb, 4 heads, 4 layer)  00:09:07s\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_iters = 4000 # total number of training iterations\n",
    "checkpoint_file = f'xmodel{max_iters}.pt'\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "train_start = time.time()\n",
    "################################# Parameters ##################################\n",
    "\"\"\"\n",
    "GPT-2 Settings - 123M \n",
    "    block_size: int = 1024\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "\"\"\"\n",
    "\n",
    "# config\n",
    "eval_iters = 200 # number of iterations to run evaluation\n",
    "dataset = 'data' # directory where the data is stored\n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
    "batch_size = 12 # [12] number of batches to run in parallel\n",
    "block_size = 32 # [32] content window size (tokens)\n",
    "\n",
    "# model\n",
    "n_layer = 4     # 4  - layers of \n",
    "n_head = 4      # 4  - attention heads\n",
    "n_embd = 64     # 64 - dimensionality of the embedding vectors\n",
    "dropout = 0.0   # 0. - for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False    # False - do we use bias inside LayerNorm and Linear layers?\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # 6e-4 max learning rate\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 60000 # [600000] should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16'\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# capture above settings & parameters to save in model checkpoint\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys} \n",
    "\n",
    "# tokens per iterations\n",
    "tokens_per_iter = gradient_accumulation_steps * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "# set the random seed\n",
    "seed = 1337\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# Load the data\n",
    "data_dir = dataset + '/'\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "# get a batch from the data\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# loop counters starting point\n",
    "iter_num = 0\n",
    "\n",
    "# init a new model from scratch\n",
    "print(\"Initializing a new model from scratch\")\n",
    "print(\"Using vocab_size of GPT-2 of 50304 (50257 rounded up for efficiency)\")\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=50304, dropout=dropout) \n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)\n",
    "\n",
    "# report number of parameters\n",
    "print(\"Number of parameters: %.2fM\" % (model.get_num_params()/1e6,))\n",
    "print(\"Layers:\")\n",
    "for number, (name, param) in enumerate(model.named_parameters()):\n",
    "    print(number, name)\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "model.to(device)\n",
    "\n",
    "# initialize a GradScaler\n",
    "#scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "scaler = torch.amp.GradScaler(device_type, enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "# compile the model if not using MPS\n",
    "if device == 'mps':\n",
    "    print(\"MPS doesn't support JIT compilation, skipping...\")\n",
    "else:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    model = torch.compile(model) # requires PyTorch 2.0\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad() # disable gradient tracking - not needed for backward pass\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# save model to checkpoint file\n",
    "def save_model(fn):\n",
    "    checkpoint = {\n",
    "        'model': raw_model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model_args': model_args,\n",
    "        'iter_num': iter_num,\n",
    "        'config': config,\n",
    "    }\n",
    "    print(f\"saving checkpoint to {fn}\")\n",
    "    torch.save(checkpoint, fn)\n",
    "\n",
    "# stats\n",
    "stat_iter = []\n",
    "stat_loss_train = []\n",
    "stat_loss_val = []\n",
    "stat_lr_iter = []\n",
    "stat_lr = []\n",
    "\n",
    "# TRAINING LOOP\n",
    "\n",
    "# init loop\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model\n",
    "running_mfu = -1.0 # Memory Footprint Utilization on GPU\n",
    "training_start = time.time()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # determine and set the learning rate for this iteration\n",
    "        lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "        if lr > 0:\n",
    "            stat_lr.append(lr)\n",
    "            stat_lr_iter.append(iter_num)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        # evaluate the loss on train/val sets and write checkpoints\n",
    "        if iter_num % 100 == 0 or iter_num >= max_iters:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            if iter_num > 0:\n",
    "                save_model(checkpoint_file)\n",
    "            # record stats\n",
    "            stat_iter.append(iter_num)\n",
    "            stat_loss_train.append(losses['train'])\n",
    "            stat_loss_val.append(losses['val'])\n",
    "    \n",
    "        # FORWARD PROP, update with optional gradient accumulation to simulate larger batch size\n",
    "        # and using the GradScaler if data type is float16\n",
    "        for micro_step in range(gradient_accumulation_steps):\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "            # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "            X, Y = get_batch('train')\n",
    "            # with gradient scaling if training in fp16\n",
    "            scaler.scale(loss).backward()\n",
    "        # clip the gradient - to avoid exploding gradients\n",
    "        if grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        # BACK PROP\n",
    "        # step the optimizer and scaler if training in fp16\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # flush the gradients as soon as we can, no need for this memory anymore\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "        # stats for timing\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        if iter_num % 10 == 0:\n",
    "            print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "        iter_num += 1\n",
    "        local_iter_num += 1\n",
    "    \n",
    "        # termination conditions\n",
    "        if iter_num > max_iters:\n",
    "            break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"User requested exit, saving checkpoint\")\n",
    "        break\n",
    "\n",
    "save_model(checkpoint_file)\n",
    "\n",
    "# print duration of run in hh:mm:ss\n",
    "training_end = time.time()\n",
    "duration = training_end - training_start\n",
    "minutes, seconds = divmod(duration, 60)\n",
    "hours, minutes = divmod(minutes, 60)\n",
    "print(f\"\\n* Iter {max_iters} - loss = {stat_loss_train[-1]:.2f} / val = {stat_loss_val[-1]:.2f}\",\n",
    "      f\"({dropout} dropout, {n_embd} emb, {n_head} heads, {n_layer} layer)\",\n",
    "      f\" {hours:02.0f}:{minutes:02.0f}:{seconds:02.0f}s\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd26b605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.825839875788878\n"
     ]
    }
   ],
   "source": [
    "# Worst-case loss when using Cross-Entropy Loss function\n",
    "import math\n",
    "vocab_size=50304\n",
    "worse_case = -math.log(1/vocab_size)\n",
    "print(worse_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca655441-6488-40fe-b83c-01b1d1a4b2c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3662a04-ce04-4007-9e46-d62660a4916f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses['train']=tensor(2.8964) losses['val']=tensor(6.3862)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f326b646a50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(f\"{losses['train']=} {losses['val']=}\")\n",
    "plt.plot(stat_iter, stat_loss_train, stat_iter, stat_loss_val)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iter')\n",
    "plt.legend(['training', 'validation'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "429cb13d-2d80-49b0-878b-b8ec8da0680c",
   "metadata": {},
   "source": [
    "### Performance Log\n",
    "* Iter 20 - loss = 10.81 / val = 10.80 (0.0 dropout, 64 emb, 4 heads, 4 layer)\n",
    "* Iter 250 - loss = 9.72 / val = 9.75 (0.0 dropout, 64 emb, 4 heads, 4 layer)\n",
    "* Iter 1000 - loss = 5.01 / val = 5.43 (0.0 dropout, 64 emb, 4 heads, 4 layer)\n",
    "* Iter 2000 - loss = 3.81 / val = 5.46 (0.0 dropout, 64 emb, 4 heads, 4 layer)  00:04:39s\n",
    "* Iter 3000 - loss = 3.24 / val = 5.96 (0.0 dropout, 64 emb, 4 heads, 4 layer)  00:07:11s\n",
    "* Iter 4000 - loss = 2.90 / val = 6.39 (0.0 dropout, 64 emb, 4 heads, 4 layer)  00:09:07s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5bf040-5615-4e2d-a678-62e3d71d1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stat_lr_iter, stat_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419183b-a46e-47f7-94c0-9533e3e7c494",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00ac0364-f706-44a8-b157-b088656227be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: xmodel4000.pt\n",
      "Number of parameters: 3.42M\n",
      "\n",
      "That any consul\n",
      "I will have made my son so dishonour's love;\n",
      "And how should the better want his death.\n",
      "\n",
      "WARWICK:\n",
      "Did had an oath but he speak? Go for his counsel?\n",
      "Not here, he did't not me: but I was gone to prison;\n",
      "Who, man, dead, to catch his nature;\n",
      "When it was't so brave, I will speak with you.\n",
      "\n",
      "GEORGE:\n",
      "And does unshining to yain's dead: say done, you not\n",
      "What offencelike your mistress' king?\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Ah, this once! would you love the proudest love of me.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Unday hast groime; all our uncle York.\n",
      "\n",
      "RIVERS:\n",
      "\n",
      "SIRER:\n",
      "Ti'Then, nor the subject talk of young Warwick,\n",
      "That Henry sworn time may allow. If thou be moved,--\n",
      "Yet wilt thou shalt have a princely father Montst.\n",
      "\n",
      "WARWICK:\n",
      "No, for she is mad, and I could set myself;\n",
      "And go you toward the devilish time that lives may be\n",
      "That speaks your father's heirs'er's very true,\n",
      "Than kill's husband which he is dead of day;\n",
      "And that my queen's father's sake.'\n",
      "\n",
      "KING EDWARD IV:\n",
      "It is the tongue of your brows with all their sorrow\n",
      "With oath and will no other me some one:\n",
      "If any think's a man that yours when you do.\n",
      "Away from your daughter come for yourself,\n",
      "Not in your husband's trial, upon my eye\n",
      "so met, my kindere born; ever thou had so\n",
      "'I' the garland's. Thy best, gentle England'st!\n",
      "O thou a fellow o'clock.\n",
      "\n",
      "First Lord:\n",
      "SICINIUS:\n",
      "He dies to live again for ourselves:\n",
      "Faith, no more than you had the first while, and be got\n",
      "fellow.\n",
      "\n",
      "CLEOMENIUS:\n",
      "We have seen a batesby, sir, what I will seem to look,\n",
      "That he did't have known to Marcius.\n",
      "\n",
      "LEONTES:\n",
      "I prithee, no grave:\n",
      "Gonce!\n",
      "SICINIUS:\n",
      "O Mont\n"
     ]
    }
   ],
   "source": [
    "#checkpoint_file = 'xmodel4000.pt'\n",
    "\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "print(f\"Checkpoint: {checkpoint_file}\")\n",
    "\n",
    "# inference parameters\n",
    "max_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda'\n",
    "dtype = 'bfloat16'\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# load model saved in a specific directory\n",
    "checkpoint = torch.load(checkpoint_file, map_location=device, weights_only=True)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(\"Number of parameters: %.2fM\" % (model.get_num_params()/1e6,))\n",
    "\n",
    "# set prompt\n",
    "prompt = \"\\n\"\n",
    "start_ids = enc.encode(prompt, allowed_special={\"<|endoftext|>\"})\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        y = model.generate(x, max_tokens, temperature=temperature, top_k=top_k)\n",
    "        output = y[0].tolist()\n",
    "        for w in output:\n",
    "            if w > 50257: # max token value, ignore the rest\n",
    "                continue\n",
    "            else:\n",
    "                text = enc.decode([w])\n",
    "                if text == '\\n':\n",
    "                    print()\n",
    "                else:\n",
    "                    print(text, end='')\n",
    "                \n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30b98e-12f8-4f11-b489-7619310b2275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5b4f116-fda5-43f8-9e2f-2ffe5a9ca8ee",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Standford Univesity - CS231n: Deep Learning for Computer Vision - https://cs231n.github.io/neural-networks-1/\n",
    "* Visual Transformer, Explained - https://poloclub.github.io/transformer-explainer/\n",
    "* Cousera: Generative Ai with LLMs - https://www.coursera.org/learn/generative-ai-with-llms\n",
    "* Attention is All You Need by Vaswani et al. in 2017 - https://arxiv.org/abs/1706.03762\n",
    "* The Illustrated Transformer by Jay Alammar - https://jalammar.github.io/illustrated-transformer/\n",
    "* Visualizing Attention, a Transformer's Heart - https://www.3blue1brown.com/lessons/attention\n",
    "* Let's build GPT: from scratch, in code, spelled out. - by Andrej Karpathy - https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "* nanoGPT by Andrej Karpathy - https://github.com/karpathy/nanoGPT\n",
    "* OpenAI GPT-2 - https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "\n",
    "## Papers\n",
    "- Makemore, by Andrej Karpathy - https://github.com/karpathy/makemore\n",
    "- MLP, following [Bengio et al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "- CNN, following [DeepMind WaveNet 2016](https://arxiv.org/abs/1609.03499) (in progress...)\n",
    "- RNN, following [Mikolov et al. 2010](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)\n",
    "- LSTM, following [Graves et al. 2014](https://arxiv.org/abs/1308.0850)\n",
    "- GRU, following [Kyunghyun Cho et al. 2014](https://arxiv.org/abs/1409.1259)\n",
    "- Transformer, following [Vaswani et al. 2017](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3689977-df5c-4885-8b9e-ab794ae8c3c2",
   "metadata": {},
   "source": [
    "## Misc\n",
    "\n",
    "Example Training Runs\n",
    "\n",
    "<img width=\"572\" alt=\"image\" src=\"https://github.com/user-attachments/assets/83ef87d4-ad8d-43d9-aabe-5e52a66fc9ac\">\n",
    "\n",
    "<img width=\"572\" alt=\"image\" src=\"https://github.com/user-attachments/assets/927e2cc2-c1a3-4472-8a83-fb3d5b410dcb\">\n",
    "\n",
    "<img width=\"579\" alt=\"image\" src=\"https://github.com/user-attachments/assets/9ceabbf0-8690-444a-ad63-d4abd420b40f\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d3c8add-c0b9-4edc-86de-8f2bfd4aab1a",
   "metadata": {},
   "source": [
    "### Using Dropout to Help with Overfitting\n",
    "\n",
    "* Iter 20 - loss = 10.81 / val = 10.80 (no dropout, 64 emb, 4 heads, 4 layer)\n",
    "* Iter 250 - loss = 9.72 / val = 9.75 (no dropout, 64 emb, 4 heads, 4 layer)\n",
    "* Iter 1000 - loss = 5.01 / val = 5.43 (no dropout, 64 emb, 4 heads, 4 layer)\n",
    "* Iter 2000 - loss = 3.81 / val = 5.47 (no dropout, 64 emb, 4 heads, 4 layer)\n",
    "* Iter 2000 - loss = 4.12 / val = 4.96 (0.2 dropout, 64 emb, 4 heads, 4 layer)\n",
    "* Iter 3000 - loss = 3.64 / val = 4.90 (0.2 dropout, 64 emb, 4 heads, 4 layer)\n",
    "* Iter 4000 - loss = 3.38 / val = 5.00 (0.2 dropout, 64 emb, 4 heads, 4 layer)\n",
    "* Iter 5000 - loss = 3.07 / val = 5.23 (0.2 dropout, 64 emb, 4 heads, 4 layer)\n",
    "* Iter 6000 - loss = 3.28 / val = 5.06 (0.3 dropout, 64 emb, 4 heads, 4 layer)\n",
    "* Iter 20000 - loss = 2.99 / val = 5.29 (0.3 dropout, 64 emb, 4 heads, 4 layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
