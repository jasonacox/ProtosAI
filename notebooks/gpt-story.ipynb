{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a6ebe8-454f-4f5a-bcdb-6ed1132625c7",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First some dependencies need to be installed\n",
    "!pip install tiktoken\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install tqdm\n",
    "!pip install torch\n",
    "!pip install matplotlib\n",
    "!pip install termcolor\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f002369e-b9ca-4c76-9f4d-6b04b7530ab1",
   "metadata": {},
   "source": [
    "# TinyStories Dataset\n",
    "\n",
    "Divide data into two parts:\n",
    "- 90% Training\n",
    "- 10% Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbe64b04-e900-4a84-9792-fdbe426049a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 564,033,209 tokens\n",
      "val has 5,698,443 tokens\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "# Free up memory\n",
    "valid_data = None\n",
    "train_data = None\n",
    "train_ids = None\n",
    "val_ids = None\n",
    "\n",
    "# Download and Load\n",
    "def downloadx(input_file_path, data_url):\n",
    "    if not os.path.exists(input_file_path) and False:\n",
    "        response = requests.get(data_url)\n",
    "        with open(input_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(response)\n",
    "        return response\n",
    "\n",
    "def download(input_file_path, x):\n",
    "    with open(input_file_path, 'r') as f:\n",
    "        data = f.read()\n",
    "        return data\n",
    "\n",
    "# Load data\n",
    "train_data = download('data/TinyStoriesV2-GPT4-train.txt','')\n",
    "valid_data = download('data/TinyStoriesV2-GPT4-valid.txt','')\n",
    "\n",
    "# Encode with tiktoken gpt2\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(valid_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# Export to bin files for training\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile('data/train.bin')\n",
    "val_ids.tofile('data/val.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7806ff78-2c68-418b-8877-a2b3d05bc059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564033209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 198, 7454, 2402,  257,  640,  612,  373,  257, 1310, 2933, 3706,\n",
       "       3932,   13, 3932, 6151,  284, 7301,  262,  995, 1088,  683,   13,\n",
       "        679, 2497,  867, 4998, 1243,   11,  588, 4950,  410, 1386,  326,\n",
       "        547,  319, 3359,  287,  257, 3650,   13, 1881, 1110,   11, 3932,\n",
       "        373, 6155,  832,  262, 3650,  618], dtype=uint16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_ids))\n",
    "train_ids[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54a3c098-51e9-44eb-96dd-002ff11a31a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things, like beautiful vases that were on display in a store. One day, Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed!  \n",
      "He said, “Wow, that is a really amazing vase! Can I buy it?” \n",
      "The shopkeeper smiled and said, “Of course you can. You can take it home and show all your friends how amazing it is!”\n",
      "So Ben\n"
     ]
    }
   ],
   "source": [
    "print(enc.decode(train_ids[:128]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "169c1d42-47b0-4a6b-9b5e-76ee74e5756b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# GPT Model Class\n",
    "\n",
    "We will use the `model.py` file contains a GPT model class that was created by Andrej Karpathy. It was modeled after the one created by OpenAI.\n",
    "\n",
    "* Attention Is All You Need - https://arxiv.org/pdf/1706.03762\n",
    "* OpenAI GPT-2 - https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "* nanoGPT - https://github.com/karpathy/nanoGPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a50c5c-2be8-4c98-b40f-ef90bab2516c",
   "metadata": {},
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1489d614-51e7-454d-98a5-30c8767ebc18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 491,520\n",
      "Initializing a new model from scratch\n",
      "Using vocab_size of GPT-2 of 50304 (50257 rounded up for efficiency)\n",
      "Number of parameters: 123.59M\n",
      "Layers:\n",
      "0 transformer.wte.weight\n",
      "1 transformer.wpe.weight\n",
      "2 transformer.h.0.ln_1.weight\n",
      "3 transformer.h.0.attn.c_attn.weight\n",
      "4 transformer.h.0.attn.c_proj.weight\n",
      "5 transformer.h.0.ln_2.weight\n",
      "6 transformer.h.0.mlp.c_fc.weight\n",
      "7 transformer.h.0.mlp.c_proj.weight\n",
      "8 transformer.h.1.ln_1.weight\n",
      "9 transformer.h.1.attn.c_attn.weight\n",
      "10 transformer.h.1.attn.c_proj.weight\n",
      "11 transformer.h.1.ln_2.weight\n",
      "12 transformer.h.1.mlp.c_fc.weight\n",
      "13 transformer.h.1.mlp.c_proj.weight\n",
      "14 transformer.h.2.ln_1.weight\n",
      "15 transformer.h.2.attn.c_attn.weight\n",
      "16 transformer.h.2.attn.c_proj.weight\n",
      "17 transformer.h.2.ln_2.weight\n",
      "18 transformer.h.2.mlp.c_fc.weight\n",
      "19 transformer.h.2.mlp.c_proj.weight\n",
      "20 transformer.h.3.ln_1.weight\n",
      "21 transformer.h.3.attn.c_attn.weight\n",
      "22 transformer.h.3.attn.c_proj.weight\n",
      "23 transformer.h.3.ln_2.weight\n",
      "24 transformer.h.3.mlp.c_fc.weight\n",
      "25 transformer.h.3.mlp.c_proj.weight\n",
      "26 transformer.h.4.ln_1.weight\n",
      "27 transformer.h.4.attn.c_attn.weight\n",
      "28 transformer.h.4.attn.c_proj.weight\n",
      "29 transformer.h.4.ln_2.weight\n",
      "30 transformer.h.4.mlp.c_fc.weight\n",
      "31 transformer.h.4.mlp.c_proj.weight\n",
      "32 transformer.h.5.ln_1.weight\n",
      "33 transformer.h.5.attn.c_attn.weight\n",
      "34 transformer.h.5.attn.c_proj.weight\n",
      "35 transformer.h.5.ln_2.weight\n",
      "36 transformer.h.5.mlp.c_fc.weight\n",
      "37 transformer.h.5.mlp.c_proj.weight\n",
      "38 transformer.h.6.ln_1.weight\n",
      "39 transformer.h.6.attn.c_attn.weight\n",
      "40 transformer.h.6.attn.c_proj.weight\n",
      "41 transformer.h.6.ln_2.weight\n",
      "42 transformer.h.6.mlp.c_fc.weight\n",
      "43 transformer.h.6.mlp.c_proj.weight\n",
      "44 transformer.h.7.ln_1.weight\n",
      "45 transformer.h.7.attn.c_attn.weight\n",
      "46 transformer.h.7.attn.c_proj.weight\n",
      "47 transformer.h.7.ln_2.weight\n",
      "48 transformer.h.7.mlp.c_fc.weight\n",
      "49 transformer.h.7.mlp.c_proj.weight\n",
      "50 transformer.h.8.ln_1.weight\n",
      "51 transformer.h.8.attn.c_attn.weight\n",
      "52 transformer.h.8.attn.c_proj.weight\n",
      "53 transformer.h.8.ln_2.weight\n",
      "54 transformer.h.8.mlp.c_fc.weight\n",
      "55 transformer.h.8.mlp.c_proj.weight\n",
      "56 transformer.h.9.ln_1.weight\n",
      "57 transformer.h.9.attn.c_attn.weight\n",
      "58 transformer.h.9.attn.c_proj.weight\n",
      "59 transformer.h.9.ln_2.weight\n",
      "60 transformer.h.9.mlp.c_fc.weight\n",
      "61 transformer.h.9.mlp.c_proj.weight\n",
      "62 transformer.h.10.ln_1.weight\n",
      "63 transformer.h.10.attn.c_attn.weight\n",
      "64 transformer.h.10.attn.c_proj.weight\n",
      "65 transformer.h.10.ln_2.weight\n",
      "66 transformer.h.10.mlp.c_fc.weight\n",
      "67 transformer.h.10.mlp.c_proj.weight\n",
      "68 transformer.h.11.ln_1.weight\n",
      "69 transformer.h.11.attn.c_attn.weight\n",
      "70 transformer.h.11.attn.c_proj.weight\n",
      "71 transformer.h.11.ln_2.weight\n",
      "72 transformer.h.11.mlp.c_fc.weight\n",
      "73 transformer.h.11.mlp.c_proj.weight\n",
      "74 transformer.ln_f.weight\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 25, with 19,200 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "step 0: train loss 10.9533, val loss 10.9531\n",
      "iter 0: loss 10.9606, time 33872.10ms, mfu -100.00%\n",
      "iter 10: loss 10.2758, time 1317.20ms, mfu 102.05%\n",
      "iter 20: loss 9.4776, time 1318.14ms, mfu 102.09%\n",
      "iter 30: loss 9.1248, time 1320.65ms, mfu 102.05%\n",
      "iter 40: loss 8.7934, time 1320.15ms, mfu 102.06%\n",
      "iter 50: loss 8.3274, time 1321.25ms, mfu 101.96%\n",
      "iter 60: loss 7.7285, time 1322.67ms, mfu 101.91%\n",
      "iter 70: loss 7.2136, time 1323.42ms, mfu 101.85%\n",
      "iter 80: loss 7.0382, time 1323.67ms, mfu 101.82%\n",
      "iter 90: loss 6.5885, time 1321.75ms, mfu 101.83%\n",
      "step 100: train loss 6.1579, val loss 6.1439\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 100: loss 6.2800, time 12402.24ms, mfu 92.72%\n",
      "iter 110: loss 6.0623, time 1323.27ms, mfu 98.65%\n",
      "iter 120: loss 5.5986, time 1323.12ms, mfu 100.70%\n",
      "iter 130: loss 5.3872, time 1323.13ms, mfu 101.40%\n",
      "iter 140: loss 5.0609, time 1324.09ms, mfu 101.62%\n",
      "iter 150: loss 4.7643, time 1324.38ms, mfu 101.68%\n",
      "iter 160: loss 4.6076, time 1322.80ms, mfu 101.73%\n",
      "iter 170: loss 4.4209, time 1323.49ms, mfu 101.74%\n",
      "iter 180: loss 4.3058, time 1323.57ms, mfu 101.75%\n",
      "iter 190: loss 4.1983, time 1324.23ms, mfu 101.73%\n",
      "step 200: train loss 4.0372, val loss 4.0303\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 200: loss 4.0399, time 14076.40ms, mfu 92.49%\n",
      "iter 210: loss 3.9602, time 1326.88ms, mfu 98.46%\n",
      "iter 220: loss 3.9925, time 1324.92ms, mfu 100.52%\n",
      "iter 230: loss 3.8796, time 1321.55ms, mfu 101.36%\n",
      "iter 240: loss 3.7044, time 1322.32ms, mfu 101.70%\n",
      "iter 250: loss 3.7563, time 1321.05ms, mfu 101.80%\n",
      "iter 260: loss 3.6212, time 1322.55ms, mfu 101.78%\n",
      "iter 270: loss 3.7237, time 1322.05ms, mfu 101.83%\n",
      "iter 280: loss 3.5568, time 1322.13ms, mfu 101.81%\n",
      "iter 290: loss 3.5098, time 1323.42ms, mfu 101.81%\n",
      "step 300: train loss 3.4502, val loss 3.4339\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 300: loss 3.5656, time 14041.33ms, mfu 92.58%\n",
      "iter 310: loss 3.4349, time 1322.09ms, mfu 98.58%\n",
      "iter 320: loss 3.3681, time 1323.05ms, mfu 100.66%\n",
      "iter 330: loss 3.4272, time 1324.22ms, mfu 101.38%\n",
      "iter 340: loss 3.3386, time 1323.54ms, mfu 101.64%\n",
      "iter 350: loss 3.2946, time 1321.36ms, mfu 101.73%\n",
      "iter 360: loss 3.1052, time 1321.77ms, mfu 101.75%\n",
      "iter 370: loss 3.1685, time 1322.28ms, mfu 101.78%\n",
      "iter 380: loss 3.1914, time 1324.24ms, mfu 101.76%\n",
      "iter 390: loss 3.1793, time 1325.39ms, mfu 101.75%\n",
      "step 400: train loss 3.0027, val loss 2.9920\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 400: loss 3.1406, time 14099.27ms, mfu 92.52%\n",
      "iter 410: loss 2.9723, time 1323.82ms, mfu 98.57%\n",
      "iter 420: loss 2.9943, time 1324.17ms, mfu 100.66%\n",
      "iter 430: loss 2.8808, time 1323.92ms, mfu 101.37%\n",
      "iter 440: loss 2.9128, time 1323.88ms, mfu 101.61%\n",
      "iter 450: loss 2.8569, time 1323.74ms, mfu 101.69%\n",
      "iter 460: loss 2.7315, time 1323.56ms, mfu 101.73%\n",
      "iter 470: loss 2.6745, time 1322.96ms, mfu 101.76%\n",
      "iter 480: loss 2.7446, time 1321.21ms, mfu 101.74%\n",
      "iter 490: loss 2.7298, time 1323.84ms, mfu 101.72%\n",
      "step 500: train loss 2.5876, val loss 2.5694\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 500: loss 2.6521, time 13995.22ms, mfu 92.54%\n",
      "iter 510: loss 2.6720, time 1324.27ms, mfu 98.52%\n",
      "iter 520: loss 2.6013, time 1325.15ms, mfu 100.61%\n",
      "iter 530: loss 2.7470, time 1324.87ms, mfu 101.31%\n",
      "iter 540: loss 2.5849, time 1322.50ms, mfu 101.56%\n",
      "iter 550: loss 2.5048, time 1324.85ms, mfu 101.64%\n",
      "iter 560: loss 2.5317, time 1325.43ms, mfu 101.65%\n",
      "iter 570: loss 2.4088, time 1324.46ms, mfu 101.66%\n",
      "iter 580: loss 2.5923, time 1324.29ms, mfu 101.68%\n",
      "iter 590: loss 2.3924, time 1323.97ms, mfu 101.69%\n",
      "step 600: train loss 2.2654, val loss 2.2476\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 600: loss 2.3078, time 14296.00ms, mfu 92.44%\n",
      "iter 610: loss 2.4100, time 1324.05ms, mfu 98.47%\n",
      "iter 620: loss 2.1459, time 1321.84ms, mfu 100.57%\n",
      "iter 630: loss 2.3369, time 1323.39ms, mfu 101.30%\n",
      "iter 640: loss 2.2030, time 1324.34ms, mfu 101.52%\n",
      "iter 650: loss 2.1667, time 1324.33ms, mfu 101.64%\n",
      "iter 660: loss 2.1720, time 1324.31ms, mfu 101.67%\n",
      "iter 670: loss 2.2099, time 1324.84ms, mfu 101.64%\n",
      "iter 680: loss 2.2038, time 1325.56ms, mfu 101.65%\n",
      "iter 690: loss 2.1682, time 1321.81ms, mfu 101.68%\n",
      "step 700: train loss 2.0060, val loss 1.9979\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 700: loss 2.1118, time 14270.97ms, mfu 92.47%\n",
      "iter 710: loss 2.1403, time 1323.16ms, mfu 98.50%\n",
      "iter 720: loss 2.0134, time 1322.87ms, mfu 100.66%\n",
      "iter 730: loss 2.0216, time 1327.56ms, mfu 101.35%\n",
      "iter 740: loss 2.0096, time 1325.69ms, mfu 101.54%\n",
      "iter 750: loss 2.0910, time 1324.03ms, mfu 101.62%\n",
      "iter 760: loss 1.9802, time 1323.73ms, mfu 101.67%\n",
      "iter 770: loss 1.9839, time 1325.76ms, mfu 101.64%\n",
      "iter 780: loss 2.0212, time 1324.32ms, mfu 101.66%\n",
      "iter 790: loss 2.0585, time 1324.94ms, mfu 101.63%\n",
      "step 800: train loss 1.8349, val loss 1.8286\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 800: loss 1.9351, time 14072.56ms, mfu 92.44%\n",
      "iter 810: loss 1.9783, time 1321.84ms, mfu 98.49%\n",
      "iter 820: loss 1.9384, time 1324.60ms, mfu 100.57%\n",
      "iter 830: loss 1.9173, time 1324.31ms, mfu 101.29%\n",
      "iter 840: loss 2.0024, time 1323.93ms, mfu 101.55%\n",
      "iter 850: loss 1.8673, time 1324.92ms, mfu 101.62%\n",
      "iter 860: loss 1.7127, time 1324.34ms, mfu 101.62%\n",
      "iter 870: loss 1.7841, time 1326.35ms, mfu 101.64%\n",
      "iter 880: loss 1.8585, time 1325.12ms, mfu 101.61%\n",
      "iter 890: loss 1.7821, time 1324.93ms, mfu 101.62%\n",
      "step 900: train loss 1.6965, val loss 1.6889\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 900: loss 1.7996, time 13979.22ms, mfu 92.46%\n",
      "iter 910: loss 1.7889, time 1325.38ms, mfu 98.46%\n",
      "iter 920: loss 1.8192, time 1324.75ms, mfu 100.56%\n",
      "iter 930: loss 1.8781, time 1323.52ms, mfu 101.30%\n",
      "iter 940: loss 1.8244, time 1324.35ms, mfu 101.55%\n",
      "iter 950: loss 1.7162, time 1325.68ms, mfu 101.62%\n",
      "iter 960: loss 1.7268, time 1325.65ms, mfu 101.66%\n",
      "iter 970: loss 1.7736, time 1324.30ms, mfu 101.66%\n",
      "iter 980: loss 1.8689, time 1322.92ms, mfu 101.66%\n",
      "iter 990: loss 1.7381, time 1324.70ms, mfu 101.67%\n",
      "step 1000: train loss 1.6101, val loss 1.6048\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 1000: loss 1.6664, time 14074.49ms, mfu 92.44%\n",
      "iter 1010: loss 1.7842, time 1325.02ms, mfu 98.44%\n",
      "iter 1020: loss 1.6643, time 1323.70ms, mfu 100.55%\n",
      "iter 1030: loss 1.6902, time 1325.98ms, mfu 101.27%\n",
      "iter 1040: loss 1.6220, time 1326.69ms, mfu 101.48%\n",
      "iter 1050: loss 1.7134, time 1323.92ms, mfu 101.59%\n",
      "iter 1060: loss 1.5991, time 1323.79ms, mfu 101.66%\n",
      "iter 1070: loss 1.7339, time 1324.45ms, mfu 101.68%\n",
      "iter 1080: loss 1.6545, time 1325.06ms, mfu 101.67%\n",
      "iter 1090: loss 1.7030, time 1325.86ms, mfu 101.64%\n",
      "step 1100: train loss 1.5277, val loss 1.5326\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 1100: loss 1.5942, time 14235.59ms, mfu 92.41%\n",
      "iter 1110: loss 1.5450, time 1323.70ms, mfu 98.46%\n",
      "iter 1120: loss 1.6550, time 1323.67ms, mfu 100.58%\n",
      "iter 1130: loss 1.5507, time 1325.36ms, mfu 101.30%\n",
      "iter 1140: loss 1.5691, time 1324.49ms, mfu 101.52%\n",
      "iter 1150: loss 1.6578, time 1325.17ms, mfu 101.60%\n",
      "iter 1160: loss 1.5441, time 1326.33ms, mfu 101.60%\n",
      "iter 1170: loss 1.6827, time 1322.97ms, mfu 101.62%\n",
      "iter 1180: loss 1.5090, time 1323.91ms, mfu 101.66%\n",
      "iter 1190: loss 1.6475, time 1326.75ms, mfu 101.64%\n",
      "step 1200: train loss 1.4772, val loss 1.4788\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 1200: loss 1.5989, time 14010.19ms, mfu 92.43%\n",
      "iter 1210: loss 1.6091, time 1323.13ms, mfu 98.47%\n",
      "iter 1220: loss 1.5646, time 1323.95ms, mfu 100.58%\n",
      "iter 1230: loss 1.5758, time 1323.26ms, mfu 101.31%\n",
      "iter 1240: loss 1.5590, time 1322.42ms, mfu 101.58%\n",
      "iter 1250: loss 1.5587, time 1325.75ms, mfu 101.60%\n",
      "iter 1260: loss 1.4757, time 1323.83ms, mfu 101.65%\n",
      "iter 1270: loss 1.5152, time 1323.69ms, mfu 101.66%\n",
      "iter 1280: loss 1.5300, time 1325.59ms, mfu 101.63%\n",
      "iter 1290: loss 1.5281, time 1324.88ms, mfu 101.64%\n",
      "step 1300: train loss 1.4277, val loss 1.4302\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 1300: loss 1.5517, time 14003.54ms, mfu 92.46%\n",
      "iter 1310: loss 1.5610, time 1324.81ms, mfu 98.46%\n",
      "iter 1320: loss 1.5722, time 1325.00ms, mfu 100.58%\n",
      "iter 1330: loss 1.5258, time 1321.85ms, mfu 101.34%\n",
      "iter 1340: loss 1.5596, time 1323.54ms, mfu 101.57%\n",
      "iter 1350: loss 1.5660, time 1323.30ms, mfu 101.65%\n",
      "iter 1360: loss 1.4291, time 1322.62ms, mfu 101.68%\n",
      "iter 1370: loss 1.4928, time 1325.47ms, mfu 101.70%\n",
      "iter 1380: loss 1.4338, time 1322.33ms, mfu 101.71%\n",
      "iter 1390: loss 1.5043, time 1323.63ms, mfu 101.73%\n",
      "step 1400: train loss 1.3853, val loss 1.3786\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 1400: loss 1.4831, time 13955.07ms, mfu 92.52%\n",
      "iter 1410: loss 1.4599, time 1322.68ms, mfu 98.52%\n",
      "iter 1420: loss 1.4842, time 1322.84ms, mfu 100.62%\n",
      "iter 1430: loss 1.4504, time 1322.70ms, mfu 101.31%\n",
      "iter 1440: loss 1.3683, time 1322.08ms, mfu 101.60%\n",
      "iter 1450: loss 1.4878, time 1323.56ms, mfu 101.64%\n",
      "iter 1460: loss 1.5801, time 1325.25ms, mfu 101.64%\n",
      "iter 1470: loss 1.5149, time 1325.87ms, mfu 101.61%\n",
      "iter 1480: loss 1.4494, time 1324.28ms, mfu 101.65%\n",
      "iter 1490: loss 1.4385, time 1325.42ms, mfu 101.63%\n",
      "step 1500: train loss 1.3624, val loss 1.3651\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 1500: loss 1.5130, time 13939.40ms, mfu 92.44%\n",
      "iter 1510: loss 1.4654, time 1327.09ms, mfu 98.41%\n",
      "iter 1520: loss 1.4099, time 1324.47ms, mfu 100.55%\n",
      "iter 1530: loss 1.4191, time 1326.45ms, mfu 101.26%\n",
      "iter 1540: loss 1.4331, time 1325.76ms, mfu 101.53%\n",
      "iter 1550: loss 1.4909, time 1323.67ms, mfu 101.61%\n",
      "iter 1560: loss 1.4070, time 1323.90ms, mfu 101.63%\n",
      "iter 1570: loss 1.5048, time 1323.69ms, mfu 101.66%\n",
      "iter 1650: loss 1.3806, time 1325.50ms, mfu 101.57%\n",
      "iter 1660: loss 1.3849, time 1324.92ms, mfu 101.58%\n",
      "iter 1670: loss 1.3404, time 1323.79ms, mfu 101.59%\n",
      "iter 1680: loss 1.3789, time 1324.08ms, mfu 101.62%\n",
      "iter 1690: loss 1.3563, time 1327.08ms, mfu 101.56%\n",
      "step 1700: train loss 1.2954, val loss 1.2971\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 1700: loss 1.3259, time 14079.33ms, mfu 92.43%\n",
      "iter 1710: loss 1.3696, time 1322.91ms, mfu 98.43%\n",
      "iter 1720: loss 1.4333, time 1323.31ms, mfu 100.54%\n",
      "iter 1730: loss 1.3285, time 1325.48ms, mfu 101.25%\n",
      "iter 1740: loss 1.3344, time 1326.60ms, mfu 101.46%\n",
      "iter 1750: loss 1.3612, time 1326.11ms, mfu 101.52%\n",
      "iter 1760: loss 1.3604, time 1325.17ms, mfu 101.54%\n",
      "iter 1770: loss 1.2877, time 1327.58ms, mfu 101.53%\n",
      "iter 1780: loss 1.3996, time 1326.93ms, mfu 101.51%\n",
      "iter 1790: loss 1.3600, time 1324.42ms, mfu 101.56%\n",
      "step 1800: train loss 1.2783, val loss 1.2795\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 1800: loss 1.2543, time 13913.37ms, mfu 92.38%\n",
      "iter 1810: loss 1.4097, time 1327.45ms, mfu 98.38%\n",
      "iter 1820: loss 1.5117, time 1326.21ms, mfu 100.45%\n",
      "iter 1830: loss 1.4041, time 1327.17ms, mfu 101.18%\n",
      "iter 1840: loss 1.3440, time 1324.78ms, mfu 101.46%\n",
      "iter 1850: loss 1.3392, time 1325.74ms, mfu 101.53%\n",
      "iter 1860: loss 1.3467, time 1325.43ms, mfu 101.57%\n",
      "iter 1870: loss 1.2644, time 1325.82ms, mfu 101.57%\n",
      "iter 1880: loss 1.3692, time 1326.04ms, mfu 101.56%\n",
      "iter 1890: loss 1.2405, time 1324.99ms, mfu 101.57%\n",
      "step 1900: train loss 1.2568, val loss 1.2632\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 1900: loss 1.3717, time 14113.63ms, mfu 92.37%\n",
      "iter 1910: loss 1.3640, time 1324.57ms, mfu 98.38%\n",
      "iter 1920: loss 1.3934, time 1325.71ms, mfu 100.49%\n",
      "iter 1930: loss 1.2717, time 1325.45ms, mfu 101.22%\n",
      "iter 1940: loss 1.3994, time 1325.51ms, mfu 101.42%\n",
      "iter 1950: loss 1.2633, time 1325.24ms, mfu 101.50%\n",
      "iter 1960: loss 1.2743, time 1326.15ms, mfu 101.52%\n",
      "iter 1970: loss 1.2741, time 1325.98ms, mfu 101.51%\n",
      "iter 1980: loss 1.3772, time 1325.48ms, mfu 101.56%\n",
      "iter 1990: loss 1.2256, time 1326.70ms, mfu 101.57%\n",
      "step 2000: train loss 1.2291, val loss 1.2385\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 2000: loss 1.3934, time 14321.14ms, mfu 92.32%\n",
      "iter 2010: loss 1.2934, time 1326.18ms, mfu 98.37%\n",
      "iter 2020: loss 1.3107, time 1324.46ms, mfu 100.47%\n",
      "iter 2030: loss 1.3312, time 1324.37ms, mfu 101.17%\n",
      "iter 2040: loss 1.2607, time 1325.45ms, mfu 101.44%\n",
      "iter 2050: loss 1.3019, time 1327.48ms, mfu 101.49%\n",
      "iter 2060: loss 1.2946, time 1326.17ms, mfu 101.53%\n",
      "iter 2070: loss 1.2863, time 1324.41ms, mfu 101.57%\n",
      "iter 2080: loss 1.2640, time 1327.67ms, mfu 101.53%\n",
      "iter 2090: loss 1.3685, time 1326.02ms, mfu 101.55%\n",
      "step 2100: train loss 1.2116, val loss 1.2196\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 2100: loss 1.2653, time 14034.19ms, mfu 92.37%\n",
      "iter 2110: loss 1.2459, time 1323.60ms, mfu 98.40%\n",
      "iter 2120: loss 1.2279, time 1323.19ms, mfu 100.49%\n",
      "iter 2130: loss 1.1705, time 1323.53ms, mfu 101.20%\n",
      "iter 2140: loss 1.3111, time 1325.91ms, mfu 101.45%\n",
      "iter 2150: loss 1.2852, time 1325.42ms, mfu 101.52%\n",
      "iter 2160: loss 1.1987, time 1326.97ms, mfu 101.55%\n",
      "iter 2170: loss 1.2222, time 1326.37ms, mfu 101.53%\n",
      "iter 2180: loss 1.3004, time 1325.60ms, mfu 101.56%\n",
      "iter 2190: loss 1.3062, time 1325.84ms, mfu 101.59%\n",
      "step 2200: train loss 1.2025, val loss 1.2037\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 2200: loss 1.2294, time 14121.57ms, mfu 92.37%\n",
      "iter 2210: loss 1.2056, time 1325.57ms, mfu 98.39%\n",
      "iter 2220: loss 1.2252, time 1325.19ms, mfu 100.46%\n",
      "iter 2230: loss 1.2201, time 1326.70ms, mfu 101.21%\n",
      "iter 2240: loss 1.2708, time 1323.98ms, mfu 101.49%\n",
      "iter 2250: loss 1.2855, time 1327.41ms, mfu 101.53%\n",
      "iter 2260: loss 1.3991, time 1325.99ms, mfu 101.54%\n",
      "iter 2270: loss 1.2167, time 1325.73ms, mfu 101.58%\n",
      "iter 2280: loss 1.2237, time 1326.65ms, mfu 101.58%\n",
      "iter 2290: loss 1.1996, time 1326.57ms, mfu 101.54%\n",
      "step 2300: train loss 1.1785, val loss 1.1941\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 2300: loss 1.2323, time 14118.73ms, mfu 92.34%\n",
      "iter 2310: loss 1.2877, time 1325.99ms, mfu 98.36%\n",
      "iter 2320: loss 1.2432, time 1324.98ms, mfu 100.49%\n",
      "iter 2330: loss 1.3612, time 1326.29ms, mfu 101.20%\n",
      "iter 2340: loss 1.2329, time 1325.78ms, mfu 101.41%\n",
      "iter 2350: loss 1.2526, time 1325.79ms, mfu 101.51%\n",
      "iter 2360: loss 1.3313, time 1323.92ms, mfu 101.54%\n",
      "iter 2370: loss 1.2641, time 1327.64ms, mfu 101.52%\n",
      "iter 2380: loss 1.2463, time 1326.91ms, mfu 101.54%\n",
      "iter 2390: loss 1.2523, time 1325.80ms, mfu 101.53%\n",
      "step 2400: train loss 1.1729, val loss 1.1758\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 2400: loss 1.2279, time 14060.85ms, mfu 92.37%\n",
      "iter 2410: loss 1.2284, time 1326.55ms, mfu 98.38%\n",
      "iter 2420: loss 1.2676, time 1325.79ms, mfu 100.47%\n",
      "iter 2430: loss 1.2448, time 1325.74ms, mfu 101.20%\n",
      "iter 2440: loss 1.2490, time 1326.67ms, mfu 101.43%\n",
      "iter 2450: loss 1.2434, time 1326.05ms, mfu 101.49%\n",
      "iter 2460: loss 1.2668, time 1327.62ms, mfu 101.49%\n",
      "iter 2470: loss 1.2302, time 1325.31ms, mfu 101.49%\n",
      "iter 2480: loss 1.2155, time 1327.67ms, mfu 101.49%\n",
      "iter 2490: loss 1.2644, time 1325.00ms, mfu 101.54%\n",
      "step 2500: train loss 1.1533, val loss 1.1601\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 2500: loss 1.2107, time 14165.32ms, mfu 92.34%\n",
      "iter 2510: loss 1.2197, time 1323.99ms, mfu 98.41%\n",
      "iter 2520: loss 1.1816, time 1323.94ms, mfu 100.51%\n",
      "iter 2530: loss 1.3123, time 1326.46ms, mfu 101.18%\n",
      "iter 2540: loss 1.2652, time 1326.37ms, mfu 101.43%\n",
      "iter 2550: loss 1.2134, time 1325.51ms, mfu 101.54%\n",
      "iter 2560: loss 1.1842, time 1324.32ms, mfu 101.61%\n",
      "iter 2570: loss 1.3196, time 1324.64ms, mfu 101.59%\n",
      "iter 2580: loss 1.1976, time 1325.50ms, mfu 101.55%\n",
      "iter 2590: loss 1.2393, time 1328.24ms, mfu 101.57%\n",
      "step 2600: train loss 1.1438, val loss 1.1509\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 2600: loss 1.2846, time 13993.32ms, mfu 92.40%\n",
      "iter 2610: loss 1.3222, time 1325.60ms, mfu 98.40%\n",
      "iter 2620: loss 1.2174, time 1325.29ms, mfu 100.47%\n",
      "iter 2630: loss 1.2383, time 1326.49ms, mfu 101.17%\n",
      "iter 2640: loss 1.2588, time 1326.35ms, mfu 101.43%\n",
      "iter 2650: loss 1.1846, time 1326.44ms, mfu 101.54%\n",
      "iter 2660: loss 1.2870, time 1324.48ms, mfu 101.58%\n",
      "iter 2670: loss 1.1849, time 1324.35ms, mfu 101.58%\n",
      "iter 2680: loss 1.1346, time 1326.93ms, mfu 101.57%\n",
      "iter 2690: loss 1.2369, time 1325.04ms, mfu 101.57%\n",
      "step 2700: train loss 1.1331, val loss 1.1459\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 2700: loss 1.2855, time 13937.76ms, mfu 92.41%\n",
      "iter 2710: loss 1.2190, time 1324.70ms, mfu 98.43%\n",
      "iter 2720: loss 1.2082, time 1323.95ms, mfu 100.54%\n",
      "iter 2730: loss 1.2332, time 1327.37ms, mfu 101.20%\n",
      "iter 2740: loss 1.1597, time 1325.83ms, mfu 101.45%\n",
      "iter 2750: loss 1.2043, time 1324.83ms, mfu 101.58%\n",
      "iter 2760: loss 1.2037, time 1325.10ms, mfu 101.58%\n",
      "iter 2770: loss 1.2125, time 1325.89ms, mfu 101.55%\n",
      "iter 2780: loss 1.1879, time 1325.05ms, mfu 101.57%\n",
      "iter 2790: loss 1.1351, time 1323.77ms, mfu 101.60%\n",
      "step 2800: train loss 1.1216, val loss 1.1324\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 2800: loss 1.1635, time 14405.73ms, mfu 92.32%\n",
      "iter 2810: loss 1.2737, time 1326.06ms, mfu 98.39%\n",
      "iter 2820: loss 1.1799, time 1325.07ms, mfu 100.52%\n",
      "iter 2830: loss 1.2446, time 1325.93ms, mfu 101.25%\n",
      "iter 2840: loss 1.1591, time 1325.05ms, mfu 101.52%\n",
      "iter 2850: loss 1.0949, time 1322.40ms, mfu 101.66%\n",
      "iter 2860: loss 1.1711, time 1323.59ms, mfu 101.68%\n",
      "iter 2870: loss 1.1511, time 1326.02ms, mfu 101.70%\n",
      "iter 2880: loss 1.1979, time 1324.83ms, mfu 101.68%\n",
      "iter 2890: loss 1.1576, time 1322.74ms, mfu 101.71%\n",
      "step 2900: train loss 1.1128, val loss 1.1159\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 2900: loss 1.2349, time 14100.10ms, mfu 92.44%\n",
      "iter 2910: loss 1.2616, time 1324.06ms, mfu 98.53%\n",
      "iter 2920: loss 1.2240, time 1322.99ms, mfu 100.65%\n",
      "iter 2930: loss 1.3448, time 1323.93ms, mfu 101.39%\n",
      "iter 2940: loss 1.1405, time 1323.38ms, mfu 101.66%\n",
      "iter 2950: loss 1.1799, time 1324.35ms, mfu 101.73%\n",
      "iter 2960: loss 1.1752, time 1324.42ms, mfu 101.75%\n",
      "iter 2970: loss 1.2366, time 1323.86ms, mfu 101.77%\n",
      "iter 2980: loss 1.2134, time 1321.83ms, mfu 101.77%\n",
      "iter 2990: loss 1.0656, time 1321.42ms, mfu 101.78%\n",
      "step 3000: train loss 1.1051, val loss 1.1227\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 3000: loss 1.1373, time 14280.48ms, mfu 92.55%\n",
      "iter 3010: loss 1.1875, time 1321.38ms, mfu 98.62%\n",
      "iter 3020: loss 1.1098, time 1321.64ms, mfu 100.75%\n",
      "iter 3030: loss 1.1675, time 1321.49ms, mfu 101.48%\n",
      "iter 3040: loss 1.1553, time 1322.82ms, mfu 101.69%\n",
      "iter 3050: loss 1.1606, time 1323.78ms, mfu 101.78%\n",
      "iter 3060: loss 1.2444, time 1323.84ms, mfu 101.81%\n",
      "iter 3070: loss 1.1874, time 1320.69ms, mfu 101.84%\n",
      "iter 3080: loss 1.1960, time 1323.80ms, mfu 101.85%\n",
      "iter 3090: loss 1.1463, time 1323.13ms, mfu 101.85%\n",
      "step 3100: train loss 1.0968, val loss 1.1115\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 3100: loss 1.1488, time 13936.67ms, mfu 92.64%\n",
      "iter 3110: loss 1.1458, time 1323.87ms, mfu 98.62%\n",
      "iter 3120: loss 1.2741, time 1323.00ms, mfu 100.68%\n",
      "iter 3130: loss 1.1426, time 1322.20ms, mfu 101.42%\n",
      "iter 3140: loss 1.2406, time 1324.07ms, mfu 101.65%\n",
      "iter 3150: loss 1.1413, time 1320.64ms, mfu 101.75%\n",
      "iter 3160: loss 1.2128, time 1325.11ms, mfu 101.75%\n",
      "iter 3170: loss 1.1215, time 1324.40ms, mfu 101.75%\n",
      "iter 3180: loss 1.1024, time 1322.66ms, mfu 101.76%\n",
      "iter 3190: loss 1.1955, time 1322.32ms, mfu 101.76%\n",
      "step 3200: train loss 1.0902, val loss 1.1048\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 3200: loss 1.1651, time 14266.11ms, mfu 92.50%\n",
      "iter 3210: loss 1.1172, time 1323.20ms, mfu 98.51%\n",
      "iter 3220: loss 1.1837, time 1324.24ms, mfu 100.59%\n",
      "iter 3230: loss 1.1618, time 1324.71ms, mfu 101.30%\n",
      "iter 3240: loss 1.2022, time 1324.03ms, mfu 101.55%\n",
      "iter 3250: loss 1.1440, time 1322.99ms, mfu 101.64%\n",
      "iter 3260: loss 1.1669, time 1326.09ms, mfu 101.66%\n",
      "iter 3270: loss 1.1250, time 1326.40ms, mfu 101.62%\n",
      "iter 3280: loss 1.1794, time 1323.23ms, mfu 101.62%\n",
      "iter 3290: loss 1.1515, time 1326.83ms, mfu 101.59%\n",
      "step 3300: train loss 1.0857, val loss 1.0972\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 3300: loss 1.1224, time 13896.63ms, mfu 92.39%\n",
      "iter 3310: loss 1.1921, time 1326.05ms, mfu 98.38%\n",
      "iter 3320: loss 1.1815, time 1325.70ms, mfu 100.49%\n",
      "iter 3330: loss 1.1896, time 1325.38ms, mfu 101.21%\n",
      "iter 3340: loss 1.1718, time 1325.75ms, mfu 101.45%\n",
      "iter 3350: loss 1.1511, time 1326.67ms, mfu 101.47%\n",
      "iter 3360: loss 1.1742, time 1326.54ms, mfu 101.50%\n",
      "iter 3370: loss 1.2393, time 1326.51ms, mfu 101.51%\n",
      "iter 3380: loss 1.0692, time 1326.76ms, mfu 101.53%\n",
      "iter 3390: loss 1.2074, time 1325.37ms, mfu 101.56%\n",
      "step 3400: train loss 1.0842, val loss 1.0891\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 3400: loss 1.1024, time 14211.90ms, mfu 92.35%\n",
      "iter 3410: loss 1.0993, time 1325.79ms, mfu 98.42%\n",
      "iter 3420: loss 1.1700, time 1324.47ms, mfu 100.51%\n",
      "iter 3430: loss 1.1140, time 1325.45ms, mfu 101.21%\n",
      "iter 3440: loss 1.1761, time 1326.62ms, mfu 101.46%\n",
      "iter 3450: loss 1.0861, time 1326.07ms, mfu 101.53%\n",
      "iter 3460: loss 1.0910, time 1323.06ms, mfu 101.52%\n",
      "iter 3470: loss 1.1993, time 1326.26ms, mfu 101.54%\n",
      "iter 3480: loss 1.1917, time 1325.67ms, mfu 101.56%\n",
      "iter 3490: loss 1.1352, time 1326.38ms, mfu 101.56%\n",
      "step 3500: train loss 1.0682, val loss 1.0878\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 3500: loss 1.1242, time 14024.96ms, mfu 92.35%\n",
      "iter 3510: loss 1.1069, time 1325.90ms, mfu 98.39%\n",
      "iter 3520: loss 1.1553, time 1327.13ms, mfu 100.46%\n",
      "iter 3530: loss 1.1284, time 1325.09ms, mfu 101.20%\n",
      "iter 3540: loss 1.1240, time 1323.92ms, mfu 101.47%\n",
      "iter 3550: loss 1.0978, time 1325.64ms, mfu 101.56%\n",
      "iter 3560: loss 1.1084, time 1324.35ms, mfu 101.58%\n",
      "iter 3570: loss 1.1471, time 1326.10ms, mfu 101.57%\n",
      "iter 3580: loss 1.0449, time 1326.64ms, mfu 101.52%\n",
      "iter 3590: loss 1.1832, time 1325.70ms, mfu 101.49%\n",
      "step 3600: train loss 1.0605, val loss 1.0777\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 3600: loss 1.1366, time 14035.19ms, mfu 92.33%\n",
      "iter 3610: loss 1.1412, time 1325.79ms, mfu 98.37%\n",
      "iter 3620: loss 1.1086, time 1325.81ms, mfu 100.42%\n",
      "iter 3630: loss 1.0938, time 1326.42ms, mfu 101.14%\n",
      "iter 3640: loss 1.1339, time 1324.71ms, mfu 101.42%\n",
      "iter 3650: loss 1.1619, time 1327.56ms, mfu 101.46%\n",
      "iter 3660: loss 1.1336, time 1324.99ms, mfu 101.51%\n",
      "iter 3670: loss 1.1909, time 1325.62ms, mfu 101.54%\n",
      "iter 3680: loss 1.1330, time 1324.60ms, mfu 101.54%\n",
      "iter 3690: loss 1.0988, time 1326.65ms, mfu 101.53%\n",
      "step 3700: train loss 1.0610, val loss 1.0737\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 3700: loss 1.1144, time 14071.38ms, mfu 92.35%\n",
      "iter 3710: loss 1.1310, time 1322.93ms, mfu 98.41%\n",
      "iter 3720: loss 1.0355, time 1324.82ms, mfu 100.52%\n",
      "iter 3730: loss 1.1596, time 1324.94ms, mfu 101.22%\n",
      "iter 3740: loss 1.0522, time 1324.67ms, mfu 101.48%\n",
      "iter 3750: loss 1.1267, time 1325.24ms, mfu 101.56%\n",
      "iter 3760: loss 1.1391, time 1325.05ms, mfu 101.56%\n",
      "iter 3770: loss 1.1257, time 1325.96ms, mfu 101.57%\n",
      "iter 3780: loss 1.1959, time 1324.84ms, mfu 101.57%\n",
      "iter 3790: loss 1.0901, time 1324.20ms, mfu 101.59%\n",
      "step 3800: train loss 1.0552, val loss 1.0716\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 3800: loss 1.1039, time 14099.02ms, mfu 92.38%\n",
      "iter 3810: loss 1.1581, time 1325.59ms, mfu 98.41%\n",
      "iter 3820: loss 1.1124, time 1323.65ms, mfu 100.50%\n",
      "iter 3830: loss 1.1655, time 1324.70ms, mfu 101.24%\n",
      "iter 3840: loss 1.1283, time 1325.00ms, mfu 101.47%\n",
      "iter 3850: loss 1.1914, time 1327.05ms, mfu 101.54%\n",
      "iter 3860: loss 1.1694, time 1326.87ms, mfu 101.55%\n",
      "iter 3870: loss 1.1277, time 1326.22ms, mfu 101.56%\n",
      "iter 3880: loss 1.0814, time 1327.27ms, mfu 101.53%\n",
      "iter 3890: loss 1.0536, time 1325.26ms, mfu 101.57%\n",
      "step 3900: train loss 1.0503, val loss 1.0661\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 3900: loss 1.2247, time 14080.88ms, mfu 92.36%\n",
      "iter 3910: loss 1.1198, time 1326.91ms, mfu 98.38%\n",
      "iter 3920: loss 1.1303, time 1325.33ms, mfu 100.48%\n",
      "iter 3930: loss 1.1085, time 1326.57ms, mfu 101.21%\n",
      "iter 3940: loss 1.0660, time 1325.04ms, mfu 101.49%\n",
      "iter 3950: loss 1.2228, time 1327.45ms, mfu 101.53%\n",
      "iter 3960: loss 1.1583, time 1327.21ms, mfu 101.54%\n",
      "iter 3970: loss 1.1587, time 1326.36ms, mfu 101.55%\n",
      "iter 3980: loss 1.1741, time 1326.12ms, mfu 101.56%\n",
      "iter 3990: loss 1.0963, time 1326.60ms, mfu 101.56%\n",
      "step 4000: train loss 1.0447, val loss 1.0634\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 4000: loss 1.1385, time 14296.76ms, mfu 92.32%\n",
      "iter 4010: loss 1.1386, time 1325.27ms, mfu 98.42%\n",
      "iter 4020: loss 1.0756, time 1325.32ms, mfu 100.49%\n",
      "iter 4030: loss 1.1840, time 1327.52ms, mfu 101.21%\n",
      "iter 4040: loss 1.0970, time 1325.53ms, mfu 101.47%\n",
      "iter 4050: loss 1.1602, time 1326.07ms, mfu 101.54%\n",
      "iter 4060: loss 1.1234, time 1324.92ms, mfu 101.57%\n",
      "iter 4070: loss 1.1237, time 1327.12ms, mfu 101.57%\n",
      "iter 4080: loss 1.0588, time 1326.26ms, mfu 101.58%\n",
      "iter 4090: loss 1.1000, time 1325.24ms, mfu 101.59%\n",
      "step 4100: train loss 1.0401, val loss 1.0623\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 4100: loss 1.1060, time 14060.74ms, mfu 92.40%\n",
      "iter 4110: loss 1.1142, time 1325.02ms, mfu 98.41%\n",
      "iter 4120: loss 1.0760, time 1324.39ms, mfu 100.49%\n",
      "iter 4130: loss 1.0886, time 1324.81ms, mfu 101.21%\n",
      "iter 4140: loss 1.1158, time 1326.31ms, mfu 101.41%\n",
      "iter 4150: loss 1.0874, time 1326.47ms, mfu 101.49%\n",
      "iter 4160: loss 1.1111, time 1326.72ms, mfu 101.53%\n",
      "iter 4170: loss 1.0292, time 1324.74ms, mfu 101.58%\n",
      "iter 4180: loss 1.0564, time 1323.73ms, mfu 101.57%\n",
      "iter 4190: loss 1.1342, time 1327.64ms, mfu 101.55%\n",
      "step 4200: train loss 1.0363, val loss 1.0457\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 4200: loss 1.0731, time 14049.09ms, mfu 92.37%\n",
      "iter 4210: loss 1.1018, time 1325.09ms, mfu 98.42%\n",
      "iter 4220: loss 1.0992, time 1325.08ms, mfu 100.50%\n",
      "iter 4230: loss 1.0869, time 1325.49ms, mfu 101.22%\n",
      "iter 4240: loss 1.1429, time 1325.62ms, mfu 101.44%\n",
      "iter 4250: loss 1.1028, time 1325.13ms, mfu 101.60%\n",
      "iter 4260: loss 1.1327, time 1325.01ms, mfu 101.66%\n",
      "iter 4270: loss 1.1066, time 1324.82ms, mfu 101.68%\n",
      "iter 4280: loss 1.1410, time 1322.80ms, mfu 101.69%\n",
      "iter 4290: loss 1.1528, time 1324.63ms, mfu 101.67%\n",
      "step 4300: train loss 1.0271, val loss 1.0531\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 4300: loss 1.1645, time 14222.65ms, mfu 92.46%\n",
      "iter 4310: loss 1.1371, time 1324.09ms, mfu 98.48%\n",
      "iter 4320: loss 1.0153, time 1323.19ms, mfu 100.59%\n",
      "iter 4330: loss 1.0534, time 1323.09ms, mfu 101.31%\n",
      "iter 4340: loss 1.1234, time 1324.09ms, mfu 101.58%\n",
      "iter 4350: loss 1.0823, time 1325.44ms, mfu 101.68%\n",
      "iter 4360: loss 1.1326, time 1323.16ms, mfu 101.73%\n",
      "iter 4370: loss 1.0549, time 1324.27ms, mfu 101.73%\n",
      "iter 4380: loss 1.1314, time 1324.28ms, mfu 101.74%\n",
      "iter 4390: loss 1.1107, time 1323.51ms, mfu 101.75%\n",
      "step 4400: train loss 1.0281, val loss 1.0511\n",
      "saving checkpoint to xmodel5000-story.pt\n",
      "iter 4400: loss 1.0333, time 14222.86ms, mfu 92.51%\n",
      "iter 4410: loss 1.0959, time 1322.54ms, mfu 98.54%\n",
      "iter 4420: loss 1.0899, time 1322.21ms, mfu 100.64%\n",
      "iter 4430: loss 1.1210, time 1325.06ms, mfu 101.36%\n",
      "iter 4440: loss 1.1126, time 1322.41ms, mfu 101.64%\n",
      "iter 4450: loss 1.0732, time 1323.02ms, mfu 101.72%\n",
      "iter 4460: loss 1.1420, time 1323.65ms, mfu 101.76%\n",
      "iter 4470: loss 1.1052, time 1322.27ms, mfu 101.79%\n"
     ]
    }
   ],
   "source": [
    "max_iters = 4000 # total number of training iterations\n",
    "checkpoint_file = f'xmodel{max_iters}-story.pt'\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "################################# Parameters ##################################\n",
    "\"\"\"\n",
    "GPT-2 Settings\n",
    "    block_size: int = 1024\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "\"\"\"\n",
    "\n",
    "# config\n",
    "eval_iters = 200 # number of iterations to run evaluation\n",
    "dataset = 'data' # directory where the data is stored\n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
    "batch_size = 12 # [12] number of batches to run in parallel\n",
    "block_size = 1024 # [32] content window size (tokens)\n",
    "\n",
    "# model\n",
    "n_layer = 12     # 4  - layers of \n",
    "n_head = 12      # 4  - attention heads\n",
    "n_embd = 768     # 64 - dimensionality of the embedding vectors\n",
    "dropout = 0.2   # 0. - for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False    # False - do we use bias inside LayerNorm and Linear layers?\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # 6e-4 max learning rate\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 60000 # [600000] should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16'\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# capture above settings & parameters to save in model checkpoint\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys} \n",
    "\n",
    "# tokens per iterations\n",
    "tokens_per_iter = gradient_accumulation_steps * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "# set the random seed\n",
    "seed = 1337\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# Load the data\n",
    "data_dir = dataset + '/'\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "# get a batch from the data\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# loop counters starting point\n",
    "iter_num = 0\n",
    "\n",
    "# init a new model from scratch\n",
    "print(\"Initializing a new model from scratch\")\n",
    "print(\"Using vocab_size of GPT-2 of 50304 (50257 rounded up for efficiency)\")\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=50304, dropout=dropout) \n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)\n",
    "\n",
    "# report number of parameters\n",
    "print(\"Number of parameters: %.2fM\" % (model.get_num_params()/1e6,))\n",
    "print(\"Layers:\")\n",
    "for number, (name, param) in enumerate(model.named_parameters()):\n",
    "    print(number, name)\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "model.to(device)\n",
    "\n",
    "# initialize a GradScaler\n",
    "#scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "scaler = torch.amp.GradScaler(device_type, enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "# compile the model if not using MPS\n",
    "if device == 'mps':\n",
    "    print(\"MPS doesn't support JIT compilation, skipping...\")\n",
    "else:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    model = torch.compile(model) # requires PyTorch 2.0\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad() # disable gradient tracking - not needed for backward pass\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# save model to checkpoint file\n",
    "def save_model(fn):\n",
    "    checkpoint = {\n",
    "        'model': raw_model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model_args': model_args,\n",
    "        'iter_num': iter_num,\n",
    "        'config': config,\n",
    "    }\n",
    "    print(f\"saving checkpoint to {fn}\")\n",
    "    torch.save(checkpoint, fn)\n",
    "\n",
    "# stats\n",
    "stat_iter = []\n",
    "stat_loss_train = []\n",
    "stat_loss_val = []\n",
    "stat_lr_iter = []\n",
    "stat_lr = []\n",
    "\n",
    "# TRAINING LOOP\n",
    "\n",
    "# init loop\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model\n",
    "running_mfu = -1.0 # Memory Footprint Utilization on GPU\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # determine and set the learning rate for this iteration\n",
    "        lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "        if lr > 0:\n",
    "            stat_lr.append(lr)\n",
    "            stat_lr_iter.append(iter_num)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        # evaluate the loss on train/val sets and write checkpoints\n",
    "        if iter_num % 100 == 0 or iter_num >= max_iters:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            if iter_num > 0:\n",
    "                save_model(checkpoint_file)\n",
    "            # record stats\n",
    "            stat_iter.append(iter_num)\n",
    "            stat_loss_train.append(losses['train'])\n",
    "            stat_loss_val.append(losses['val'])\n",
    "    \n",
    "        # FORWARD PROP, update with optional gradient accumulation to simulate larger batch size\n",
    "        # and using the GradScaler if data type is float16\n",
    "        for micro_step in range(gradient_accumulation_steps):\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "            # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "            X, Y = get_batch('train')\n",
    "            # with gradient scaling if training in fp16\n",
    "            scaler.scale(loss).backward()\n",
    "        # clip the gradient - to avoid exploding gradients\n",
    "        if grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        # BACK PROP\n",
    "        # step the optimizer and scaler if training in fp16\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # flush the gradients as soon as we can, no need for this memory anymore\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "        # stats for timing\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        if iter_num % 10 == 0:\n",
    "            print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "        iter_num += 1\n",
    "        local_iter_num += 1\n",
    "    \n",
    "        # termination conditions\n",
    "        if iter_num > max_iters:\n",
    "            break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"User requested exit, saving checkpoint\")\n",
    "        break\n",
    "\n",
    "save_model(checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3662a04-ce04-4007-9e46-d62660a4916f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses['train']=tensor(1.0103) losses['val']=tensor(1.0302)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb6a154e1d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(f\"{losses['train']=} {losses['val']=}\")\n",
    "plt.plot(stat_iter, stat_loss_train, stat_iter, stat_loss_val)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iter')\n",
    "plt.legend(['training', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c5bf040-5615-4e2d-a678-62e3d71d1b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb6a34da210>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(stat_lr_iter, stat_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419183b-a46e-47f7-94c0-9533e3e7c494",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00ac0364-f706-44a8-b157-b088656227be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: xmodel5000-story.pt\n",
      "Number of parameters: 123.59M\n",
      "A story about Tim.\n",
      "One day, Tim's mom said, \"Tim, it's time to bathe!\" Tim did not want to stop playing, but he knew he had to listen to his mom. So, he went to the big tub and filled it with warm water. He had so much fun playing in the water. After his bath, Tim felt clean and happy.\n",
      "The next day, Tim saw his friend, Sam. Sam was sad because he was done. Tim wanted to help, so he told Sam a story. They both laughed and were glad they had listened to Tim's mom. And from that day on, Tim always remembered to be alert and listen to his mom.\n",
      "<|endoftext|>\n",
      "Once upon a time, there was a big fish who lived in a pond. The big fish had a special job. He had to weigh things. He loved to help his friends.\n",
      "One day, the big fish saw a tall tree with a box on top. He wanted to weigh it too. He swam up to the tree and jumped on it. The box started to shake. It was not a normal box.\n",
      "Just then, a small bird came and told the big fish that it was not a normal box. It was a magic box. The box opened, and out came a big octopus. The octopus thanked the big fish for his help and promised to help him weigh things again. The big fish was very happy and they became good friends.\n",
      "<|endoftext|>\n",
      "One day, a curious little cat named Tim saw a big tree with a long stick hanging from it. Tim wanted to play with the stick, so he jumped up and tried to reach it. But the stick was too high up.\n",
      "A nice girl named Sue saw Tim and wanted to help. She said, \"Come, Tim! I will encourage you to jump and grab the stick.\" Tim was scared, but he wanted the stick so he tried to jump again.\n",
      "This time, Tim jumped hard and grabbed the stick. He felt happy and proud. Sue and Tim played with the stick all day. They had lots of fun together, and Tim learned that with a little encouragement, he could do anything.\n",
      "<|endoftext|>\n",
      "\n",
      "Once upon a time there lived a girl named Julie. She was very creative and looked out of her window every day. One day Julie decided to take a walk in the park.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = 'xmodel5000-story.pt'\n",
    "\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "print(f\"Checkpoint: {checkpoint_file}\")\n",
    "\n",
    "# inference parameters\n",
    "max_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 42\n",
    "device = 'cuda'\n",
    "dtype = 'bfloat16'\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# load model saved in a specific directory\n",
    "checkpoint = torch.load(checkpoint_file, map_location=device, weights_only=True)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(\"Number of parameters: %.2fM\" % (model.get_num_params()/1e6,))\n",
    "\n",
    "# set prompt\n",
    "prompt = \"A story about Tim.\\n\"\n",
    "start_ids = enc.encode(prompt, allowed_special={\"<|endoftext|>\"})\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        y = model.generate(x, max_tokens, temperature=temperature, top_k=top_k)\n",
    "        output = y[0].tolist()\n",
    "        for w in output:\n",
    "            if w > 50257: # max token value, ignore the rest\n",
    "                continue\n",
    "            elif w == enc.eot_token:\n",
    "                break\n",
    "            else:\n",
    "                text = enc.decode([w])\n",
    "                if text == '\\n':\n",
    "                    print()\n",
    "                else:\n",
    "                    print(text, end='')\n",
    "                \n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30b98e-12f8-4f11-b489-7619310b2275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5b4f116-fda5-43f8-9e2f-2ffe5a9ca8ee",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Standford Univesity - CS231n: Deep Learning for Computer Vision - https://cs231n.github.io/neural-networks-1/\n",
    "* Visual Transformer, Explained - https://poloclub.github.io/transformer-explainer/\n",
    "* Cousera: Generative Ai with LLMs - https://www.coursera.org/learn/generative-ai-with-llms\n",
    "* Attention is All You Need by Vaswani et al. in 2017 - https://arxiv.org/abs/1706.03762\n",
    "* The Illustrated Transformer by Jay Alammar - https://jalammar.github.io/illustrated-transformer/\n",
    "* Visualizing Attention, a Transformer's Heart - https://www.3blue1brown.com/lessons/attention\n",
    "* Let's build GPT: from scratch, in code, spelled out. - by Andrej Karpathy - https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "* nanoGPT by Andrej Karpathy - https://github.com/karpathy/nanoGPT\n",
    "* OpenAI GPT-2 - https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "\n",
    "## Papers\n",
    "- Makemore, by Andrej Karpathy - https://github.com/karpathy/makemore\n",
    "- MLP, following [Bengio et al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "- CNN, following [DeepMind WaveNet 2016](https://arxiv.org/abs/1609.03499) (in progress...)\n",
    "- RNN, following [Mikolov et al. 2010](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)\n",
    "- LSTM, following [Graves et al. 2014](https://arxiv.org/abs/1308.0850)\n",
    "- GRU, following [Kyunghyun Cho et al. 2014](https://arxiv.org/abs/1409.1259)\n",
    "- Transformer, following [Vaswani et al. 2017](https://arxiv.org/abs/1706.03762)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
