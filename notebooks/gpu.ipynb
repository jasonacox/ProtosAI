{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88586b3d-17ec-4b07-a209-11e4641274c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Information for PyTorch\n",
      "   Version of torch:  2.4.0+cu121\n",
      "\n",
      "GPU Details\n",
      "   Number of GPUs available: 2\n",
      "   Type: cuda \n",
      "\n",
      "   GPU 0: NVIDIA GeForce RTX 3090 - cuda:0\n",
      "      Compute Capability: 8.6\n",
      "      Memory: 23.69 GB\n",
      "         Used: 20.72 GB (88%)\n",
      "         Free: 2.97 GB (12%)\n",
      "\n",
      "   GPU 1: NVIDIA GeForce GTX 1060 6GB - cuda:1\n",
      "      Compute Capability: 6.1\n",
      "      Memory: 5.93 GB\n",
      "         Used: 0.26 GB (5%)\n",
      "         Free: 5.67 GB (95%)\n",
      "\n",
      "PyTorch Test with cuda:1 - Random 4x4 Array\n",
      "\n",
      "tensor([[1194, 1066, 9717, 4323],\n",
      "        [7049, 2869, 3129, 2122],\n",
      "        [1918, 2077, 7417, 5947],\n",
      "        [9159, 7214, 2688, 3171]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "print(\"GPU Information for PyTorch\")\n",
    "print(\"   Version of torch: \",end=\"\")\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# AMD ROCm requires environmental override to prevent segfault\n",
    "sim = \"\"\n",
    "if \"rocm\" in torch.__version__:\n",
    "    os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = \"10.3.0\"\n",
    "    sim = \"(ROCm)\"\n",
    "\n",
    "print(f\" {torch.__version__}\")\n",
    "print()\n",
    "print(\"GPU Details\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Get number of GPUs available\n",
    "    gpus = torch.cuda.device_count()\n",
    "\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"   Number of GPUs available: {num_gpus}\")\n",
    "    print(f\"   Type: cuda {sim}\")\n",
    "    \n",
    "    # List each GPU's name, free memory, and total memory\n",
    "    for i in range(num_gpus):\n",
    "        mem_free, mem_total = 0,0\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        capability = torch.cuda.get_device_capability(i)\n",
    "        cuda_device_name = f\"cuda:{i}\"\n",
    "        mem_free, mem_total = torch.cuda.mem_get_info(i)\n",
    "        mem_free_mb = mem_free / 1024**3\n",
    "        mem_total_mb = mem_total / 1024**3\n",
    "        pct = (mem_free / mem_total) * 100\n",
    "        print()\n",
    "        print(f\"   GPU {i}: {gpu_name} - {cuda_device_name}\")\n",
    "        print(f\"      Compute Capability: {capability[0]}.{capability[1]}\")\n",
    "        print(f\"      Memory: {mem_total_mb:.2f} GB\")\n",
    "        print(f\"         Used: {mem_total_mb-mem_free_mb:.2f} GB ({100-int(pct)}%)\")\n",
    "        print(f\"         Free: {mem_free_mb:.2f} GB ({int(pct)}%)\")\n",
    "        cuda_device_name = f\"cuda:{i}\"\n",
    "    torch_device = torch.device(cuda_device_name)\n",
    "\n",
    "# Check PyTorch has access to Apple MPS (Metal Performance Shader)\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"   Device: Apple Silicon Found\")\n",
    "    print(f\"   MPS (Metal Performance Shader) built: {torch.backends.mps.is_built()}\")\n",
    "    print(f\"   MPS available: {torch.backends.mps.is_available()}\")\n",
    "    torch_device = torch.device(\"mps\")\n",
    "\n",
    "# No GPUs Available\n",
    "if not (torch.cuda.is_available() or torch.backends.mps.is_available()):\n",
    "    print(\"   ** No GPU support found **\")\n",
    "    print(\"   Device: CPU\")\n",
    "    torch_device = torch.device(\"cpu\")\n",
    "\n",
    "# Run a simple PyTorch test\n",
    "print()\n",
    "print(f\"PyTorch Test with {torch_device} - Random 4x4 Array\\n\")\n",
    "random_array = torch.randint(low=0, high=10000, size=(4, 4), device=torch_device)\n",
    "print(random_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112b59fb-b888-4572-9dd5-c0a7d69a6d15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_gpus():\n",
    "    # Get the number of GPUs available\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    \n",
    "    # List each GPU's name\n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"GPU {i}: {gpu_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12112f6c-e3aa-4c36-8086-45c6ff2d9f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 2\n",
      "GPU 0: NVIDIA GeForce RTX 3090\n",
      "GPU 1: NVIDIA GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "list_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583fb7cb-6c73-4c39-8f4f-7a872460f20d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
